# Introduction

Before we begin, I’ll make a very brief mention of a notebook that I made available to you regarding the previous lecture's section. Anyway, today we approach the problem of the generation and characterization of **extracellular signals**. For the moment I showed you, and we discussed extensively, the biophysical bases of the generation of intracellular signals, or those recorded from the intracellular point of view. Today, effectively, we start to see what is needed to understand extracellular signals.

### Notebook Analysis: *Short Term Synaptic Depression*

Before doing this, I'll go to the usual GitHub and there is a notebook called **Short Term Synaptic Depression**. In fact, it traces and resumes the description of the kinetic Markov model of short-term depression, therefore of the temporal dynamics of the vesicles, of the resources for neurotransmission that we saw last time. Besides having the text and the little formulas, you theoretically have the Python code to generate some figures that I showed you in class. Some are like this, they are not interactive. If you are curious, you might want to look at how it is made, where the equations are.

This thing, which shouldn't particularly disturb you, is that for each of those states, for each of those variables that describe the fraction of vesicles in a state $R$ (*recovered* and effective) or $I$ (*inactive*), **Euler** was used, the numerical method of Euler, for which the time derivative is approximated with the difference quotient. It is not particularly complicated. The potentially interesting part was to show you from an interactive point of view, particularly for the combination of short-term and long-term plasticity.

What I do here is, again, simulate exactly the same thing, however, I can change the frequency. I can change the arrival frequency of this train of presynaptic action potentials with uniform frequency, so they are intervals between one event and another that are identical (again the university that pays me the salary every month). I can change the frequency and I can change two other parameters. You see that if I slow down, if I increase the distance between one spike and another, the so-called presynaptic **inter-spike interval**, the dynamics of the amplitude of the post-synaptic excitatory currents change a little bit.

But here I have two other parameters. The first is called $\tau_{d2}$; I don't remember why I called it $\tau_{d2}$, or rather I remember it and it is another context, but it is the time, the time constant of recovery from depression. It is the time constant that returns from state $I$ to state $R$ and now is $290$ milliseconds. If I make it very, very rapid — think of the llama that spits, "sputters", and now has a capability to restore the vesicles containing the neurotransmitter very rapidly — you see that although I am asking the synapse to release, release, release at a frequency of $47$ spikes per second, the amplitude is not so dramatically different from the first compared to the responses to the other events.

And if I make it even shorter, even more rapid, in fact, it is a synapse... here maybe it gets pissed off, it gives me an error because at a certain point there might be somewhere a division by zero, somewhere there is an $e^{-\dots t / \tau_{rec}}$. So in a numerical simulation if I put $\tau_{rec}$ to zero someone has to get angry, has to get upset. Anyway, when there is no phenomenon, when the *recovery* is instantaneous, you see that the amplitude of subsequent EPSCs practically does not change. Instead, when there is a certain inertia, a certain dynamic, for example $190$ milliseconds ($100-150$ are typical biological values, at least in cortical synapses), there is this attenuation of responses.

The third parameter you can change is $U$, which was the **release probability**, or the quantity of vesicles that are released at every instant, at every event. And if I lower it dramatically, you obviously see that the amplitude decreases: I am releasing much less neurotransmitter, true. What happens is that, however, since I released less, little, I depleted the reserves little. In fact, you see that here the amplitudes are not so different. To see an effect with this value of $U$ I have to go precisely to a very, very rapid frequency and perhaps it is almost not seen, it starts to be seen here. If you see here I see it from the tail, that here the first and second are all interrupted, but it seems a little higher and then tends to decrease.

This can perhaps develop your intuition on the interaction between parameters, so what is the presynaptic frequency, the demand for use, the recovery time constant, the time scale on which recovery occurs, and the release probability. What for example is seen is that a particular interaction between long-term and short-term plasticity could change precisely $U$, it could potentiate precisely the parameter $U$. This would be the behavior before a long-term potentiation and, if I change $U$, I have again a kind... what is called a **redistribution of synaptic efficacy**. It is true that some events are potentiated, but others are much more depressed. To recover exactly the traces that I showed you in the presentation last time, in the slides last time, I would have to put inside also synaptic facilitation and all synapses probably show both depression and facilitation at the same moment, however, they have different parameters. Anyway, I hope someone can find this notebook interesting and useful.

# Extracellular Signals: Overview and Techniques

So, for this part of extracellular signals, the reference text is again this one here, *"Electric Brain Signals"*. I hope there are other texts, as I believe I said during the weekend or whenever it was in recent days in response to a colleague of yours: you can find part of the information (which anyway is in a very, very reduced manner compared to this book) also in this chapter on other texts.

I start by telling you that, to motivate the story of extracellular signals, not everything is intracellular, in the sense that not all current technologies allow measuring the electrical activity of one neuron at a time and on time scales that are those of milliseconds, so they would allow us to see single action potentials. Although this is a goal, the Holy Grail of the research field, for which I would like to be able to listen to the voices of the single units that make up a network and listen to the voices with good temporal resolution, so have a very low temporal and spatial resolution. So in this graph, which is a description of the resolution of the spatial scale with which different techniques are resolute, the desire would be to be here.

### Limits of Intracellular Recordings

In fact the *intracellular recording*, intracellular recordings with an electrode that is called *sharp* (pointed) or *patch* electrode, which does not necessarily go inside the belly like a knife but stops at the membrane (you remember, then a negative pressure is applied, etc., etc.), are in this part of this graph. In fact, they allow measuring events that are very rapid, of the order of a fraction of a millisecond.

I will ask you at the exam, roughly, how much is the duration of an action potential? What is the duration of an action potential, roughly? How much is the width? The amplitude you know more or less all. From the resting potential to the peak how much is there? How much? 100... 100, excuse me? 100 volts? 1000 volts, ok. And more or less how long does it last? Seconds, days, months, microseconds, milliseconds, nanoseconds? Milliseconds. And roughly? Four, one... about one. It depends on the cell, but typically it is a millisecond.

So ideally I would like to be here and I would like to be here also to see one. How big roughly is the belly of a neuron, the soma, the cell body of a neuron? They are micrometers, but more or less how much can it be? The order of magnitude? Tens of micrometers, yes. Different cells can be different and certainly, and today we will start to look at it, the dendritic tree can change things dramatically. In humans, a pyramidal cell of layer 5 of the cortex has an apical dendrite almost a millimeter long. In a research project that ended a few years ago and in which I collaborated, in which I was involved, it saw at least in rodents cells that projected from the thalamus to the cortex for several millimeters, because they had an axon that was very long, however the soma roughly is the one indicated.

So I would like to be able to play the keyboard or hear the single notes and the moment in which they are articulated, so I would like to stay here, both in space and in time, however, it might not be possible. An intracellular or patch electrode in the soma of a neuron, aside from being a thing that requires a research laboratory, an anti-vibration table, a Faraday cage, a piezoelectric micromanipulator, ok, it is unlikely that this is ever extrapolatable to a clinical context. But above all, once a neuron has a pipette inside the belly, there occurs — I have said it other times — a dilution of the cytoplasm, a dialysis of the cytoplasm, because the pipette is an enormous volume: you have to imagine it like the volume of a football stadium in comparison to a ball, for example. The ball could be the intracellular volume, the football stadium is the entire volume [of the pipette]. So there is certainly a diffusion and a *washout*, a dramatic change of the cytoplasm and therefore after half an hour, an hour the cell is dead. Certainly, even if I retract the pipette the cell is not that it is particularly well, in fact, it dies, after a while it dies.

So it is necessary to move also because if you have to have some diagnostic question you cannot think of having a neurosurgery operation every time, of exposing a part of your body; it is not feasible. So typically unfortunately one stays in this part: I am thinking of EEG, MRI, they are time [spatial] scales of centimeters, if not tens of centimeters and therefore it is extremely macroscopic stuff.


### Macroscopic Techniques and Temporal/Spatial Resolution

From the point of view of temporal resolution, I could have a macroscopic resolution like someone could be usually outside this room, could hear us talking, could... now you are silent, if you made noise a generic clamor would be heard and one could, ok, not distinguish single voices, but could trace it in time with great accuracy. And this is true for EEG: EEG is poor in space but good in time. Then interpreting EEG signals — we start in some way to play with it — is another pair of sleeves [another kettle of fish], it is complicated to interpret, to make sense of that, of the voices, of the confused clamor of a large group of people, of neurons. It could be confused, but I can hear it.

There are other techniques like functional magnetic resonance imaging (fMRI) that have a very poor temporal resolution, of tens of seconds, so forget that there is any correlate with the emission of action potentials; it is more something, as in the case of fMRI, metabolic, particularly due to oxygen.

There are other techniques here indicated as **intrinsic optical imaging**, which means that without doing anything, due to properties of oxygenated or non-oxygenated tissue, if you illuminate with a light source a part of the cortical tissue, you would see that it has a different optical reflection characteristic. They were pioneering studies several years ago.

In other cases, there are organic substances that change their conformation and start to become fluorescent when the membrane potential in which they are intercalated changes. They are called **voltage sensitive dyes** and inside a plasma membrane they have a lipophilic domain, so it loves to stay among lipids, so it intercalates in the membrane and feels the transmembrane potential and changes its own conformation depending on the electric potential. So in theory you could, doing with an imaging technique, see as an optical correlate little lights that turn on correlated with the membrane potential.

Another thing that happens, I don't know if it's here, yes, there is **calcium-based imaging**. Again, substances that can be either put from the outside or genetically engineered, so that it is the cell that expresses them directly as if they were its own membrane proteins, which are not sensitive to voltage but sensitive to calcium. When there is calcium they bind, change and fluoresce. But it is difficult that from the clinical point of view people inject these dye molecules *calcium imaging*, *calcium dye*, *calcium sensitive dye* or *voltage sensitive dye*. What is much more common is the part of electrophysiology, even invasive, but without the use of these compounds.

In this case, you would have with *calcium imaging* or *intrinsic optical imaging* you would have a very low spatial resolution, in the sense good, of the micrometer (you could look even at a single compartment of a very complicated neuron), and a temporal resolution however of some order of magnitude slower compared to electrophysiology, because these objects here are slow to react, or because intracellular calcium — now we know it because in particular for the story of frequency-dependent adaptation, the bank account, etc., I made that example also in that case — takes time for a molecule of free intracellular calcium to go away, to be extruded or internalized in the endoplasmic reticulum. So all this leads to having a slow temporal scale.

### Local Field Potential (LFP)

In the middle, there are the so-called **Local Field Potentials**, which we will deal with, which are a technique, an invasive measure and it is in some way electrophysiological, with an electrode, and depending on the type of positioning, also technology, shape, they have a temporal resolution that varies from a few milliseconds up to a few tens of milliseconds, or pardon, tens of seconds or minutes. They are relatives of the EEG and we will understand what they are. Just to make it brief, not all recording methods are the same. There are some methods that are for example based on magnetic fields or on positron emission, positron emission topography (PET), and they have advantages and disadvantages, but above all, unfortunately, they are located in a part of this graph that might be unfavorable to the characterization in detail of the electrical phenomena of the nervous system. Moreover, these temporal scales are multiple, they are heterogeneous and they are multiple.

Anyway, I wish to dwell on this part here, which are the Local Field Potentials, also called *low frequency part*, even if it is wrong, in the literature you don't find it yet. It means that compared to spikes — they are indicated here as *sodium spikes*, they are the ones we know, these are extracellular spikes, they are also called **multi-unit activity** — they are capable of... so Spikes and Local Field Potential are the type of signal, they are two faces of the same signal that you can measure if you insert an electrode in depth, or on the surface like the electrocorticogram, or on the scalp like the EEG. Anyway, what it is requires a description at different spatial scales and obviously the question remains on what it means that spikes have a frequency characteristic of the order of hundreds or thousands of cycles per second while Local Field Potentials have something slower. Beyond the definition that I say, that the Local Field Potential is the low-frequency part, what do they tell me about the biological phenomenon? Maybe if these are slow they are talking to me about synaptic activity, while these that are fast are talking to me about action potentials? All very beautiful but it is necessary to be able to understand it somehow, to be able to verify if this is the case.

***

# Relationship between Intracellular and Extracellular Signal

This is an example of what is, in an extremely intuitive way, the situation of the techniques we have seen so far. Normally we can, with intracellular access, access everything that can... well actually it is not even everything, anyway [we access] with maximum temporal resolution recording the so-called **sub-threshold** and **supra-threshold** activity. It means the synaptic activity, which I see here sub-threshold because the neuron is integrating it, and I also see the so-called supra-threshold activity because the neuron fires in some conditions and I see these signals that are much larger in terms of signal-to-noise ratio.

What I said anyway — this intracellular electrode is not the end of the story in theory, going in the direction of maximum accuracy — is true because the neuron is not a small ball, it is not a sphere. It has an incredible, in some cases incredible, dendritic tree and axonal tree; what is generally called **neurites** (means dendrites and/or axons, it is a generic term). These arborizations have — and we will see it today — they are not **isopotential**, they are not at the same electric potential. If they were, and the soma was too, it might make sense that I record in the soma: "Ok, what the synapse is doing over there, the compartment of the dendrite where the synapse is insisting, is the same thing". The potential instead no [is not the same].

As you can imagine, also for example the axon is a structure morphologically, electrically distributed — it is said **electrotonically distributed** — and would require having a recording mode with many simultaneous intracellular pipettes, or with many of these voltage-dependent dyes that are distributed over the entire membrane of the neuron. So in theory I could see it like a Christmas tree, I could see it in different points, I could see the activation, the wave of what is the electrical activity propagating in the same cell.

The problem is that those objects there could be toxic. You could do an experiment that lasts a few minutes and then there is a toxicity phenomenon that kills the cells; there could be light-induced toxicity (you have to have a strong light source and a progressive inactivation of these objects occurs which is called *bleaching*, like bleach, whitening, it is called **photo-bleaching**). So what might seem without problems, in reality anyway is an experimental technique that maybe lets you see, gives you the possibility to see a few seconds and then you see nothing more.

### Extracellular Recordings: Multi-Unit vs Single-Unit

The thing that evolved first, actually simpler even than intracellular recordings, is the technique of **extracellular recordings**, in which [one talks about] measurement of **multi-unit** activity. Which units? Neuronal units. "Multi" because evidently, like this microphone — you are not talking, but if you were talking, this microphone would hear multiple voices simultaneously — because in the extracellular space, to hear only the voice of one person, I should put it in your throat, and then there you would hear only my voice, the voice of a single unit (**single unit**). Which could be possible, certainly it is with an intracellular electrode; with an extracellular electrode it is more complicated.

Associated with this extracellular technique there are other terms — again I repeat it — these **Local Field Potential**, corticograms, electroencephalogram. And in the end you might think that from the extracellular point of view, in a very banal way, you hear only the echo from the outside and of the strongest signals. A very simple thing you could think of is that when a neuron, inside compared to outside, emits an action potential, it means that the inside becomes more positive than the outside. So it is as if positive charges here, which are for example sodium ions, entered abruptly because sodium channels open and therefore left a void of positive ions outside.

So outside you would see that the potential locally could dramatically change: it would change to negative. So the dual, the opposite compared to the intracellular action potential which instead is a positive deflection. But to make it brief, you would see only dramatic events. Because yes, it is true, synapses when they activate the neurotransmitter binds to the post-synaptic receptor, there is a charge flow (in the case of AMPA/NMDA it is a mix between sodium and calcium; it could be instead for GABA-A chlorine, GABA-B potassium; for metabotropic receptors I believe it is again sodium and calcium etc.), however these currents could be very small and so I, from the extracellular point of view, could simply not be able to measure them, or rather I measure them but they are covered by noise.


### Signal Filtering: Spike vs LFP

It turns out that an extracellular electrode yes "kind of works", but it makes you see only spikes; what you lose is the whole world of integration of sub-threshold synaptic signals. And so the question is: is it so? If a neuron never fires could I see nothing? In reality, things are even more complicated.

In the sense that if you take an extracellular electrode and put it in distinct points of the morphology of a neuron — now morphology is important, and it is important because it is something that is extended for tens of micrometers, up to hundreds of micrometers in the case of larger cells and larger mammalian cortices — you would have in a case like this (which is a simulation, and we get to see how this was done, of the extracellular electrostatic potential... I continue to call it electrostatic because we are not in a regime... quantities change in time, but we are not in a radiofrequency regime, we are not in a regime where Maxwell's equations would predict the generation of a magnetic field concatenated to the electric field; for this reason it continues to be called electrostatic case. It's not that static means necessarily *steady state*, it means at low frequencies and we will see it: the fastest things that are are action potentials, they have a Fourier content of frequencies up to $1000-2000$ cycles per second and although you can have radiofrequency emission at $1000\text{ Hz}$, one kilohertz, hardly this is electromagnetic activity, a true electromagnetic propagation).

To make it brief, in this simulation one wants to represent what is an **ECoG** electrode, so of electrocorticogram (so that is placed on the surface, so on the pia, a membrane that isolates under the bones the brain tissue from the rest), or above the scalp, an **EEG** electrode. What would it see when a neuron — and in this case I am not indicating what this neuron is doing from the point of view of synaptic inputs; I only know that intracellularly this neuron seems to receive synaptic inputs, integrates and makes a spike.

If I put an electrode close, extracellular, close to the soma, I see what I told you: I see a strong negative deflection. But careful with strong or weak. Here, as your colleague said, there is a bang of $100$ millivolts, which is a remarkable thing. Outside, despite during these huge bangs, I first of all see only... it almost seems that I don't see the subthreshold behavior, as I anticipated before. So it is as if there were a kind of **high-pass filtering**, like a kind of **derivative**. That is, it is as if I, if I take the derivative here, maybe I change the sign... clearly the derivative of something that is very very steep is large, a large velocity. You know that if I take the derivative of a certain quantity the absolute value has to do with the change in the unit of time, so the more something is rapid, the more ample it is in the world of the derivative.

In fact here these transitions, this dynamics [subthreshold] is rather slow, it is some ten, hundreds of milliseconds; here [in the spike] it is a fraction of milliseconds. That's why roughly I say: you must know that roughly a spike exhausts itself in the span of a millisecond, so if I ask you "but what is the *upstroke phase*?", you could say: "Ok, half a millisecond, $0.25$ milliseconds, a fraction of milliseconds". So outside I see a signal that is negative and fundamentally I see nothing of the sub-threshold activity.

If however I move the same electrode to different points, for example here, where presumably synaptic inputs were arriving... pardon there is one thing I wanted to say, careful speaking of strong or not strong. Here is millivolts; here it is true they are $100$, but they are **microvolts** ($\mu V$), so $10^{-6}$, so a thousand times smaller for a neuron. So tens or hundreds of microvolts, which is the current situation that for example we see in the laboratory when we use extracellular electrodes on top of which we grow neurons: at most we see a few hundred microvolts these signals.

If you move the electrode elsewhere (neglect the magnetic aspect, this is just to say that in theory with first principles I could also derive and understand, from the point of view not only electric, what happens), moving here, anyway the scale hasn't particularly changed. Actually this has become a fraction of microvolts, stuff that is a hundred or a thousand times smaller than what was the amplitude of the extracellular spike I saw here. Here however I see something that seems to be a kind of strange signal, which becomes first negative and then positive. Perhaps this thing here has to do, although it is extremely small, practically almost impossible to record... perhaps it is the extracellular echo of the opening of an AMPA channel, an AMPA receptor. It lets positively charged ions enter, so it leaves a negativity from the outside. I probably see this negativity. Here in the dendritic tree, at the top of the dendritic tree, I see a similar signal.

### Local Field Potential and EEG

Note: in EEG I can do the same thing imagining that, from the electrostatic point of view, to model the scalp, model not only the distance... here in the end we are in the world of electrostatics where it matters if I move away from the source: signals change as $1/R$, $1/R^2$, distances matter. If I have an electrode in the belly of the neuron no, because I record what the whole soma from the electrical point of view is doing. Here it matters. And if you see here we are at a fraction of **nanovolts**. And it is... I wouldn't be able to do as I would have liked during the first lecture (for a long time I tried with electrodes, the same electrodes, to put them on my forehead), simply it is impossible: there is too much electromagnetic noise. I told you about the famous $50\text{ Hz}$ and other intrinsic noises of the amplifier, so signals that are nanovolts... it is true, I don't have a single neuron, I have a few billion, nonetheless it doesn't matter, signals are obscured by noise. This would be the correlate of a single neuron.

It would seem interesting then that the spatial extension must be taken into account and something of the type: now that I know the excitable electrical behavior of the single neuron, perhaps I can combine it with what is a more classical electrostatic treatment. If I have a distribution of charges in space, then I can calculate what is the potential at this point, for example seeing all distances. Do you remember the electric potential? It was the weighted sum, weighted by distance, of single charges, it went as $1/R$, point charges. Here they are not exactly point charges because I am imagining precisely like here there are holes, pores, ion channels that activate sucking... they are like wells into which if I have a charge it disappears in there, or they are sources, they spit, they have a breath of ions, of charged particles, that is they are currents. So perhaps the treatment must be slightly more complicated, more refined, but it makes me think that if I know everything about this neuron, then I can predict what is from the extracellular point of view in different points the electric potential outside.

And this can be crucial to say: look that if you from the point of view on the EEG, on the scalp, are recording signals like this, look that it doesn't mean that neurons activated at this moment. Here one even sees the echo of the spike, but again it is a fraction of nanovolts and if you also have a billion of these cells, a few million maybe underneath (also because the human cortex is convoluted, so it is difficult that there is a flat part... this is to scale more or less, so this neuron could be accompanied by a few million other neurons nearby), it is unlikely that they fire at the same identical microsecond. In that case yes, you would have a summation of these little peaks and you would see them on the EEG. And, *by the way*, it is seen during pathological states of epilepsy, where spikes are seen at times... they are called spikes, not necessarily are they [intracellular] spikes. Conventionally this is called action potential and from the extracellular point of view it is called spike. Anyway spikes are seen because there is pathological synchronization.

### Interpretation of Raw Signal: High-Pass and Low-Pass Filters

So how do we extract sense from this extracellular thing? I show you before starting two characteristics. We concentrate on this part here, so the **raw signal** that you would see from a metal electrode of a fraction of a micrometer, so there is a metal part of a few square micrometers of surface exposed to the extracellular environment placed close to some neurons. This is the raw trace. Note here there is no unit of measurement but they are microvolts and you see that there are slow deflections and then every now and then there are little peaks. Leave aside now that these little peaks go up instead of going down as I showed you, it is not so crucial, here it is the technique, simply the recording technique that is different.

Here is just to say that if you take this trace and do a **high-pass filtering**, that is, in fact you take a frequency component (probably something that you are... tell me if it is something that you are not seeing with Gibertoni and Gibaldi... this could be crucial), you should do something like filtering. Here I don't want to throw away noise, I don't want to do a low-pass or high-pass filtering to throw away noise; I want to isolate different characteristics.

If in fact I throw away the slow components for free I have that this *baseline* that oscillates I don't have it anymore. In fact the filtered trace is not exactly the same (this anyway is a trace recorded in our laboratory): it is flat by definition because I removed the DC, I removed the Fourier components at very low frequency. In fact the trace doesn't... what remains are these very very rapid depressions, they are negative, they are spikes. They are phenomena that if I zoomed (and I will do it in the continuation) have a duration that is very similar, a little smaller than what is the duration of an intracellular spike. Well, thanks, they are the extracellular correlate, now I know it. But if I look, if I... so this is the theory, if I record a signal and I see it like this, I have to play it to be able to convincingly say: "No no, these are spikes of a neuron".

Is it only one neuron? Because I see here that there are events that are equally big and then there are others that are a little smaller. Maybe the electrode is here and I have a closer neuron and a farther neuron. The farther one makes sense that it might be attenuated in its effect it has on the electrode. While, so these high frequencies means between $100$ and $100\text{ Hz}$ or $100$ cycles per second and $5000$ cycles per second, so between $100\text{ Hz}$ and $5\text{ kHz}$.

And instead if I take with a **low-pass filtering** only the slow component I don't have the little peaks anymore, they are too fast, I removed them and a low-pass filtering only the slow signals pass. And you see that I have a signal that by definition, having filtered between 0 and $100\text{ Hz}$, $100$ cycles per second (I say cycles per second because Hertz at times can make you think of spikes per second and here is another world, here is the world of analog signals in which I am talking in effect of a frequency component in the sense of the frequency domain, in the sense of Fourier), and if I do it here by definition I have signals that have frequencies that are lower than my *cut off*, than my cutoff frequency which is typically around $100\text{ Hz}$.

In parenthesis if you go maybe to an epsilon to look at the introductory part of this course I told you something on EEG and I told you if I remember correctly that in EEG in a phenomenological way, in a totally descriptive way slower oscillations are described, oscillations a little faster, conventional names are given: gamma frequencies, alpha frequencies, beta frequencies. Do you have... do you remember something? For example **gamma** was between $50$ and $70$ and $100\text{ hertz}$, **alfa** and **beta** are associated with some sleep states, **delta** and further are slow, are between $0$ and $100\text{ hertz}$, if nothing else because they come out of such a filtering and so thanks, the signal cannot anyway be, since I threw away the higher frequency part it doesn't contain other oscillations.

This is what is called the **Local Field Potential** and these fluctuations, these frequencies, these speak of anyway neural activity, only that I don't know... while spikes are simple to understand, ok, there must be the soma somewhere and I am observing the result of synaptic integration, the emission of a spike. Here I have the impression that it must be something in a much coarser way, probably at the population level, not single unit, of maybe synaptic activity, however I have to try to understand it.

And *that's it*, here the part on which I wanted to insist is that LFP often, now one is starting to see, particularly from the didactic point of view, by definition one speaks of low frequency component, because Local Field Potential has implicitly the concept that it is an electric field, a potential of an electric field that I am measuring and that is local. Local where? Local in the sense of some ten, hundreds of micrometers around the electrode. Yes, but it depends on the electrode and perhaps, again, it is more correct to speak of the action that one had on the signal, rather than already jumping on what eventually might not be it, the spatial scale.

Here for the **multi-unit** or **single unit** activity... I repeat, I believe this is multi-unit because here units are more than one, these and these have distinct amplitudes. It is true that, you saw it also with the Hodgkin and Huxley model simulation, the same neuron could fire action potentials that were the first of the train, they could be very steep, the others could be a little degenerated. This *slope* tended to bend when I asked the neuron to fire in a very rapid way. And if in between $V$ intracellular and $V$ extracellular there is some derivative with respect to time (I have to show you because this is not...), if this is a derivative it could also be that the same neuron has steeper spikes, others less steep and I see it with lower or higher amplitudes depending on steepness. So there is intrinsically an ambiguity: is it the same unit that maybe fired twice in a row so the second spike is slightly less *steep*, less steep, more *sloppy*, more scarce? There it's not that I notice, amplitude could be equal but the rise is a little lower because voltage-dependent sodium currents are not so intense, because voltage-dependent channels, voltage-dependent sodium are not 100% ready and available, some are inactive and if in between there is a derivative, yes amplitude does [count] for me, but *slope* also counts for me, the inclination. So everything is: can I extract some sense on what I see extracellularly? Because like this it is a big mess, it becomes heuristic: "these signals are fast, so they are action potentials", but how many are they? Ok, it becomes a little complicated.

### Joao Couto's Experiment: Simultaneous Patch-Clamp and Extracellular

This is a heroic example, and I keep it because I am fond of it, of a PhD student who is now a researcher at the University of California in San Diego I think, or San Francisco, I don't remember San Francisco, **Joao Couto**. When we were in Belgium, heroically he put in the prefrontal cortex of an anesthetized rat not only a series of extracellular electrodes, but he even shoved a pipette, roughly — he didn't hit exactly the same, but roughly with an angle for which the tip of the glass pipette, a *patch* electrode in vivo, so further heroic — was roughly in the same region where the extracellular electrodes were.

And what you see is that here are multiple traces and they have artifacts because I don't have the right trace, so this is an image and I wanted to stretch it to be able to align it in an authentic way with what he recorded at the same moment, simultaneously, from the soma of a neuron. The soma of that neuron somehow acted as an antenna with respect to network activity. And I here... he, I did nothing, with electrodes 1, 2, 3, 4 (actually he had 8, here are 1, 2, 3, 4, they are metallizations on a kind of fork) recorded only fast events, because Local Field Potentials here were not particularly indicative. So he filtered only the high frequency part to show multi unit activity.

And you see that extracellular activity seems to be organized in packets. I don't know if to you or to your second year colleagues a few weeks ago I told about sleep, *slow wave sleep*, in which during sleep or anesthesia the cortex has a mode of oscillation between **UP** activity and **DOWN** activity: up, down, up, down. It depends on the anesthetic but in the case of ketamine-xylazine that is used in this experiment it is maintained as in sleep.

So this activity seems to be synchronous and seems to be in packets and intracellularly the neuron has this subthreshold activity that you don't see, you see only perhaps here the echo of what is the occurrence of activity seen extracellularly. When the neuron fires, this neuron fires, it's not that it fires many spikes in one of these *up states* (I call it up because the potential seems to be stably for some ten or hundreds of milliseconds, seems to be more depolarized, seems to be bistable, as if there is a synaptic bombardment that here is off, here is on, here is off and here is on). But here for example the neuron does not fire, it doesn't fire but someone else evidently  nearby fired.

So, in the end — and now we take the 10-minute break — it would be nice to be able to correlate intracellular activity (this was the soma), even intracellular activity in the whole dendritic tree, and give sense to what is extracellular activity. In this case probably they are four other neurons that were in the periphery that more or less did the same thing because all of them had spiking activity roughly at the occurrence of these *upstates*. I stop for ten minutes.


# Derivative Relationship between Intra and Extracellular Signal

Ok. Ok. Let's resume slowly. Ok so here is another example in which again in a heroic way, but a little less heroic than what was done in vivo by Joao Couto, two researchers of the laboratory when I was in Belgium and before that in Bern did however *in vitro*, so not *in vivo* but *in vitro* on a culture of dissociated neurons growing on a matrix of planar microelectrodes (**MEA**). Like your chairs, maybe I mentioned it and showed you a recording already, as your chairs where you rest your bottom, so neurons lean on a surface and lean in some points on an electrode. Electrode that was microfabricated with the same principles and technologies of microfabrication via microphotolithography. If you know how they are [made], what are printed circuits or even PCBs, *Printed Circuit Boards*, or chips, microprocessors or anyway very high integration systems are in fact with the same method. Here somehow, despite being small, they are however an order of magnitude larger. Each of these electrodes is of the order of $50$ or $30$ micrometers and the distance between two electrodes is $200$ micrometers.

And here you see a big mess in the sense that there are... what is said for other types of cells is called confluence, cells are packed so much that they touch, their soma touches, neuritic processes are not seen, axons and dendrites are not visible, they are immersed. After a while that they stay in culture, after about two or three weeks, these cultures, these two-dimensional networks, are very interesting because they allow studying emergent behavior due to connections. They have every now and then this activity that if you want is a zoom of what *in vivo* I described to you very briefly, *up state*, so this is probably... damn there is no time scale... This must be around $200$ milliseconds.

Again here you see the extracellular recording. You see that the signal is slightly different from before. Before somehow it seemed that the extracellular signal was a totally negative deflection. Now it seems to be something that is made like this. Or something that could be made like this. Excuse me, this goes straight down with these little humps following or preceding. This is a non-simultaneous recording, in this case, made by Dr. Anastasia Moskaliuk, who insisted saying "no, in my opinion this is not the behavior of a single neuron, this is the network". Then, to convince me that we were wrong, she did the intracellular recording and rediscovered that actually even single cells, single neurons that are around this electrode, have the same *up state* and *down state* behavior.

The interesting thing is that behavior seems to be not only an irregular emission of action potential, so very different compared to what we saw in simulations where applying a current the neuron behaves like a kind of pacemaker; here it seems to be a disordered, disorganized behavior, and then the sub-threshold membrane potential seems to be a **random walk**, a kind of stochastic process, a realization of a stochastic process that makes one think it is the integration of many synaptic inputs which however do not arrive in synchronous instants, instants at the same moment, they arrive in an asynchronous way, in a disordered way.

To make it brief, obviously, again, there are enormous differences. I here see only supra threshold activity, in the sense that I see only in correspondence of spikes. Here are $100$ millivolts of signal, here are some tens of microvolts unfortunately. What is seen here is that the amplitude of these extracellular signals seems to change, particularly during this initial phase. So here it could be that it is something that has to do with frequency-current adaptation. And then there are these two little dots that I highlighted for you where clearly there are signals that are completely different in amplitude. It could have been a neuron that was slightly more to the right, farther from the electrode and that fired on its own during this epoch, it is called epoch of synchronous activity. But it was more distant, I see it also here.

So this signal, then I should call it a **multi-unit** signal, is simply the part filtered at high frequency, of which Local Field Potential do not make particularly sense, do not have a particular content. Probably because it is a cell monolayer; in the case of the brain a block of tissue, whatever Local Field Potential is, probably is the effect of summation in 3D. Here in 2D the Local Field Potential doesn't contain details, you don't have particular events, you have it in the high frequency part and in this case it is multi unit activity because certainly they are more than one unit. So again here is filtered from $100\text{ Hz}$ or $100$ cycles per second up to $5.000$ cycles per second or $5\text{ kHz}$, while this, the intracellular part, is not filtered if not at $5\text{ kHz}$. And in both cases sampling frequency is of the order of $20\text{ kHz}$, $20.000$ samples per second, which is information that has to do and doesn't have to do with frequency content, I am thinking of Nyquist sampling theorem.

This is another example, a zoom of the shape of the single event, which here is — since the trace is zoomed to show, I repeat, here it will be $200-300$ milliseconds, the event is too fast — here is a zoom of single traces. This electrode records these events, this electrode here instead you see records another one whose shape has a subsequent little hump as I was saying. So the question is why one has a little hump and the other doesn't? What does it mean? These are $10$ microvolts, again these are $100$ millivolts.

### Extracellular Electrodes and Derivative of Intracellular Potential

And this is a further example in the literature of an article that we resume now in a couple of slides in which intracellular potential was measured and the same neuron had an extracellular electrode simultaneously in its proximity. And it would seem therefore to be a temporal relationship between when intracellular potential stops growing, so the point where the first derivative becomes zero, and the extracellular signal hasn't exactly gone to zero, but it became very small. Before it was negative — aside there must be a minus — this was a growth and then a decrease, here the signal becomes negative and then positive. If there is this thing that extracellular signals are the **derivative** of intracellular signals... again I don't understand why they should be the derivative. The only thing that does the derivative is the capacitor, the capacitance, so it is possible that here this capacitor is responsible somehow for the fact that from outside I hear a signal that is not direct coupling DC but is an AC coupling. In fact in electronics when one couples a system with a capacitor in fact cuts the continuous, is doing a derivative operation.

So the idea is: how do I make sense of this behavior? The key to everything is that dendrites, it is not a big discovery, probably I repeated it other times during previous weeks, **dendrites are not isopotential**, they are not isopotential portions of the same neuron. The fact that neurons evolved to be spatially distributed can be read as having a meaning, an evolutionary reason why it is necessary that electric potential here, at the Soma, be different from up there where I have dendrites, where they fish inputs, and maybe also different from my axon that is projecting somewhere. It might make sense that there is the need to have signals propagating in time. Delays? Maybe it is necessary to do information processing that there be delay lines coupled as in some computation paradigms? Another possibility is that this is simply an epiphenomenon, no one yet knows with certainty, the fact that a network can be wired effectively.

Keep in mind that in a little piece of cortex of a centimeter on the side there are, I believe, millions of kilometers of cables, of wires. If neurons were point-like it would could have been very very complicated for all connections, simply from the point of view of volume, to attach, attach one neuron to another. If I have instead a structure morphologically, geometrically, spatially distributed, I have many different points to accept inputs. Again, this makes me think... here there are no trees outside... trees also have this behavior, in fact they are arborized, they are called dendritic arborizations (for this also dendrite means from Greek tree), they are probably like this because they have to compete with neighboring trees to catch light, they have leaves and branches extending high to take, maximize surface, take light. Here maybe they have to maximize and take synaptic inputs, or both things. It may be that this is yes a *leftover*, something that evolution left as a side effect, but since the neuron is there it might be important that it has an electric surface, electrically active, that can integrate information.

Imagine, and we will see it, if synaptic inputs arrive in a distant way or in a close way. Maybe they have a different effect in recruiting the neuron. Some could be the famous tiger entering the lecture, sensory neurons might want to project close to the soma, because maybe their effect could be integrated much more rapidly in a much more effective way. If they are far, in the end if they behave like cables, cables with distance attenuate, in the end they are pieces of resistance, it may be that they have perhaps less sense, there is information that is worth having immediately and other that can be neglected.

### The Paradox of the Point Neuron and the Need for Distributed Models

Anyway the fundamental point is that they are not isopotential. So this is an animation, a stupid *sketch* that I showed you already in which the intracellular part, the extracellular part was described as a resistive continuum and we still keep this that is a resistive continuum. Now if you give me the possibility to make an exception: in the extracellular part, which is also a resistive continuum, is not all at the same electric potential, however for the moment here I will put short circuits, that is zero resistances, later I fish them out these resistances. Here there are capacitive properties of the membrane, of the lipid bilayer and every now and then there are channels. Here, ok, there would be Nernst reversal potential, but now it is not so crucial. What I did last time is say, since the resistance of this pore is much greater than cytosolic and extracellular resistance, I ignore the others, I put them to zero.

This time I cannot do it. Outside I do it, I repeat, then I remedy this, but for the moment it presses me to say that it is not such a good idea, since these cables have a rather small passage section, perhaps with a lumen of a fraction of micrometer or a micrometer (depends on dendritic tree and various branches, it may be that with various branches diameter becomes smaller and smaller, up to a considerable fraction of micrometer). And so it might be that at least in this type of **longitudinal resistance**, which is exactly that, is not transmembrane but is longitudinal, it counts if I have a piece of dendrite that is long some ten or hundreds of micrometers.

So if I leave these resistances, you see well that if there is a resistor by Ohm's law, probably if a current passes the potential here is different from potential here. So if these are two points of the membrane and I indicate, touch the capacitor because the capacitor is what gives the state variable, gives memory, they are a kind of reserve, of charge, of *reservoir*, not reserve, of container, of accumulator, here. And since potential at this point and at this point might not be the same, here it is that here and here are two pieces of the same neuron that have a different transmembrane potential. This is not a problem, it is a complication, and it is fundamental to take this complication to understand this thing of extracellular potentials.

***

# Buzsáki's Experiment: Relationship between Intra and Extracellular Potential

This is another example, again in a heroic series of works published by **György Buzsáki**, researcher of international fame, Hungarian working in New York for very many years. In particular in the hippocampus, already more than twenty years ago, twenty-five years ago, he had tried to approach the same problem. Here, you see it, this very thin line is the tip, is the lateral section of a cone: it is a pipette, a so-called *sharp* electrode. I always indicate it to you like this, but actually it is quite long, it is stuff that can be several centimeters, whose final part passes from a millimeter of diameter to a micrometer, even smaller in diameter.

If you put it in a tissue — I believe this is a *sketch* to scale however appropriate — here you see a little cell, which is a pyramidal cell of the hippocampus, of CA1 of the hippocampus (it is called an anatomical zone that is called *Cornu Ammonis* 1). In this case the tip of this very thin pipette — which is that pipette there, but put to scale, to scale on something that is a few millimeters, 2-3 mm (these are $250$ micrometers, so four of these make a millimeter) — is practically invisible, but it was inside the soma of this cell. And in its vicinity, heroically, these put a **tetrode**.

A tetrode is an extracellular electrode composed of four wires (here it says three, but *tetra* stands for four) that are more or less as when you braid hair: they are intercalated and wrapped around each other until having the final part without insulation. So the tip of this tetrode — which I should have shown you but I don't show you, which here is compared to a little cylinder — in fact has one or more metallizations close. They behave like, again, extracellular electrodes attached to an amplifier and allow measuring electrostatic potential at that point.

Which is again this very rapid event. These are two milliseconds, so this is faster than a second... excuse me, than a millisecond. This is $2$ milliseconds. Here is... ok, this perhaps I should define when I take width: one possibility is taking **half width**, which means *half amplitude width*, that is I take maximum, take this distance here, divide it by 2 and at this distance I measure spike width. And roughly if this is 2, it will be a little less than a millisecond.

Beyond this thing here of temporal scale which is useful, again, this is here spatial calibration... excuse me, vertical: it is $15$ millivolts for this graph here, while it becomes $30$ microvolts for the graph below. But this we knew, these signals are very, very, very small. And it is so clean because the neuron, since it had a pipette inside the belly, was made to fire several times in such a way that they could do an arithmetic mean of multiple repetitions; for this it is so clean.

### The Derivative of Intracellular Potential

The thing these did, that Buzsáki did for the first time, he said: what happens if I take this trace here [intracellular] and I take the **first derivative**? Taking the first derivative of a trace like this is very easy, it is not a mathematical function for which you have to apply rules. If you have a vector on Python or on whatever is, and you have many numbers in many little boxes (all these are samples, for example: $-65, -65, -64, -64, -63, -5, +20$... I am dramatizing), you have these numbers placed here.

So if this is $v$ and it is a vector, in some programming language like Python it is indicated like this, `v[i]`, and one puts here an index and this index is an integer variable that says where you are. To do the derivative it is enough that you do:
$$\frac{v[i+1] - v[i]}{\Delta t}$$
And if you want you can divide by $\Delta t$, by time, by sampling interval. I told you that roughly these signals are for example sampled at $10, 15, 20\text{ kHz}$, so you could divide also to have millivolts per millisecond, but if also you don't divide in the end it is only a scale factor.

It seems to me that in MATLAB there is a command called `diff`, so if you do `diff(v)` you automatically have this operation, point by point. And another way to do it is then doing `v[2:n] - v[1:n-1]` (with MATLAB, in which basically you use this that is called **slicing** in jargon, that is allows you to take a subset of the vector). Here from point 2, imagining they start from 1 as in MATLAB (even if it is not a great practice), from 2 to $n$ minus the same vector however *shifted*, which in fact starts from 1 to $n-1$, so that I have this difference always, difference between neighboring points.

I obtain a signal, the derivative. The derivative is this dashed one, this *dotted*, which is not so distant [from measured extracellular trace]. Here is a little... seems a little anticipated, but it seems it gets it quite a lot. So this is one of the first times in the 2000s in which people said: let's go understand the origin of extracellular potential, focusing on the spike, because it is a large signal, because it is easy (easy relatively, but certainly it is easier to have a pipette in the soma, which is relatively large, even if here it was placed randomly and it is easy to record in proximity).

### Physical Relationship: Currents and Kirchhoff's Law

So there would seem to be a relationship with the minus sign between extracellular potential ($V_{ext}$) and intracellular potential ($V_{soma}$), with minus. Again this makes intuitive sense because, I repeat, during a spike sodium from outside enters rapidly inside leaving a negativity outside, leaving therefore... disappearing positive ions inside [of outside].

Some of you might feel a bit resentful to say: "But how, darn, I am an engineer and if you love circuits say: ok, but here this current enters inside, but somewhere it has to exit, it's not that circuits are hanging". So the circuit is called circuit precisely because it is closed. So if here I have a sodium current entering, somewhere [it must exit], because otherwise it is not a circuit, I don't close the circuit, a current cannot flow remaining hanging. By definition of **Kirchhoff's law**, if I have a wire like this, current is zero, because if I take a surface around this wire, summation of currents is null; the only current entering or exiting is one, but it is that one equal to zero.

This is also the reason why little birds don't die on a high voltage cable. This is a little bird and if I do Kirchhoff, sum of currents, $I$ equal to zero. Yes, it is true, they have two legs, but this we see another time.

### Variations of Spike Shape in the Train

This is the same trace in which they show that, what I was saying, if by chance the shape of action potential by chance changes in a train of multiple spikes... You see that here are intracellular potentials: the first is this one which is the most hyperpolarized, the most polarized and the fastest; the others sitting on this *plateau* of depolarization are a little different, are in particular different both in rise phase and in descent phase. So if there is something that has to do with rise phase and descent phase, maybe it makes sense that in previous graph true trace of extracellular potential results a bit *shifted*, since here it seems that this part here is also *shifted* in time.

To make it brief, here they continued to support the fact that even not only in an isolated spike, but in a train of spikes in which it is known that membrane potential changes, action potential changes a little shape... First spike, second spike, third and fourth seem to be very correlated — here is amplitude, pardon, this is yet another thing — seem to be correlated with what is the **derivative** of intracellular potential.

The derivative, if I write it like this, obviously this is a quantity that grows the more ample is this somatic action potential. This I remind: if I have a function I call $g(t)$ and its derivative I call $dg/dt$, if I multiply this by 35 or by 63 or by 121, derivative of this new function that has this constant before is the one you had before times 35 or times 63 or times 121.

So the fact that these researchers see that when intracellular spike is more or less ample, also extracellular spike is more or less ample... so there is, when every dot is an action potential, and if I put it in a *plot* like this: this is measure of action potential recorded extracellularly and take this as amplitude (clearly microvolts... ok here is millivolts because 0.04, they are tens of microvolts actually), here instead intracellular amplitude (in fact are tens of millivolts), here points align in a cloud. For which it is not necessarily that there is a determinism, a perfect and not noisy relationship, but it seems there is a correlation, just as I would expect if it were a derivative. So derivative of a constant times a function is constant times derivative of function.

Then obviously there is a question of **slope** (inclination), because derivative... this is seen well for example if you remember graph of a line that grows. For example if I have this function of time $g(t)$ that is $\alpha \cdot t$. If I... so this $\alpha$ is angular coefficient: the more it is large (if positive), the more this line passing through origin tends to be steeper. I know that when I take derivative of $g$ with respect to time I obtain $\alpha$. So it is not surprising this thing that when these objects, these waveforms, are steeper — there steepness I simplified it, I extracted it to max, and it is even angular coefficient of that line — but steeper lines have larger derivatives. Here again, spikes having action potential, having steeper rise phase like the first, have wider extracellular amplitude. So it would seem that this thing of derivative adds up.


***

# Limits of Point Neuron and Introduction to Cable Theory

However this discourse, if one wants to make it slightly more quantitative, does not stand if we have, if we consider neurons described with a point formalism. This is the Thévenin equivalent circuit type model we saw, in which for simplicity, to show and start insisting on the role of what is synaptic input, I put a further arc due for example to AMPA synapses. So here they have again their Nernstian equilibrium potential, here they have conductance, permeability of that post-synaptic receptor and when this opens it lets pass a current in agreement with what is electrochemical potential of ionic species to which post-synaptic receptor is permeable.

However, if I consider also this structure and say: "Ok, here I see it this thing I described to you before", I imagine neuron surface as a membrane with many pores and these pores can behave as sources or wells. They "sputter" ions to me, I see it more as a flow of ions — we called it flow, a current, an ionic current — but if I do so for electrotechnical reasons, in fact here in extracellular space output and input of these currents is at the same identical point: circuit is closed here in extracellular space.

A **point neuron** has pores, wells and sources practically coinciding; I wouldn't have here... I between these two points don't have any potential drop, I have zero. I would like it to be extracellular potential. I think it must have to do with this current exiting and maybe then re-entering, because it has to re-enter at a certain point, but in *point neuron* there is no way, no possibility, because these currents cancel, yes, but at same point in space, so no way.

If I start however to consider a spatially elongated structure, distributed, I am thinking of a model that is a kind of what is called **Ball and Stick**. So obviously it is extremely crude stuff, and actually not only is it crude. Now we will see that here this dendrite I can describe it accurately.

### Cable Theory: From Lord Kelvin to Wilfrid Rall

Maybe I mentioned it during introductory lecture: we will do it with an equation that is same equation of electric cables or transmission lines. And it strikes me because it is same mathematical formalism that in Nineteenth century **Lord Kelvin** used for oceanic cables, transoceanic for telegraphic communications. And it is surprising: but how, we are in 2025, it is 200 years later and nonetheless that math works. Here you don't see any cable, I am simply imagining there is a compartment, like graph I showed you before, in which I asked you... forgive me, for moment let me simply say there exists an axial resistance, cytosolic, between a piece and another of membrane.

Here this is soma and this is dendrite. If you want it is very similar to what we did when we saw electrical synapses. There were two different cells, here is same cell and there is no connexin, here there is simply a hole: that is this soma is in direct electrical contact, ions can move and can enter and exit from this other compartment. And this other compartment is made of membrane and therefore from here to outside — inside, outside — there are capacitive properties, there are resistive properties (because evidently there are membrane channels), only I repeat that here is sufficiently long to not be able to assume anymore that it is isopotential.

Here if ions move they encounter a resistance; distance is sufficiently large not to be able to be ignored with a fact of proportion: "this as function of this, this is a much larger [resistance] and so that one I neglect". No, I cannot neglect it anymore because object is large, is large and long some ten micrometers, some hundreds of micrometers. There must be some property that tells me if a thing is long or is not long and we will see it later: comes out of cable equation.

### Distributed Models and Axial Resistance

And doing so I can write... beyond fact that here I should write in theory a differential equation writing $C \frac{dV}{dt} = \dots$, and then so I would write a differential equation probably with two derivatives, or a second derivative, so I would write a system of differential equations. It is not anymore as trivial as $C \frac{dV}{dt} = \dots$, it is not anymore a simple charge balance equation. But for simplicity, in this extremely simplified scheme — and simplified, I repeat, because actually here I should describe it with a cable, not with another what is called compartment — I simply write charge conservation, that is **Kirchhoff's Law** of currents, for this node, assuming that extracellular space has its own resistance. But this is not enough for me alone, there must be also this [internal resistance] to have this diversity between soma and dendrite.

For continuation allow me to put aside for a moment this second component of theory of volume conduction, because we must surely develop first an *insight*, an intuition, or quantitative considerations on this dependence on space, on spatial coordinate, on shape, on position in dendritic tree in 3D, of what are electrical phenomena linked to excitability — actually in general electrical phenomena in case of neurons — and in fact we approach what is called **Cable Theory**.

Surprisingly it is not such an ancient thing, it is not so ancient the use of this formalism, of this similarity of cable equation in a neuroscientific ambit. Now, this **Wilfrid Rall** [transcribed as Rohl], who is father of this formalism, worked on this theory during '50s and '60s (I believe he passed away very few years ago and he is a giant, you will guess why), and his motivations to use description of cable theory — in general to describe mathematically neurites, dendrites and axons as objects that are spatially long — was not to understand origin of extracellular potentials, understand why they have a hump up, understand a hump down or in general why... this we won't get to, but why if EEG does like this or if does like this has to do with synchronized or non-synchronized population activity (so *slow wave high amplitude* or *high frequency low amplitude*, but anyway doesn't matter).

His motivations were that actually most of synaptic input currents are not at SOMA. Biologically neurons do not receive all synaptic inputs at soma; they receive a part certainly, but not a considerable part, unless exceptions. So somehow possibility to understand how change currents generated by a synapse that *impinges*, that insists in a distal point and somehow evidently propagates electrically up to soma (me where I have an electrode and see an excitatory post-synaptic potential, inhibitory post-synaptic), is worth understanding how it works.

And another motivation is that — this is more similar, more realistic, more linked to aspect of generation of extracellular potentials — is that anyway currents, I repeat, must be... to be able to be a circuit, must be closed, so these currents must anyway flow outside membrane of a neuron to be able to close circuit. So somehow, despite being extremely convenient, useful, particularly precious in some contexts (in which you will see it those who will do neuro track in a course with me next year, population behaviors have some type of very simple description when however complexity of space is thrown out the window), however in current considerations, and I showed you simply with generation of extracellular spike, a *point neuron* leads nowhere.

### Attenuation and Propagation in Dendritic Cable

Intuitively this Rall and colleagues had idea that if synapse could be conceived as a current source (you know that actually it is a conductance change, that changing conductance lets flow in accordance with an electrochemical potential for ionic species to which post-synaptic receptor is permeable, a current, but it's not that it is a current generator), but imagining also that it is in simple way a current source: so synapse activates somehow and here by magic flows an electric current. You could think that, always for this reason of circuit closure, it's not that this membrane is impermeable: there are lots of ion channels, maybe most are passive (that is to say are not voltage-dependent or ligand-dependent).

And so if electric current is very intense here, as it moves, as it flows along lateral section of a cable, of a dendrite, it becomes smaller and smaller because in distinct points (here I dramatized it and put it in discrete points), for charge conservation, for Kirchhoff, if a little bit of current escapes out, this that remains must be original one minus the one that escaped out. You can think of it as conservation of mass in case of flow of a torrent that has effluents, so has losses, or a pipe that has losses. If pipe has losses it is clear that downstream you will see a quantity, a flow — I must say pressure or velocity, I don't know, I have to think about it — anyway a quantity that is reduced because part of mass was lost before.

So here intuitively if current changes it means that this structure spatially behaves differently. Note: current escaping here could be a fraction of current passing and if this current here is smaller (little arrow is smaller compared to little arrow here that is nice thicker), it may be that from electrical point of view, Ohm's law through these channels, potential might have a gradation, might not be same in different points, precisely because current is not same in different points.

So armed with this intuition, let's look at spatial properties and start reasoning about fact that intracellular medium, cytoplasm, must have some **axial resistance**. So not only a transmembrane resistance or conductivity, but also inside axis of neuritic structure there is a certain resistance or conductance (it is same thing), and it is important that, as said in first models, the membrane not be clumped in a single point, but be a distributed quantity. So I don't have a membrane capacitance: I have a membrane capacitance here, then I have a resistor, then I have another membrane capacitance here, a resistor, a membrane capacitance. This is the cable equation. We see it.

In particular this made sense above all in the 60s and even more so today because the shape of dendrites is not so uniform in the nervous system. There are dendrites like those of **Purkinje cells**, the most important inhibitory cell of the cerebellum, that have a frightening complexity, not only because it is a tangle of arborization of the same cell, but because if you could put yourself in the plane of the slide, you would see that this arborization stays only in one plane: it is like a hand, my hand here you see it extended, but from the side you see it thin; it is not a bush, which is an intricate crochet work that is only two-dimensional, so it is as if really I put myself like this, from the side, and I see it thin. Other types of neurons have different morphologies, so somehow I expect that the spatial properties of a Purkinje cell here, here and here, are very different from this cell that probably is a stellate cell of the cortex which instead has a very different dendritic tree.

So both the fact of the impact of remote dendritic activation, is important to understand what is the impact on the soma. I repeat, it could be that in this big mess a synapse that is here or is here or is here behaves very differently from a synapse that is instead proximal, a somatic synapse, close to the soma. And in particular, what does a synaptic input do here? Maybe it doesn't make a neuron fire, maybe intuitively being distant it attenuates a lot... but does it attenuate or does it not attenuate? Does it attenuate and change shape? What hopes do I have to understand something of the nervous system if I record always from the soma and see events that maybe have a different shape? It is the echo of a filter: in the end we are engineers, so as soon as I start talking about cable, as soon as I start talking about capacitor, resistor, capacitor, maybe in someone's mind says: "Ok, I don't know, it will be a filter, it will be a low-pass filter, maybe distributed". But every time I have a capacitor with a resistor somewhere it is a filter, it is a low-pass filter, maybe elongated.

What is seen is that — so the result is — not only dendrites are not isopotential, electric potential attenuates from synapse to soma. Actually it is also interesting that if there is an action potential in the soma this can **back-propagate**. I am not talking about active propagation that happens in the axon and of which we don't speak, but of the fact that a potential in the soma can propagate towards dendrites attenuating. In the end a cable is a bidirectional thing: I can talk from one side or I can talk from the other side, it would seem that it can be symmetric. Electrical engineers in the room would call this duality principle? No, **reciprocity**, it is a property of reciprocity. If I inject in one point and record from the other I should, exchanging generator and observer, have the same phenomenon. Actually it is not perfectly reciprocal, we see why.

Moreover there exists a kind of propagation time so that a distal post-synaptic potential reaches the soma. Careful, I call it propagation time, but it is not a propagative phenomenon like electromagnetic waves are or like the propagation of an action potential along an axon. In that case, when one says propagation, one has in mind the wave equation, which I won't talk to you about, but you have a waveform, a signal, that is always in time, is always identical to itself; if recorded in another point at a certain interval later, it is still another point, it is always the same waveform. In the case of the axon it is effectively a propagation, because there exist ion channels, voltage-dependent, sodium and potassium, along the entire extension of the axon cable. This has as result the fact that potential continues to self-regenerate. But we don't talk about it because the most basic part, most elementary, most to be understood in a way not necessarily only numerical, are passive dendrites in which there are no active conductances, there are no voltage-dependent conductances. But there is nevertheless a kind of propagation time, so not only activity attenuates, a distal synaptic potential attenuates, but it takes time to interest, to invade the soma. And the shape of this signal, so from the site where it was initiated up to the soma, changes and changes with distance. So in theory if I see it at the soma I could understand from how far away it happened depending on how much its shape is deformed. All things that are of unheard-of power, very interesting, if one knows the mechanism with which this change of amplitude, time and shape are generated.

# Spatial Discretization and Specific Parameters

So, this is the first impact with the cable equation which in fact represents a spatial discretization of what are properties of a continuum. I call it continuum in this case because I am thinking simply of capacitive characteristics. The membrane of this cable, of this dendrite, besides the soma if you want, is a collection of a phospholipid bilayer. Yes it is true phospholipids are molecules that are discrete but I can think that they are so small and so numerous that between two points... in two points taken randomly I can continue to have that comparison, that parallel with the parallel plate capacitor. However, thing that for example I cannot do with transmembrane conductances (because there I already spoiled you, we already talked abundantly of Neher, Sakmann, of the discovery of the discrete phenomenon, there it is a discrete phenomenon, channels are not continuous, are here, here, here, maybe there are lots of them), to make it brief, the approach that is done — and is done also with the cable — is that first to pass into discrete, that is to say to think, as I did a little ago for a dendritic compartment and a somatic compartment, I say that membrane is not a single capacitor with in parallel a resistor (yes there should go Nernstian reversal potential), but is a combination of blocks, of these cells.

Excuse me, here intracellular part is below, *inside*, intracellular cytosol. In fact I was saying: "but why aren't there resistances here?". Ok, whatever, there is resistance outside, it is thought that the medium is isopotential. Then we will see that it is not so, otherwise extracellular potentials that I showed you before, in one point were such, in another point were another value, another waveform. But at this level what I need is to emphasize the fact that axial intracellular resistance is not null. And you see that this little block repeats.

Here it took Lord Kelvin to establish something like that for an electrical conductor, in which electrical conductor had... actually had a conductor and had a *shield*, a shielding, and so there were capacitive properties between conductor and insulator. As now if you cut a piece of wire, you see most insulated wires have a metal mesh around, ground, with respect to which signals are referenced (for example I am thinking of a USB cable; actually inside USB cable you have 5, 6, depends on USB cable, anyway you have multiple conductors). So between conductor and mesh around you have a capacitive effect and also potentially a conductive effect at a certain frequency. And inside you have a trifle [negligible resistance]. Here it is only in biology that you have a resistance between inside and outside, otherwise you would have a short-circuited electric cable. Kelvin had capacitor and resistor alternating like this.

### Specific Membrane and Axial Properties

This structure we define with accuracy, in particular we talk about properties of capacitance, of transmembrane and axial resistance, referring to **specific** characteristics, because it is convenient, now you will see in what sense. In general, I for example told you that a value that you have to remember, otherwise you fail the exam, is that **specific membrane capacitance** ($C_m$) is **$1\ \mu F / cm^2$**. So I am interested, because I don't want for the moment to make a commitment on what geometry is, I want to have magnitudes that are specific, which means they are independent of choice of geometry, for the moment. Shortly, clearly I instantiate them, because once I did discretization I have to go down: "ok, but how big is this, how long was this little piece and how much is lateral surface?". So, first element is capacitance, one microfarad per square centimeter.

This is another value, **membrane resistance** ($R_m$), that I sold you always as membrane conductance, but it is the same thing. The only thing to pay attention to is that if conductance like capacitance tended to increase the more surface was large (surface in this case is a cylinder, so the more lateral surface of this cylinder is large, the more pores there are, so the greater conductivity, conductance), if I — and now you will realize, simply for mathematical convenience it is better to reason in terms of resistivity... pardon, specific resistance — you see that this is not divided by square centimeter, it is **times square centimeter** ($\Omega \cdot cm^2$). Thanks: conductance was... tended to increase, was divided by square centimeter [Siemens/cm²]; since resistance is inverse of conductance, also specific resistance is inverse of specific conductance. Here millisiemens becomes kOhm and what was divided by square centimeter goes to numerator becomes times square centimeter. This is the only thing that requires attention.

Instead this number I never told you, it is **axial resistance** ($R_i$ or $r_a$) and has as value, again, $200\ \Omega \cdot cm$ [Ohm times centimeter]. Note, this is a transmembrane resistance, the more surface is large, the more this resistance is small; while here the more cable is long, the more resistance is large. This does not have to do with currents crossing the cable, but those flowing in cytosol inside. For this you see that it depends linearly... has as unit of measurement... so linearly in length, not in surface. But still centimeter is unit of measurement, but here square centimeter is because surface is lateral, here is section, longitudinal length [volume resistivity].

I point out to you however that the famous $RC$ continues because specific characteristics per unit of surface simplify, and it is right that it be so. If I do $C_m \cdot R_m$, square centimeter and square centimeter cancel and numerically I obtain what is time constant of a membrane that I told you to be around $20-50$ milliseconds for most neurons. So these numbers are not up in the air, they are experimental measures that somehow I give you to fix ideas.

What we do is that morphology even complex of a neuron can be approximated to a kind of combination of pieces of cable, pieces of cable that have a different length and a different diameter. So this part here for example is relatively short but has a large diameter; where there are bifurcations, ok, electrically it will mean that the piece, the start point of a cable — there will be boundary conditions, now we see what they are — coincides for example in terms of current or in terms of potential with end of previous cable. So a structure morphologically even complicated — ok this is not particularly complicated — translates into a sequence of cables. So in the end this is interesting because here is a two-dimensional stuff. Instead in case of cable and of a combination of cables, ok, I have to say on which branch I am, but I have only one linear coordinate, one, a single value. This means that equations I pull out are one-dimensional equations. Yes, it is an ugly partial differential equation, actually it is not so ugly because it is same equation of diffusion and of heat propagation. So perhaps some of you have already seen it: particularly electronic engineers do in semiconductors equation of charge carrier diffusion and in heat dissipation in electronic devices do heat equation, of heat propagation. So here it is convenient for me because it becomes one-dimensional case.

So I know what are specific properties: capacitance, transmembrane resistance and axial resistance. If one gives me: "take this little block with this surface, with this length", it is not a problem. Capacitance of that piece I obtain multiplying this by surface. Here I obtain resistance dividing by surface and here I obtain resistance dividing by length... so you have to give me diameter from which I derive lateral surface. So suppose one gives me radius, $2\pi r$ times length $L$ is external surface of cylinder and $L$ is length; so I divide here, divide here, multiply there. I can pass in this way trivially from specific magnitudes to total magnitudes, total capacitance, total resistance, etc.

I take therefore a little piece, I call it... for moment it is not infinitesimal, I make it become infinitesimal because otherwise one doesn't have fun, derivatives don't appear, there are no limits and difference quotients. Because there we go to parry always; for some reason, maybe I was saying it at beginning of this course, nature has always... physical laws have a structure for which it is easier — at least in classical case, I don't know quantum mechanics — description is through laws that say how things change in time and space. See Newton, see electromagnetism, etc. Here I do same thing. I take a little piece that is long $\Delta x$, has a diameter $a$ (so I area know how to calculate it, pardon, radius is $a$, yes), is $2\pi a \cdot \Delta x$, is lateral surface, because it is there that is piece of membrane with phospholipid bilayer; instead of being made as a sphere it is made as a cylinder. Inside is cable. We will see what happens at sides, caps, there will be caps, but for moment it is cable, so capacitive properties are of lateral surface. You could also think that $a$ is very small compared to $\Delta x$, so effectively *aspect ratio* — I don't know how one says in Italian, ratio between length and height — favors length; in fact neurons are objects that seem long, don't seem squat and fat cylinders. Another thing I need is passage section, which is simply $\pi r^2$, $\pi a^2$. And at a certain point I say, since it changes with position, it is not enough for me anymore to call it membrane potential at time $t$, I have to call it at point $x$ and at time $t$, in most general case possible: $V(x,t)$. Why? Because it is a function of $x$ that potentially changes in space.

So here I... excuse me I said a bullshit, for part of axial resistance I don't divide by $L$, by length. It has unit of measurement ohm times centimeter because if resistance is resistivity times length divided by surface or divided by area, $\rho$ becomes ohm times... this brings it to numerator, becomes, suppose, square centimeters divided by centimeters. So there continues to be both a dependence on length and on surface, on passage section, but dimensionally it is for this that it depends on ohm times centimeter ($Ohm \cdot cm$), because so it is not only how long you are but how wide is passage section obviously. So I apologize, here "ohm per centimeter" had made me think that counted only length, no counts also... this is correct formula. Yes, since these here have same unit of measurement, one squared and other no, square disappeared for me, so it seemed to me that there was no more a dependence on surface, but actually there is. In fact it comes here when total axial resistance is given by exactly that formula there, where $R_i$ was resistivity times length divided by passage section. I remember this thing here because intuitively the more a thing is long, the more it resists, while vice versa the more it is wide, resistance is lower, because I increase possibility of current to pass. I think always of a hydraulic analogy, for example, or pneumatic. For capacitance there is no problem, I multiply by surface and for transmembrane resistance I divide by surface. Ok, here is simply remembering how one calculates lateral surface of a cylinder and how one calculates area of a circle of given radius, $\pi r^2$. Ok, here are numerical values.

***

### Chunk 6 of 6

# Discretization of the Cable and Kirchhoff's Law

What I have to do now is consider that I don't have only a small little piece, but I have a little piece preceding and a little piece following. I know that at a certain point there will be start and there will be end, but let me imagine for simplicity to start in middle. If I start in middle it is easier.

So $V(x)$ describes transmembrane tension of a small little piece of cable, but then there will be $V(x + \Delta x)$ on right and there will be $V(x - \Delta x)$ on left. Since electrical properties are dependent on position, this little block here surely has a capacitive current and a resistive current passing inside this little cylinder. However, since also preceding little block and following one have same thing, it will be presumably a balance between an axial current entering here and exiting from other side. It is not so different from electrodiffusion equation, if we did it (at this moment I don't remember if we did it), in which one imagines in that case charge conservation.

Here I am in a pure electrical context, which I show you in next slide, and start looking at that electrical circuit of a little ago: intracellular part is high and here is extracellular part. In fact here there are... it is isopotential outside for moment, for this chapter is outside. And here at end I am in effect considering a so-called structure, a ladder electrical network (**ladder network**, I believe one says ladder), in which in theory this object here is infinite in this direction and infinite in other direction: these blocks repeat. So in theory I could be rightly frightened and say: "I don't have theoretical tools to be able to treat a circuit that has no end, a circuit that has distributed parameters and is somehow here implicitly an infinite cable in all two directions".

But I show you that it is possible to reason taking a piece in middle, preceding piece and following piece. These three are enough for me. These three are enough for me because somehow there is some symmetry, things repeat. For this piece that is preceding, there will be a piece still preceding that supplies it an axial current, which in turn for Kirchhoff's Law a bit goes in here and branches into a capacitive piece and into a resistive piece, and here continues in another axial component. This axial component passes through a resistor which is axial resistance of cable and again then bifurcates into a current, redistributes into a capacitive, resistive current and another piece of what survives.

Already here you can imagine that intuitive discourse I made you — a current flowing, then is working — at a certain point I imagine that, since this redistributes in this plus this plus this, this here will be lower than that. So as spatial coordinate increases, current will become smaller and smaller. Just to tell you that current is changing: I cannot use same current; current here entering is not same current entering here (aside that there is a resistor, but then there is this branch here in which part of current went away).

Anyway, here in theory you can "plug" yourselves, you can turn off brain and say: "Ok, he is giving me to solve a circuit, so an exercise of electrical engineering". Clearly I do it, I am emphasizing it, dramatizing it to try to simplify it, because I know that not all of you have [studied] electrical engineering like this or some of you haven't even studied it.

So, since I am in phase of calling with a spatial coordinate quantities, I say that this — I define it — is axial current $I_a(x,t)$, because in theory it can change also in time. This I call $x + \Delta x$, so it is the one exiting. This I call $V(x,t)$, transmembrane potential in that point; this I call $V(x + \Delta x)$ and this $V(x - \Delta x)$. I don't need anything else. I don't need anything else because these $C$, $R_m$ and $E$ I assume are same. If they were space-dependent, ok, I should take it into account. Maybe they are numbers, so they are not dynamic magnitudes changing in time, but they could change in space: it could be something that is not uniform. It means "is not same in all points of space". Why? I don't know, because simply it could be that cable thins. If cable thins radius changes, capacitance changes, resistance changes, axial resistance also changes. But in this case we are thinking of a uniform cable, simplest case.

What I do is I write for this node charge conservation, again Kirchhoff, law of currents (algebraic sum, so take signs as hell you want, provided then you are consistent with constitutive equations), and given this node or "shell" — so this closed surface that does not cross any electrical component but crosses only wires — and by convention give a sign to what enters and to what exits. I did it a thousand times, so intuitively I see that here enters this thing here and exit three other terms. So yes, I should have put everything at first member, $I_a$ with plus and all others with minus, but also as charge conservation I like to think that balance is that $I_a$ enters and is equal to what exits: $I_c$, $I_m$ and other $I_a$. It is only annoying because now we have to write $x$, $x - \Delta x$ (ok here there is not $x - \Delta x$), we have to write $x + \Delta x$ and all quantities in theory can change with time.

I mark aside what is $I_m$. $I_m$ I know what it is because that is usual, it is story of current with ohmic model: $(V - E) / R$. $V(x)$, because this is $I_m(x)$, transmembrane current. And transmembrane capacitance, I don't write it yet, but transmembrane current is $I_c = C \cdot dV(x)/dt$. But I write it in a moment.

Here I wrote myself $I_a$ and I wrote it as, for Ohm's law, current flowing in this resistor given a particular potential difference between this point and this point, and known resistance. Resistance I know because it is total resistance $R_i$, that trans... excuse me, that axial. And since I took current like this, going from left to right, is $\Delta V$ divided by $R$. And this $\Delta V$ is $V$ here, which is $V(x - \Delta x) - V(x)$. Ok, and this "mess" here I write it. I would like in other terms to try to write a relation having only $V$ at end of ends, so I move forward, since I know what is constitutive equation of a resistor.

Only it is not finished, because I have also this $I_a(x + \Delta x)$, and it is exactly same thing for constitutive equation, Ohm's law. If I take arrow like this, it means that this current is a $\Delta V$ divided by $R$: $R$ is same, is always this axial resistance, and is difference between potential being here minus potential being here. And I read it: here is $V(x) - V(x + \Delta x)$.

So you can anticipate that I won't get away smoothly with story of difference quotients, because I have both $V - \Delta x$ and $V + \Delta x$, and difference quotient wanted only to have $(V(x + \Delta x) - V(x)) / \Delta x$. Here I have also middle, in short it is a bit more complicated, and in fact first derivative is not enough, in end it will be only that... but we do it step by step.

In meantime capacitive current appeared, which I hoped arrived later, this one: capacitive current is one I recited before, $I_c = C \cdot dV/dt$, with dependence on position. And I can simply rewrite this equation substituting terms. It is ugly, but it is not difficult, it is algebra, at this point it is algebra, I did nothing else. So I can see, I can note that here $C \cdot dV/dt$ appears, so appears already as a total derivative. What evidently does not appear yet as a derivative — and I expect it must appear as a derivative — is dependence on space, what I would call a spatial gradient. This is a change in time, a change in space. I here see change in space, but it is discrete: here is $V$, so difference quotient before, then there is difference quotient after, and then this is again transmembrane current.



# Derivation of the Cable Equation and Taylor

Maybe I would like to massage this equation, I rewrite it there. Again, like all other derivations, try to do them yourselves. I repeat, the only thing for example to remember here is the little circuit. If you remember the little circuit you can write everything else. If you try to remember this expression here by heart it is more complicated, in my opinion you won't manage to remember it. I don't remember it, I have to re-derive it every time.

The thing I do, if you allow me, is change the variables because it annoys me that I keep carrying around $V-E$. If you do this change of variable, small $v$ equal to large $V$ minus $E$ ($v = V - E$), you discover that when you do the derivative in time, the derivative in time of $v$ is equal to the derivative in time of large $V$, minus the derivative of $E$. But the derivative of $E$ in time is zero because it is constant, so here basically it comes out exactly the same with the small $v$. This is by definition small $v$ at the numerator, because I did this change of variable.

Here and here you have anyway the difference of two quantities, each of which carries a minus $E$, an offset. When you have the difference — I think of the famous differential amplifiers... no, they aren't called differential, they are called? Are they called differential amplifiers? — in which common terms are cancelled. In the end it is a similar thing: each of these, each of these terms has small $v$ minus $E$, then there is a minus, small $v$, minus minus plus $E$; the $E$ and the $E$ cancel out. It happens both here and there. So this expression becomes a little simpler, simply because I removed this part here and it is only an idiosyncrasy of mine, it disturbs me a lot to have that $V-E$. I don't want to have constant terms, I want to have terms that are referenced to the resting potential. But it is simply a change of variables, it is simply changing the labels on axes.

The other thing I did is bring this term here to the first member and you see that it appears... but I didn't do anything else. You see that $V(x - \Delta x)$, $V(x + \Delta x)$ appears and then appeared with the minus sign here, minus $V(x)$ and also here there is minus $V(x)$ because I brought it to the other side, for this reason it is written $2 \cdot V(x)$. And this thing here, although it is a little strange, seems to be a kind of symmetry. The tension on the left, the tension on the right, minus two times... it looks like a kind of, exactly, the discrete description of the second derivative.

But now we see it in a simpler way, while on the right things remain unchanged, apart from having done the change of variable, so small $v$. Here I multiplied both members by $R_m$, that is to say I brought this thing here, I made the common denominator, but it is easier for me to say that I multiplied both members by $R_m$. You see that here it became $R_m$, here it is $R_m C$ and here it simplified. I wanted to do it because by eye things must add up from the dimensional point of view. Here there is nothing, it is millivolts for this $v(x,t)$. This $R_m C \cdot dV/dt$, is a millivolt with respect to a time, times time, times milliseconds, but $RC$ is a time, so time and time simplifies. Again this is millivolts — yes it is a delta, it is a difference, ok fine but it is always millivolts — and in this at the first member I have resistance and resistance that cancel, dimensionally at least, and again I have millivolts. So I can say: "ok, at least from the dimensional point of view there are no big problems, I haven't done, I shouldn't have made errors".

And so I invoke Taylor. Taylor and Taylor for a function that I can expand, that I think of being in the vicinity of a specific point $x$; I can calculate it both in $x - \Delta x$, and in $x + \Delta x$. Now I tell you why. In both cases Taylor tells me that the value of the function I can approximate with a polynomial. In this case I stop it at order 2. You will see why I don't go further (because I am not a masochist) but why I didn't stop at order 1. It means having the value of the function when this delta is 0, so it is $v(x)$, minus the increment $\Delta x$ times the first derivative (excuse me, it is minus because it is minus and the delta is negative, for this there is the minus), first derivative. Then there is $1 / n!$ times the $n$-th derivative; since $n$ is equal to 2, 2 factorial is 2, 2 times 1 is 2, one half remains the increment squared. So even if it was minus, I don't care, it became $\Delta x$ squared, plus $\Delta x$ squared, and obviously I have to write the derivative $n$, $n$ equal to 2, second derivative.

I do the exact same thing in another point, so I calculate... approximating the function calculated $v(x + \Delta x)$ in the neighborhood of $x$: again the function that has at that point there, the increment times the first derivative, one half the increment squared times the second derivative.

If I write them like this, it comes to me, let's say, I am tempted to sum them. I could also divide them, but in this case it is convenient to sum them. If I sum them, this term of first derivative goes away, disappears, and in this case of $v(x,t)$ it becomes 2, that is a 2 times $v(x,t)$, which is exactly what we had a little ago. So at the first member I have $v(x - \Delta x) + v(x + \Delta x)$... roughly equal because it is an approximation, I am not considering the infinitesimal terms of higher order, I stop at the second order and I stay there... two times $v(x)$ plus $\Delta x$ squared (because there was a half and a half) second derivative of $v(x)$.

Here I am reading that I can write the second derivative with this strange discrete behavior. If Euler's numerical method told me "do you have a differential equation of the first order? Do you have first derivatives? Write the first derivative as the difference quotient", this is in fact the same thing but saying "by chance do you have the second derivative to write?". If you want to do it obviously $\Delta x$ must be very small because otherwise the approximation you make of Taylor is off, you cannot move away so much. But since $\Delta x$ shortly will become practically infinitesimal, it becomes an equality, it doesn't become an approximation anymore. And I can do it if I have the value of the function on the left, the value of the function further on the right, minus two times the function in the middle point, because this formula here tells me so.

So in the equation where I have this "mess" $v(x)$ on the left, $v(x)$ in the center and $v(x)$ on the right, with the minus and with the 2, it is exactly the quantity I have before. So I can take, if you want, this $2v$ I bring it to the left and here remains second derivative of $v$. So I have this parenthesis here, I write it as a second derivative. It is true, I have to carry along $\Delta x$ squared.

And I am embarrassed because I always said: "No, wait, $\Delta x$ tends to zero", and so here I don't have the limit anymore because I wrote abruptly what is the expression of the second derivative, with this discretization. It is for this that I wanted at the beginning to write $R_m$, $R_i$ and $C$ as in terms of the specific magnitudes, because there I still have the radius, I still have the length which was precisely $\Delta x$ of this infinitesimal cylinder, and I still have... *that's it*, I don't have anything else. So I have the surface, the length, all the quantities that now I go to abstract, because this equation here holds for the little piece of length $\Delta x$.

And ok, this $\Delta x$ squared remains. Very well. But I would like to have a thing that is not only for that little piece, where that is I can vary $V$ with $x$ from minus 200 micrometers to plus 300 micrometers, assuming that zero is in the middle. You see that it comes out in a moment, remembering that large $R_m$ is small $r_m$ divided by $2\pi a \cdot \Delta x$, aha, I have $\Delta x$ here; $R_i$ is $r_i$ times $\Delta x$ divided by $\pi a^2$, I have another $\Delta x$ here. I feel like these $\Delta x$ however simplify... no, because I am not multiplying them. If I multiplied them yes, because here for $R_m$ the $\Delta x$ is at the denominator and here for $R_i$ it is at the denominator [numerator?], but I am not multiplying them, so I continue to remain. And it cancels. And $R_m$ and $C$, which therefore both have the same... depend on the lateral surface in a way one inverse to the other, so multiplying $R_m$ times $C$, we did it before, came to 20 milliseconds (it didn't come to 20 milliseconds per square centimeter or multiplied per square centimeter), but you see it from here: in this case the numerator, in this case the denominator times the numerator cancel.

So I manage to write an expression that doesn't have $\Delta x$ anymore, which is remarkable, it is powerful, because the only complicated thing was understanding this thing of the second derivative but it was only Taylor, it wasn't a complex thing of complex numbers, line integration, triple integrals... no, it was a stuff of understanding what is the discrete expression of the second derivative and recognizing it. This equation here, which is second derivative, I have to use the symbol of partial derivatives because these are not total derivatives being a function of two variables. This is a mathematically correct thing, ok, it is important, it is important, I don't want to trivialize it, but in the end you saw where it came out from. I obviously doing this operation was keeping $t$ fixed, because I didn't want $t$ to change, I wanted it to be exactly the same $t$ in all terms of these two equations that I summed, one to the other, in such a way to be able to write that total second derivative with respect to... excuse me... this second derivative with respect to $x$, also there I should have written partial derivative. Whatever, here I write it and I reconcile with the mathematicians among you. This equation here is the same equation of heat propagation, in which obviously it is not $v$, it would be $T$, temperature in time and space depends both on spatial gradient and temporal gradient.

I show you before further massaging... no, I have to take the break, forgive me. So we stop and see what happens when it is not only the cable but there is also a synaptic input, an external input. We stop for 10 minutes.

# The Space Constant Lambda ($\lambda$)

Good. Good. So, before proceeding, I would like to check things from the dimensional point of view to be completely calm. So we did it before, but I do it again now, because maybe something interesting might come out.

This quantity at the second member is millivolts, it is a potential $v(x,t)$. This quantity is a partial derivative of the function $v(x,t)$ with respect to time and is multiplied by $r_m$ times $c_m$. We already did it before, that beyond the fact that they are specific magnitudes, given that they have the dependence on space one at the numerator and the other at the denominator, independently of the choice of space, of geometry, they have as unit of measurement that of a time. Among other things it is $RC$, by ear it is always the usual thing, it is a time. In the end it is a time constant ($\tau_m$), it is the time constant that I recognize when there are not these spatially distributed phenomena. It is the same usual equation. Remember that $v$ is large $V$ minus $E$, so if I that was zero and I brought this to the other side, I would have the usual first order equation, constant coefficients, etc. So anyway time and $RC$ for the $\Delta t$ simplify, so this too is millivolts.

This thing here is a bit more interesting because at the numerator continues to be millivolts. Do not be deceived by the fact that here is the second derivative. You have here the writing of the second derivative. So this is always millivolts. At the denominator there is a length squared. This quantity here, that if you want we try to look at it, is $r_m$ over $r_i$ ($r_m / r_i$). So $r_m$ was ohm times square centimeter ($\Omega \cdot cm^2$), $r_i$ was ohm times centimeter ($\Omega \cdot cm$), which was what had deceived me, I had initially given a different physical interpretation.

So $r_m$ over $r_i$ — excuse me, they are small, $r_m$ over $r_i$, they are specific quantities — becomes... $r_m$ over $r_i$ should become centimeter, because ohm and ohm simplify and centimeter remains at the numerator. And so it would not be sufficient, but fortunately it is pre-multiplied, it is multiplied by the radius $a$, so $a$ times $r_m$ over $r_i$ becomes... is a length squared and so it adds up, simplifies somehow with the denominator of this second partial derivative.

So this quantity here is the square of a length. If you don't mind, I would be pleased to call it $\lambda^2$ (lambda squared). By analogy with what $r_m$ times $c_m$, small $r$ $c$, I would like to call it $\tau$, time constant. I anticipate that, so not by direct inspection as we are doing, not by identification of magnitudes on the basis only of physical dimension (it might not be possible, or I should be a mathematical genius that I look at an equation and understand things), but this thing here, beyond the fact of the square... so if you allow me to remove the square and take the square root... So this quantity here — excuse me there is also a 2 divided by 2 [in the radius], the 2 however has no dimensions, for this I had forgotten it before — so the root of the radius divided by 2 and of the ratio between the transmembrane resistivity (or specific resistance) and the axial specific resistance is called **Space Constant** ($\lambda$).


Perhaps it is the one that tells me if a cable is short or long, depends on parameters, in the same way with which this $\tau$ tells me if a time is short or long. Short or long in the sense that in previous lectures, when I had quantities, for example, linked to membrane potential — so in a case with a *point neuron*, a point neuron not extended in space — if I had some phenomenon to my great joy at a certain point maybe membrane potential would have finally dissipated, would have relaxed to the potential for example of rest exponentially. Here, if I have to tell you is a lot or little time passed, I do it on the basis of what is the time constant. If I say that $10\ \tau$ passed it means that here the transient has been exhausted.

So I like to be able to normalize the time scale and the space scale with a kind of quantity $\tau$ of membrane and space constant, that tell me what is the order of magnitude. Could have in other terms a cable of very few micrometers of length, but due to parameters, their ratio or radius, if radius was very very small, space constant could be even very very very small, so even a length of some micrometer could be electrotonically long, justifying existence of cable equation. You could tell me: "But the few micrometers... anyway, depends on that lambda, if it is small or is long, if it is short or is long".

# External and Synaptic Currents in the Cable

The thing I want to do now is add... because here now from the point of view, since the goal is to understand what happens in different points when there is for example a distal synaptic input, I here do not have simply the endogenous, autonomous behavior, I don't have external inputs. So what I want to do is take a step back, return to this point, to this point of derivation and say: look that here at this node I am putting in the balance of currents — so I start exactly from the same balance of currents, from Kirchhoff to that node — and I would like to be able to (then maybe I put it to zero when it is not there) I would like to be able to inject an external current, a pipette. I want to put a pipette in a central point, maybe of a cable that is infinite, or a cable that is semi-infinite (so has a start and has no end), I want to put the pipette there, in a point, and I want to inject a total current, large $I_{ext}$.

Or, this I do in a generic way here, in such a way to say: "Ok, if there are no synapses, whatever, if there are no synapses you put to zero the synaptic conductance". So here I put a further branch that describes to me synaptic currents due to synaptic receptors of some chemical synapse, AMPA, NMDA mediated or GABA-A, GABA-B, what you want. So this node here enriches: it is not only entering $I_a$ equal exiting $I_c$, $I_m$, $I_a$; there is also $I_{syn}$, synaptic, and there is also another quantity, $I_{ext}$. Note, these here were all exiting, this is the only one that is entering, like also this one, so it will have a minus sign.

So, ok, synaptic $I$ I write it as I have to write it; external $I$ anticipating the fact that at a certain point, as I said before, there are crazy $\Delta x$ that I want to simplify. So allow me to say that if total current coming out from my amplifier, from my current generator, is large $I_{ext}$, I write, define — and I can always do it because I am changing symbol, so small $i_{ext}$, which in fact is a current density — as $I_{ext}$ referenced to lateral surface $2\pi r \cdot \Delta x$. I write it like this because at a certain point this $\Delta x$ simplifies, so I can carry it along. I can also not do it, but if I don't do it then I have terms left over. In this way I identify it, identify quantity in a single shot.

Sum of entering currents $I_a(x,t)$ plus external current large $I_{ext}(x,t)$ is equal to sum of all exiting currents: $I_c$, $I_m$, this blessed new synaptic current, and other term $I_a(x + \Delta x)$. Enough, I finished, I have to redo same little game. Note, only different thing compared to before is that here at second member I have this synaptic current (and whatever this is expression). And then I have this external current that I bring to right, to second member of equation and will have a minus sign. Which is interesting because I thought that at a certain point you inject a current, you inject it positive... ok, for how this equation is written, when goes into right member must have a minus sign. So this was that of before, I can already at a glance say that here I will have a term plus synaptic term minus external term. Do I have it right? Here it is.

So you see that here instead of using small $v$ I went back because due to synaptic term... and here its synaptic here is wrong, because this $E$ must be $E_{syn}$ (E-synaptic). It is not said that it is... pardon, this is an error of mine, so this $E$ here in this equation is $E_{syn}$. Was right here, was right here, but obviously is wrong here. In general, if you find then, since maybe many of you will surely have a study even more in-depth during next weeks, if you catch some error in slides, speak, actually, I am grateful to you.

So and... yes but here then ok then error is on this slide here... no here is still large $V$ everything is because here small $v$ still is not there and I don't want to do it, but wanted to show you how equation came without that offset. Because fact of having an equation of partial derivatives, second derivative in time... equal first derivative in time... excuse me, second derivative in space equal first derivative in time plus something, this is cable equation. Here for moment there is not large $V$ and this is its inversion potential that could be zero in case of AMPA and NMDA. So here I have not yet done that operation of offset, because these can be different. Problem is, this was normalizing small $v$ equal large $V$ minus $E$, this I told you before and I wrote it, maybe I emphasize it still a bit.

So this was that of before, but here this is another, so doesn't descend from this, descends from an expression in which small $v$ never was there and unfortunately this is synaptic $E$. I think about it but I ponder but should be written as $E_{syn}$. And here is minus small $i$ because large $I$ multiplied... excuse me divided this $2\pi a \cdot \Delta x$, which is what remains to me after identifying term of second derivative, brings me to say: "Wait, don't write $I_{ext}$ divided, write small $i_{ext}$, which is a current density", and you are again in game with an equation that in theory by hand we will never solve (now we try, maybe next time, in some specific cases), but numerically yes.

# Boundary Conditions for the Cable

And I show you a notebook on Google Colab in which I show you how to use a neuronal simulator that is called, with great fantasy, **NEURON**. And it is a simulator that exists from Eighties and surpassed different generations of obsolete languages and now has a Python interface and is quite easy to use. Enough to write `pip install neuron` and you have simulator installed. There are other things, but you see it with Google Colab.

I want to underline, first, that there is this identification of time constant and space constant. For moment has no sense of why one is called space constant. Is analogous to time constant, so here is something exponential in time, evidently there will be something exponential in space, clearly in some simplified condition, in some simplified regime. And, again, here allows me to say if a cable is long or short.

Now, differential equations of partial derivatives, like this — these are partial derivatives, and $V$, unknown, is a function both of point and of time — are not really very easy, even from numerical point of view, for this I use NEURON, an ad hoc simulator. I cannot use Euler method. I would be tempted to say: "You know what? This you know how to write with difference quotient term, with increment in time (so in second variable $t$); this quantity, second derivative with respect to space, you have discrete formula, was simply a passage before, so in theory you can use it". But choice of $\Delta x$ and $\Delta t$ is *tricky*, is complicated and there are other numerical methods much more accurate because system risks becoming unstable. Unstable means that is not anymore accurate, solution you pull out no... what you pull out from algebraic iteration of this that became an algebraic equation has nothing to do with true solution.

So you do it numerically, but every time you have an ordinary differential equation — this who of you has mathematical reminiscences knows that **Cauchy problem**, given a differential equation of type at total derivatives, for example of first order, you know that exists solution and is unique, existence and uniqueness theorem — when you have specified initial condition, initial in time. Here there is also second derivative in space, so you have to specify other initial conditions. Initial conditions in space are called **boundary conditions**, boundary because it is thought they are at frontier, but are initial conditions in $X$, but are called *boundary conditions*. You have to specify two, because here degree of derivation of spatial derivative is order of second, and one initial condition in time, because here degree of derivation of this temporal derivative term is 1. Without this you don't have a solution, you have a family of solutions.

And is a bit *tricky*, now we see some types of initial conditions, of *boundary condition* and initial conditions.

### Sealed End

First boundary condition we see is that in which, that is called **sealed extremity**, *sealed end*. In jargon of an electronic engineer, this is an **open circuit**. Means that here current is null, I don't have a membrane, simply current has no termination. If current has no termination I should go... for in that circuit of which I showed you only a little piece and circuit extended to infinity, you had a piece on left, a piece in center, a piece on left... I have to instantiate it to final piece and final piece will have an $I_a(x,t)$ with $x$ equal to $L$, for example length of $L$. With this diction I am saying that $I_a(x,t)$ is a generic quantity and I am saying how varies in that point at this frontier, at this value of $X$, $L$, length of cable, extremity.

Could be a cable that on other side is infinite, but in this point is finite, ends, at coordinate $L$ (here $X_0$ could be where you want, doesn't matter). And fact that here current... there is no closure of circuit, if is an open circuit, for Kirchhoff's law — again for little bird standing on electricity wire and has a single little leg on high voltage wire, summer of currents equal to zero — here only current existing is this lateral current: $I_a$ in that point equal to zero. This is boundary condition for *sealed* extremity.

So if I do things correctly I should go look at expression of $I_a$ in that point and instantiate it for $x$ equal to $L$. If that current is zero means that... excuse me, if at that value current is zero means that current... this has no sense saying current at zero, ok, I have to think because this is not boundary condition... this is same thing here, instead here I am writing that exiting current is first derivative of potential in that point. Excuse me, I have to review equivalent circuit. Is not an open circuit, is a sealed extremity in which here is a membrane and this membrane is such for which there is nothing anymore downstream and current term for this definition is derivative — making $\Delta x$ tend to 0 — is total derivative of first order calculated in $x$ equal to $L$.

Next time I have to think about it and tell you in intuitive way, that here is not $I$ equal to 0 by definition. This is a type of boundary condition. I believe who of you has reminiscences of mathematics maybe calls this a **Von Neumann boundary condition** instead of **Dirichlet**. Vaguely I have this reminiscence: when boundary condition has a derivative term, inserts in a generic class of boundary conditions, of equations that mathematicians solved and studied for millennia, in which this is called Von Neumann.

Excuse me, this I have to review because I didn't tell you well. No, I made a big mess, pardon, pardon. It is current equal to zero! This is current equal to zero, so engineering jargon is correct, engineering jargon is correct: here current is zero because is an empty extremity, circuit is not closed. So I got wrapped up here because basically I wrote what is current in that point; to put it to zero, current in that point I wanted to write according to definition. And definition traces me back however with a minus sign... so this is definition (apart $R_i$ that I wrote as function of small $r_i$, for this there is this $\Delta x$ that appeared, $a$ squared pi, $\Delta x$, $r_i$). And this is not $V(x + \Delta x) - V(x)$, but is $V(x) - V(x + \Delta x)$, so this is minus difference quotient.

So from this that is definition I substituted $R_i$ with specific quantity because I am considering an infinitesimal compartment and doing this I realize that once $\Delta x$ appears to me this is difference quotient apart minus sign. So I can write it as a quantity that is derivative of $V$, total derivative of $V$ with respect to space, apart this multiplicative term and minus that is important. But since boundary condition tells me that here current is zero, what is current? This, that is this. Means that derivative of solution is zero. Again is a Von Neumann condition. I apologize for mess.

So, here I am saying that is a condition... is a bit different compared to an initial condition in time. Initial condition in time tells me that at time $t$ function is equal to 0, or equal to minus 60 millivolts. This could be an initial condition in time: along all $x$ at time zero, all cable is at *steady state*, is at rest. $V(x)$ for whatever $x$, $t_0$ at time zero is equal to a number. Here instead I don't say what is $V$ at that point, I say what is derivative of $V$ at that point and I say that for example that derivative in that point is zero. Ask well.

### Killed End

There is another condition called **killed or open end**, in which electrotechnical jargon would mean that there is a **short circuit**, so here is as if tension was exactly tension of *bulk*, of extracellular bath. And in this case — this I say at a glance because I know — so this is a **Dirichlet condition** because is a boundary condition of type "in that point you have that value" and that value is zero because extracellular potential is zero. So $V$ in that point $L$ for whatever $t$ is equal to zero.

Another condition (we won't have to use them in large measure and numerically are in part implemented for free, even if examples I will show you are of semi-infinite cables, so somehow is easier compared to this context). Another example, who has a background of transmission lines recognizes many of same conditions. Formerly also Ethernet network cables had to have a 50 ohm termination to function, or if one left them open or short-circuited had other types of behavior, had in particular reflected wave phenomena. Here is a bit different, but context is similar, in sense that case of terminated boundary condition means that here is a *patch* of membrane with particular Ohm's laws, with particular conditions. In this case is simply a resistive load, so in electrotechnical jargon here means I attached a load, and so for Ohm's law $V = R \cdot I$ at frontier at $x = L$, $I = V / R_0$. And for same discourse of before this current at frontier I can write as a minus difference quotient, apart sign, and this quantity, that which had been put to zero in first boundary condition we saw, is put exactly at value of $L$. Again, this is a further boundary condition a bit complicated.

### Ideal Voltage and Current Generators

There are other two boundary conditions, simple. First is that exists an extremity in which there is — see, Hodgkin and Huxley who treated cables, in fact were them in end to formalize these discourses on basis of cable equation of Lord Kelvin — potential at extremity is **clamped** with some electronic amplifier imposing a certain tension, whatever is current. This means that tension at that extremity is given, is known, is what I called as a command function. I have a command on amplifier saying "now put at minus 30 millivolts" and he keeps it, or "now make it change in sinusoidal way in time 30 times a second" and he imposes me that potential. Is an ideal voltage generator applied here and is for this I wrote it like this.

Dual case is that I here have a pipette behaving as an **ideal current generator**: whatever is potential at that extremity I inject a current. This in particular is more interesting. Again, as before we wrote, so here current is not zero, current is given, known, fixed, an assigned value. And since $I_a(x,t)$ at extremity I can write it as that ratio, minus that difference quotient, and that difference quotient I translate into $dx/dt$, $dV/dx$ [excuse me], I can write that that derivative in that point at $x$ equal to $L$ and value (apart $r_i$ times $a$ squared over pi, that pre-multiplied this derivative) is equal to current you inject. This makes sense because in a moment we inject... take a cable that is semi-infinite and inject from an extremity a constant current. So to do it we will have to use this formula.

# Steady State Solution of Infinite Cable

It is only case, or a couple of cases, that we see because a partial differential equation, as said, is a bit complicated to handle, in particular is complicated to handle in case of transient regime. So in transient is complicated (not impossible, as is complicated writing solution of diffusion equation in transient regime).

First thing we try to do is look at **steady state** regimes. Steady state means that magnitudes don't change anymore in time and if don't change anymore in time that partial differential equation becomes a total differential equation. Yes, there is a derivative term at second order, maybe you don't remember how one solves a linear differential equation but of second order (now we review it), but becomes possible, becomes easy writing solution. And writing solution is very instructive because, for example, makes come out story of **space constant**.

So what we do is consider a cable that is **semi-infinite**. I don't put any synapses for moment, I am simply saying is a cable *nature*, without anything. So this is cable equation, with small $v$, in such a way I liquidate also problem of $V-E$. There are no synapses, only thing existing is me with a current generator injecting at an extremity a known current, that I call $I_0$, a constant current. So implicitly I am applying a boundary condition and for rest simply I ask myself: by chance, this equation seeming complicated simplifies if I delete derivative with respect to time? Obviously I can do it when solution loses its dependence in time, that is doesn't change anymore in time, that is is constant, so derivative in time with respect to time is zero.

So this I am imposing. So I have to think that passed transients, if there are any, if I continue to keep here current always constant and turned on at $I_0$ (suppose 200 pA), so I am continuing at 200 pA here, at a certain point I will have some distribution of membrane potential. And intuitively you can think that for Ohm's law, maybe here, since this is a passive structure and you remember story that when you inject a current here, part gets lost, a part continues, a part gets lost... in end nothing remains, so is conceivable that potential tends to become smaller and smaller. Maybe changes with an exponential, given that exponentials are omnipresent, but need to see, here I don't see yet that comes out an exponential.

Now maybe yes, in sense that this term I put to zero, derivatives became from partial to total and this, compared to all second order differential equations with constant coefficients that can happen to you, is one of simplest. You remember vaguely that every time you had an equation of order $n$, you had a so-called **characteristic polynomial**. Famous eigenvalues of matrix (you haven't seen it in matrix way), you have a polynomial taking... that is not differential, is an algebraic polynomial, having an unknown for example call $s$ (how did I call it? $s$), takes coefficients those that are and terms at exponent, terms being in order of derivative. So here would be $\lambda^2 \cdot s^2$. If there was a first derivative with respect to $x$, you would have $s$ with some coefficient; if here was written plus 1 $dV(x)/dx$, you would have a characteristic equation that not only has $\lambda^2 \cdot s^2$, would have plus 1 times $s$. And here, at right of equal, you have term having a zeroth derivative, that for convention means (is called zeroth derivative but is function itself) that in case of algebraic is $s^0$, so order of derivation extended in this case, here is 0, is mapped in this algebraic case. You should have done it in analysis because is one of fundamental things, but here is very easy because abruptly I can write this equation. Is a second order equation, ok, but I don't have to write usual story minus b plus or minus root of 2b minus 4ac... no, here I see it "between quotes" by eye that solutions of this algebraic equation are $1/\lambda$ and $-1/\lambda$. I am not a genius, I divided both members by $\lambda^2$, I can do it because is a non-zero quantity (quantity that is not 0) and remains $s^2 = 1/\lambda^2$. Apply square root on both members, pay attention to fact that every time I do square root I have in theory to take plus or minus. If you are not convinced you can try substituting here: also 1 divided... also excuse me, also $-1/\lambda$ works and satisfies that equation.

And once you have these eigenvalues, are modes, are called **modes** of equation, are in fact exponents going to exponentials. So solution of this equation is, apart constants to identify, is:
$$V(x) = k_1 e^{x/\lambda} + k_2 e^{-x/\lambda}$$
One of two has minus sign and makes me happy, other has plus sign and I worry a bit because this thing here means that, ok, here I don't have time, but means that in space I have something exploding, I have potential exploding. But calm, because I have still to identify constants, that is I have to use boundary conditions, initial conditions or... excuse me here only boundary conditions because time is not there anymore. And here reasoning is following, might not like you but is a hypothesis of physical consistency in which I say that when $x$ is very large potential cannot explode. Here you could be unhappy because you say: "Thanks, you make it fit, is you imposing that doesn't explode, but would explode". Yes, but I search among all these solutions those remaining finite because biologically wouldn't make sense, physically makes no sense if I here inject a current and this cable has resistances, a transmembrane resistance per unit of surface and per unit of length, and so at a certain point quantities must dissipate, there is nothing accumulating.

So if when $x$ is very large, $V$ must remain finite, only possibility is that $k_1$ is 0, because if $k_1$ is not 0 this doesn't happen. Meanwhile I identified one of two constants. $k_2$ I use with this boundary condition: this boundary condition says "do what you want, you have to take solution, have to differentiate it and this is equal to minus $I_0$, $I_0$ given". So I remembering what is $\lambda$, derivative with respect to $x$ is minus $k_2$ divided $\lambda$ times $e$ to minus $x$ over lambda... written like this, calculated in $x$ equal to 0, so $e$ to 0 makes 1 is not there anymore, equal to $I_0$. $k_2$ I write in this way, because I wrote $\lambda$ as square root of radius divided 2 times $r_m$ divided $r_i$, and to make it brief, whatever disgusting or antipathetic thing can be written there, in end this is **Ohm's Law**, but for a cable. In sense that $V$ continues to be equal to $R$ times $I$, $I$ is current I am injecting.

Only problem is that at a certain point if I take $x$ equal to 0, if I put myself at this point of cable and look at it, I stamp my eye on it, $x$ equal to 0, $V(0)$ is equal to this quantity, that is a resistance, I call it $R_\infty$, times $I_0$ (because $e$ to 0 makes 1). So somehow continues to hold Ohm's law, in that point looking like in a binocular value of potential in that point. Clearly resistance has a strange characteristic that is not familiar one, depends on physical, biophysical characteristics of cable. Nevertheless, when I am not myopic and want only to look $V$ of zero (but experimentally I can do it, I can, with a pipette, can inject a current and read membrane potential in that point).

When instead I search generic case and say: "Ok, tell me what happens at 10 micrometers of distance from where is injected current, 100 micrometers, 1000 micrometers, tell me what is value of $V$", I read it here: is $e$ to minus $x$ divided $\lambda$. Is exactly similar to behavior that in time you have with time constants, is to minus $t$ over $\tau$, here is to minus $x$ over $\lambda$. For this I called it **Space Constant**.

I show you solution and then we finish, apart this input resistance. Ok, here at cable I see an equivalent resistor, is not so important. In end what I see varying $x$... note here I wrote, instead of writing $x$, I wrote $x$ normalized to space constant, so that I can use it for whatever value of $\lambda$. Means 1, means I am at one $\lambda$, a bit like what geek engineers do saying time in this case... here means if I am in this point, means passed $\tau$, I measure time in units of $\tau$, so I can say one $\tau$, two $\tau$, here is same thing. And I do it to be able to make this graph holding always whatever is $\lambda$.

So $V$, so trivially is decreasing. Here I normalized amplitude to 1, so I wrote... I divided by value of $R_\infty$ and $I_0$ (that I call $R_\infty$ because is apparent resistance of a long cable) and function is a decreasing exponential in space. Is a... let's say, is very interesting because if you think about it I have a simple dissipative behavior with a decreasing exponential of first order, a simple thing, telling me if you inject, are somehow pumping energy in a point and is a DC regime, a constant regime, *steady state*, you are continuing to pump, progressively echo of this input is heard but at a certain point is heard no more. Again, and if this wasn't me with an electrode and was a synapse, if soma was particularly distant, if I activate or don't activate, membrane potential at that distance would be negligible.

Obviously cables are not infinite. Here was to say that if you take values indicated for $R_i$ (here $C$ counts no more, in space constant counts only $R_m$ and $R_i$), with those values I gave you in one of first slides $a$ was missing, radius (here excuse me I wrote $d$, diameter, ok what is 2 micrometers of radius), space constant becomes 500 micrometers. This sets... says if a cable is long or short with that diameter. If diameter was very small this quantity would be very different, this quantity would become smaller because is at numerator.

To finish I want only to show you what happens and we see it next time in a **short cable**. A short cable doesn't function in same way. Here I "stretched" it in such a way to compare with same distance between 0 and 2, what in reality is a more complicated graph. Here depends on length, since is a finite cable, depends on length of cable how rapidly $V$ decreases. If cable is very long, that is for example is two times $\lambda$, then behavior is similar, not too dissimilar from infinite case. But if cable is only half $\lambda$, practically is as if not... yes there is a bit of attenuation, but as if didn't count much. Ergo space constant tells me if in case of a finite cable length is short (which means I can not care about cable equation, everything is isopotential, almost), or if no, actually cable is very very long. For example if is two times $\lambda$ can be long and if is long has a dramatic attenuation in its length, attenuation going from 100% up to 20%, so cable you have to keep, you cannot throw it.
