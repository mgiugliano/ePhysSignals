
### Announcements and lesson plan

Today, the idea is to conclude the part on **neuronal excitability**, continuing the introduction of some concepts related to higher-order phenomena with respect to the excitability described at its minimum, as I have introduced it to you so far, and also moving on to a non-deterministic, **stochastic** description. In the end, I emphasized to you that excitability is not due to continuous distributed properties of voltage-dependent ionic permeability of a membrane, but is linked to the characteristic of certain pores that are discrete and small. And when something is small, especially in a biological environment, at physiological temperature, it is associated with phenomena that are not deterministic, they are therefore... in particular, I am thinking only of thermal agitation.

And I believe in the second half of today's lesson, we will shift our focus to the description of **synaptic communication**. It is fundamental for the subsequent steps because the same formalism with which we described ionic excitability is related to the excitability of the membrane due to voltage-dependent ionic conductance, and it is related to what ionic conductance is linked to **ligand-dependent** permeability properties, not voltage-dependent.

### Review: The first-order differential equation and filters

This is again the model of four differential equations that **Hodgkin and Huxley** first described in the 1950s in a series of articles. A book was published recently, a few years ago (I don't have the electronic version but the paper version, I can lend it to you if you want, I don't think the university library has it), in which the articles were collected and there is a *commentary*, there is a description, a comment on the various parts.

I wanted to revisit the heuristic concept I tried to convey to you last time. When in other terms you have a differential equation of the type $dx/dt$ equals, *whatever*, $-x/\tau + f(t)$. Although rigorously the solution to this differential equation, where $f(t)$ is a known, time-varying forcing term, is rigorously the sum of two terms: the solution of the associated homogeneous equation (which means the equation obtained by removing this part) is a decreasing exponential, apart from the initial condition, apart from the constants to be identified; plus the particular integral which in the general case is not done in any other way except with the convolution integral.

---

Convolution between this quantity... involution or filtering, it's a filtering operation between the impulse response, which is a decreasing exponential (which is zero before and a decreasing exponential after), and the function itself.

But beyond this mathematical formalism, I have somewhat indicated to you that if $\tau$ is small, or conversely, if the speed with which $f$, the forcing term, changes over time is slow (it's a low speed), it's as if $x(t)$, roughly, is following $f$. This is a rough estimate, it's an approximate reasoning, but it's very useful.

Those of you who have an engineering background and have a minimum of sensitivity from the point of view of systems theory, recognize that this description is the same one that engineers love to death, where there are black boxes, where woe betide opening them, woe betide opening and writing the mechanism (albeit with phenomenological components), in which here I only have the impulse response, in time or as in some transformed frequency domain, in which the input $f(t)$ passes and is converted into the output. This is a filter and in the specific case, it's a **low-pass filter**.

Those of you who have some experience in electronics will recognize, beyond all the things we've discussed in recent weeks, this is the equation of an **RC** [circuit]. And you know the RC, in this configuration, when this input at this node is the current $F(t)$, it behaves like a low-pass filter, that is, the relationship between this current $F(t)$ and the potential across that capacitor is a relationship of the type that dampens fast changes.

### The Quasi-Steady-State Approximation

When obviously either $\tau$ is very large or conversely, when $f$ instead varies very rapidly, it's not that there's much tracking by... the signal varies too fast, so this approximation of **quasi-stationarity**, in which I basically say it's as if the equation were always at *steady state* (that is, this term doesn't change), even if it's wrong because we are in a dynamic regime... but if this is zero, $x$ is proportional to $f$.

This serves and is very useful for understanding what happens during an action potential, remembering that the individual gates of the sodium channels, you remember **M** and **H**, which are hanging there (why two? because there's the hatch that closes from the intracellular compartment, inactivating the channel, while there are the gates, perhaps three gates, since it appeared as $m^3 \cdot h$, there are three gates that represent the activation of the channel), **N**, the four $n$ gates of the potassium channels again, are all variables that satisfy a differential equation that ultimately is this one.

---

So $n$ and $n_{\infty}$, $m$ and $m_{\infty}$, $h$ and $h_{\infty}$, in the end, apart from different constants (perhaps written this way it would be dimensionally better), but anyway, apart from the different time scales on which the individual channel kinetics behave, roughly, sooner or later, the individual state variables would like to go to the value that $m_{\infty}$, $h_{\infty}$, $n_{\infty}$ have respectively.

And these $m_{\infty}$, $h_{\infty}$, $n_{\infty}$ I sold to you as identified experimentally, *fitted*, identified on what are experimental recordings made, okay, with a complicated **voltage clamp** setup using toxins to disambiguate, to decouple the currents. Okay, and they have a sigmoidal dependence on the potential.

### The Activation Curves and Threshold Behavior

You remember I tried to pique your curiosity, so $m_{\infty}$, $h_{\infty}$, and $n_{\infty}$, roughly, except for $H_{\infty}$ which does something the opposite (where this is **0** and this is **1**, the channels all closed or all open), $M$ and $N$ are roughly sigmoids, more or less of the same alignment, which have the intermediate midpoint at the same potential value which I remind you is roughly around **-50 millivolts**. So it may be that on the exam I will ask you to draw a graph like this and I will ask you "can you roughly put the order of magnitude?". And so here it will be more or less **-100**, **-90**, roughly, it's not **-2000 millivolts**, it's not **+500 volts** and here it will be roughly **20 millivolts**. I'm probably misremembering, but it's roughly just to make you understand that the *span* over which these proteins have evolved to become sensitive to the potential is exactly in the *span* in which the membrane potential varies, and conversely you could say "no, it's the membrane potential that varies in this *span*, in this interval".

So this is the dependence, okay, to finish the sentence... several sentences I left hanging a moment ago... I was trying to stimulate the fact that this is as if it were a kind of **threshold behavior**. If it were a rigid step, it would be "if you are below a certain value you are off, if you are above a certain value you are on". I tell you this because a large part of *machine learning* models based on artificial neural networks have that type of threshold behavior, they don't necessarily have a *smooth* behavior, smooth, "smoothed" (smoothed is ugly), sigmoidal. But nature makes sigmoids, it doesn't necessarily make switches/steps. Even transistors are not actually objects that are strictly switches, they are analog devices.

---

### Graphical Analysis of State Variables (m, h, n)

So, if I take, instead of just plotting $V$ as a function of time, as you see here $V_m$ (membrane potential, this is a graph taken from some book $V_m(t)$, we called it $V$, again this is the potential inside versus outside, but we set outside to zero because conventionally we can do that), I can not only plot $V$, I can plot $m_{\infty}(V)$. I know what the instantaneous relationship is, it's an instantaneous non-linearity and so I can, if $V$ changes over time, I can see how $m_{\infty}$ changes over time, how $h_{\infty}$ changes over time and so on.

And you have it here, the "infinities" ($m_{\infty}$) are the dashed ones or *dashed* (I don't know how to say it), with the little bars, with the little lines. For example, you see that the **$m$** and the **$n$** which are the most interesting ones for me (one is for sodium, the activation of sodium, the other is the activation of potassium). It's true that $M$ and $N$ have a different time scale, so much so that that one is called *delayed rectifier*, voltage-dependent potassium channels, delayed. But when you represent $m_{\infty}$, $h_{\infty}$, and $n_{\infty}$ there is no time, in the sense that there is no dynamic, there is no delay.

And it's a useful representation for intuition, for our intuition, because you see that if $m_{\infty}$ has this behavior, it would have this behavior, because the action potential is changing from hyperpolarized values to depolarized values and then back to even more hyperpolarized values. So this yellowish curve (the one I always forget is that this light blueish one is called magenta and there's no way I can remember it), this yellowish one starts from **0.3**, **0.4** because evidently when the potential is at **-60**, **-65**, $M_{\infty}$ is not **0** and starts to be at least around **0.4**, so the graph might not be accurate, my *sketch* on the blackboard.

You see that when the potential changes, $M_{\infty}$ changes, it copies it. The same thing happens for... sorry, $M_{\infty}$ was the magenta one, it's this one here. The graph is ok, in the sense that when the potential is more or less at the resting value, these sodium channels $M_{\infty}$ are more or less closed. For potassium, the curve is evidently *shifted* more to the left, which is to say at the same potential value I have a greater activation for $n_{\infty}$ compared to $m_{\infty}$ and you see it starts from here, but more or less temporally it does the same thing because time isn't there, time is decided by the potential trace. Whereas $H$, you see the behavior is exactly the opposite, this violet one, it starts from values closer to unity, **0.6**, **0.7**, so even at rest there is a portion, a fraction of voltage-dependent sodium channels that are in an inactive state. At these depolarizations, it's already no longer **1**, it's a bit lower than **1** and it does the opposite thing, so when the potential increases it turns off, this $H$ gate turns off and when the potential hyperpolarizes it returns to being not inactive, and then to later return to a more equilibrium value which is probably around **0.6**. During a firing, during a regime where there is a sustained succession of action potentials.

I'm showing you this because if you then look, if you compare $M_{\infty}$ with $M$ (so this magenta trace and this blue trace), you see that they roughly do the same thing. Why? Because the activation of sodium is practically instantaneous, it's very fast, I reasoned it out for you, it's one of the fastest biological objects that exist. In a fraction of a millisecond, this opens. It was due to these $\tau$'s, which yes, I agree, are voltage-dependent, but roughly it's a quantity, a small number, that $\tau_m$.

Conversely, you see that if you compare the yellow to the red (so $n_{\infty}$ to $n$), you see that $n$ is lazy, it doesn't follow, I mean it follows, okay, it follows, so this reasoning is ok, but it follows with an even greater delay. And the same goes for $h$, $h$ is anyway delayed, if it weren't, the action potential wouldn't exist. $h$ you must compare the green to the violet. You see that roughly $N$ (the red compared to the yellow) and $H$ (the green compared to the violet), seem to have a similar time dynamic.

This could have implications that we will not address in this course on **dimensionality reduction**. It's possible to write a system of differential equations that has two instead of four, in which in other terms the sodium $m$ in the end is always at *steady state*, so I can throw the differential equation out the window, I can directly consider that substantially $\tau$ is so small that it's always at *steady state*, so $dm/dt$ is gone. And the equation for $n$ and for $h$ in fact, you see, they even seem almost a mirror image of each other if there were an axis of symmetry around **0.4**.

So for those of you who will follow the curriculum on neuro, there will be a modeling effort of complexity reduction and from this graph here you further understand why the issue of time constants, particularly $H$ and $N$, are fundamental to allow the action potential to go up and down.

---

### The Biophysical Reality: Markov Kinetic SChemes

In reality, in particular to revisit what I mentioned last time and a little while ago, the fact that from the point of view of an intuitive description, I imagine the sodium and potassium channels, since the currents... I'll take the example of the sodium current, which is a maximum conductance times... what? $m^3 \cdot h \cdot (V - V_{Na})$. I imagine it with these little gates, of which there are four, one of these is different, but... oh, I have the blue color.

It's not exactly like that, I sold it to you from a biochemical, molecular point of view, as the presence of different subunits, for example, seen from above, this object from a molecular point of view is the assemblage, the assembly of four objects that only when they are close create a pore and each of these has its own dependence, its own dependence on the transmembrane electric field, therefore on the membrane potential inside versus outside.

In reality, doing an experiment, the **kinetic scheme** that best represents the dynamics of potassium activation, the potassium conductance (so the study of the currents and their conductances), doesn't have a single *gate* and then the current has these exponents **3** and **4**, but in reality, there are many states: **1, 2, 3, 4**... there are **5** states of which only one state is conductive, associated with a conductance. And the same goes for sodium, you have **eight states** in which in fact it's as if structurally there were a symmetry and it depends, this branch above, this branch below, on the state... if you like, if I write them like this with states (state number one, state number two), there is no symmetry. If instead I put these $M_0H_1$, $M_1H_1$, in some way I'm thinking in these terms, but in space the structure is this, in which for each of these configurations $H$ can be in two states (either active or inactive), and in each of the states for which each *gate* ($M_0, M_1, M_2, M_3$) are active, I can *flip* from one to the other.

I won't talk about it and I won't prove it to you, but this type of **Markovian** kinetic scheme with these specific coefficients can be shown to be equivalent to the product of the individual *gates*. So the model I told you about (open, closed, maybe repeated for each of these subunits, which are identical and statistically independent) continues to work, but strictly speaking, the states through which the overall protein moves are multiple.

So in theory I should write a differential equation... I would probably write a system of **5** differential equations or **8** differential equations. Remember that in kinetic schemes they are all linearly dependent, so I can remove one because I can write a mass conservation relationship $n_0 + n_1 + n_2...$ equals **100%**, which means the protein is in one of those $N$ states, it cannot be in another state. But strictly speaking, I could then put here only one number, a state variable which is the fraction of channels in that state $n_4$ or in the state $m_3h_1$ assuming that is the conductive state.

I'm telling you this because in a moment we'll move on to the **stochastic description** and the stochastic description requires the most accurate, most extended description possible of the kinetic scheme of the channels we're talking about. So this is simply a slide to say: that type of kinetic scheme is reduced because for that choice of values that seem to be the ones that *fit* the most, it turns out that the four subunits are independent and so it's as if the open state were the simultaneous occupation of four subunits in the same open state. For sodium, there is one that, however, is an exception to the *rates* which are different.

---

### Exploring the Model: Does a Threshold Exist?

I encourage you (you have some figures, but you have the code, you have the Google Collab Notebook), even if you don't want to get your hands dirty writing numerical methods, changing the values... you have a damn *slider*, you can play with it and you can answer some questions that maybe I already stimulated last time.

Is there a **threshold**? Is there a value of external current, for example, and if one applies a value $-\epsilon$ (where $\epsilon$ is an infinitesimal quantity) the neuron doesn't fire, and if one applies that value $+\epsilon$ then the neuron fires?

The consideration is that there don't seem to be biological *if-then-else* statements, there isn't a kind of rigorous condition. Or conversely, the steepness of these sigmoids is not so extreme as to think that a threshold value exists for which, once exceeded, for example a threshold value in the potential (not in the current, but one influences the other), automatically, *boom*, maybe not suddenly but with a certain kinetic, all the channels switch from open to closed. No, it's a more *smooth* process.

And it's a *smooth* process probably because it's the collective effect, in this deterministic population description, of the effect of many, many units, which in themselves are gates that are either all open or all closed. If I have a door, right now it's either closed or open (even if it's ajar, it's open). Instead, a population of... you should think of windows in a large building with many, many windows. Probably when you have many windows the behavior, for example depending on the outside temperature (if it's very cold they'll all be closed, if it's hot all the people in that building, in those apartments open the windows), there will be some kind of gradation. But the question is: in the end, I see the electrical behavior of a neuron which is the result of this collective of voltage-dependent channels. Is there a threshold? Try to play with it.

The thing you might encounter is that you would have to decide what the stimulus is. If the stimulus is a DC pulse, a constant current, which is constant (now here I've shown the transient but only to make you understand that I turned it on, from zero it became some value $I_{ext} = I_0$, something like that). This is one choice, but it's not the only choice.

In the simulations I prepared for you so that you could play with them or get your hands on them, modify them, break them, change the parameters, explore them (something you couldn't do with the same ease in a biological experiment), you also have other cases where the stimulus is a small pulse, a small rectangle of current, always constant, but for example, this is **10 milliseconds**.

So you would realize (I hope you will realize) that it depends on the stimulation protocol. Whether there is or isn't a threshold, what you would conclude "yes, a threshold exists", depends on the protocol. In a way, it also depends on the **integral underneath**. And the integral underneath shouldn't give you mathematical nightmares, but if you remember that in some way current means a charge per unit of time, when you take the integral of the current with respect to time, it's as if you were obtaining the transferred charge.

All these things are trivial for those who have an intuitive idea or a mathematically accurate one or field experience with a capacitor. When you inject current into a capacitor, it charges, and obviously, it charges all the more, the longer you are applying a current. Same thing.

---

### Temporal Integration and Summation

This concept of **temporal integration**, then, is partly linked to the capacitive properties of the membrane. Partly, it is obviously also linked to the active properties, to the fact that you have channels and conductances that are potential-dependent and so you could try to investigate what happens if you have a sequence of these little rectangles.

Now that I've told you the story of integration you would say: "ok, I get the game, here you're basically saying that if the integral, if the area under these little pulses, maybe they are very small pulses, but they are repeated in time, if they are repeated in time and if there's a sufficient number of them, then the neuron, hence the membrane potential, the capacitive properties of the membrane will sum up and the neuron will fire".

If you play with it (changing the stimulus in one of those simulations isn't complicated and you could easily resort to the various ChatGPT, Gemini, etc.), you might realize that it also depends on how much time passes between one stimulus and the next. In the end, it's a **leaky capacitor**. So if you do nothing, to my great joy, the potential decays exponentially because it's a dissipative system and it doesn't explode. So if you wait too long between one pulse and the next, you lose dynamic memory of what has been in the past.

So the story of temporal integration is seen not only from the point of view of charge transfer to make the neuron fire, but in general, it has a relationship with what is also called **summation of pulses**, summation of stimuli. In some way, the membrane potential behaves like a variable that memorizes, keeps in mind, then forgets. And it's fundamental to forget things, even from a cognitive point of view at a very high level. If you were to remember in exact detail what you ate **12** years ago at **8:30** on **November 11th**, probably... at least this is what happens in an artificial system like a Hopfield network, you might have a catastrophe, that is, the inability to store any more stimuli. This thing about the learning catastrophe is a very interesting theme clarified by statistical mechanics, so by fields of research completely distant or distinct apparently from biology.

So even at a cellular, dynamic level, the fact that if I leave the room at a certain point (a time I hope is not so brief) you would forget me, has the effect of saying: "if the stimuli are frequent enough, I fire; if they are too sparse, if their frequency is too low, I don't hear them anymore, I forget them".

This you could play with on the generation of the action potential and the activation speed of the individual gates. In the end, you have the numerical values. You could simply multiply by **10%**, or divide by **10%**, the absolute values of those taus, $\tau_m$, $\tau_h$, and $\tau_n$, and see if the thing continues to work. In some directions, you would find that the action potentials become very broadened, in others you would find that perhaps they initially become very, very narrow, but then it may be that when the time scales become comparable you don't see anything anymore, because for example, the sodium activates just as fast as the potassium and they cancel each other out as they go.

---

### The Frequency-Current (F-I) Curve

The interesting thing that I'll show you in a moment, and again you could easily do this, is to systematically change the value of your stimulus (assume for simplicity you do it when the stimulus is constant). And like last time I showed you I counted **1, 2, 3**, I think I did it here to pull out what the average firing frequency was and I told you that for the Hodgkin-Huxley model it's a **type 2** excitability model, it goes directly from **0 spikes/second** to a certain value, I think it was around **50, 60, 70, 80 spikes/second**.

What I'm suggesting to do, in other terms, is to systematically change the amplitude from zero. Each time you have to run a new simulation, just as experimentally each time we have to let the neuron rest, change the setting on the electronic amplifier or in the computer and say "ok, now I'll inject you with another current with an even greater amplitude, and even greater". So I'm thinking of a protocol that... increases, sorry, these should all be horizontal, so they are all constant values, for example, I can double them, each time I add **10 picoamperes** and each time if I do it over a sufficiently long time I can estimate how many spikes per second there are.

We'll see this together now because I'd like to give you the intuition for why when I increase the stimulus, I increase the firing rate. Maybe I said it here or I said it on Friday, I don't remember, maybe in both cases. There are some neurons in the dorsal part of the spinal cord that encode the weight of a proprioceptive stimulus that is, for example, in my hand right now. If you double the weight on me, right now I have a neuron in the dorsal part of the spinal cord that is firing at a certain frequency, if you increase the weight the frequency increases. It's not necessarily linear (doubling the weight doubles the frequency), here too it's not necessarily that by increasing the current by a certain number of times, then the firing frequency increases proportionally, but it does increase, the behavior is monotonically increasing. I'd like to make you understand why, and it's relatively trivial, and it involves talking about the **first-passage time**.

### Refractoriness and Ionic Concentration

**Absolute and relative refractoriness**, we talked about it, I showed it to you (correct me if I'm wrong) in one of those stimulation exercises where with the *sliders* I demonstrated to you when a neuron became partially or totally insensitive to stimuli.

This we haven't done, I'll show it to you now. What happens if I change the **extracellular concentration of potassium**? Which you'll remember, potassium is highly concentrated inside, it's in low concentration outside. So if I put a lot of it outside, what happens to the **Nernst potential**? The reversal potential for potassium currents? In terms... in what terms? The number that comes out, how does it change? Normally it's **-80 mV** because if it lowers...

Perfect, perfect. It tends to become **0** because the logarithm of the ratio, if it were the same inside and out, would be logarithm of **1** which is **0**. So it's **-80** because there's this imbalance. If the concentrations become comparable because it increases outside, what do you think might happen if the reversal potential for potassium currents tends to increase, given that the role of potassium currents is to pull the action potential down? Any ideas?

I hope it doesn't happen to anyone, or that none of you suffer from this medical condition, but you would have an epileptic seizure, one of the many ways an equation [sic] can be triggered, because somehow the resting potential, even the resting potential itself, tends to become more depolarized and so sodium, which doesn't give a damn about potassium, starts to activate. Yes, it's true the potassium isn't sufficiently able to bring the potential way down, to hyperpolarize it, but I'll show you that it's sufficient, with no stimulus, to have a spontaneous, sustained, tonic activity, as in an epileptic crisis, in a *seizure*.

This we talked about last time and I'll show you that by setting a particular value to zero, I'll show you the change, what would happen if I had toxins that selectively block either the sodium channels or the potassium channels.


### Optogenetics and Neuronal Control

I mention it only because in the literature and also in our lab (but not for therapeutic purposes yet), people have developed, perhaps in the field you've heard mentioned, I think I told you about it at the beginning, it's called **optogenetics** (and they will surely win the Nobel Prize in the coming years, it's two researchers, one from MIT and one from Harvard). They put proteins, membrane channels or ion pumps that are found in other organisms, mainly in algae (or at least the main ones they started with were in algae), where the algae need to, for example, photosynthesize, they need to have some kind of information if there's light, a bit like some photoreceptors, some cells in our retina, which transduce light phenomena into electrical phenomena, into ionic phenomena.

And so in our lab, we have a way, with a viral vector, to convince real neurons to express ion channels that when we illuminate them with an orange, reddish light, these channels open, they are channels that typically let... they are permeable to potassium [corrected: chloride, as mentioned later], and when these, actually chloride... and when they activate, when I turn on the light, the spiking activity of the neurons is completely abolished. So if one were to put them in the brain of an epileptic person, just before the onset of a convulsion, of a dramatic, synchronous activity, one could flash the light and it would reset all the neurons. And thus it would prevent this hyper-synchronization, hyper-activity.

From a control, engineering point of view, we've seen that when we hold the membrane potential because we have the light on, even for just a few tens of milliseconds, all the negative feedback mechanisms relax and we have a situation like this with that famous *rebound* I showed you earlier. Ok, we've turned off the activity, imagine this could be a little longer, and as soon as we let go, all the neurons are very happy to resume, in fact, they are even happier to resume their electrical activity.

And the key would be how do I, once I've turned on the light, turn it off in a gradual way or vice versa? How do I, like with the thermostat in this room, sorry it's too... How do I, like with the thermostat in this room, have a control law that prevents oscillations, prevents unstable behavior? Here, with your colleagues, we set it to **20 degrees**, I don't think the temperature has dropped dramatically. It's trying, the thermostat is trying in a very slow way, due to the dynamics of the system to be controlled, to activate the radiators, probably the hot air reaction [sic], to keep the temperature constant. We have to do the same thing, or one should, to think about neuroprosthetics based on optogenetics to treat diseases of excitability. Another example would be schizophrenia, but that's another topic, another story.

---

### Simulated Pharmacology: Channel Blockers (TTX, TEA)

In the last part, I'll simply tell you that it's possible to simulate **pharmacology**, this is a very simple thing, where in the code you see I have variables called $G_{\text{bar\_sodium}}$, $G_{\text{bar\_potassium}}$. I remember that one of these in the Hodgkin-Huxley model is **+120 millisiemens**, I don't remember what this one is, suppose it's **60 millisiemens**. I think there are more sodium channels, assuming the single-channel conductance is the same, there are more sodium channels than potassium channels, or the total conductance of sodium channels is much greater than that of potassium.

If I want to simulate the presence of that toxin **TTX** from the *pufferfish* (I don't know what it's called, the tropical fish that puffs up and that can be toxic because its toxin binds to the extracellular domain of sodium channels and blocks them), so not only do I no longer have an action potential, but I don't even have the ability to control my respiratory system and so I croak, I die, technically I die. It's incredible that nature has evolved molecules so selective to say "you are based on information processing, on sodium channels because you make spikes, I'll sabotage your sodium channels". It's what I was saying that to order it, one must declare they are not a terrorist. When you put it in an experiment where you have cells, as soon as you put it in, after a few seconds, when the substance diffuses and gets near the neurons, the impressive action potentials disappear immediately. You practically go in time from one second where the neuron's response to a small pulse of current evokes a spike and evokes it, to a condition where there is no more excitability, and it's remarkable.

And it's a very useful experimental control to understand what depends on the electrical activity of the cells. If I remove, from a, if you like, chemical point of view, I turn off the sodium channels, if what I still see, if what I'm studying I still see, it means it doesn't depend on the electrical activity of the individual cells.

So here it's as easy as putting **0**, this variable equal to **0**, instead of where in the code it said this variable equals **120**, I set it to **0**. I can repeat the simulation. Ditto for... ok, I should have asked you which conductance you think I turned off, here I turned it off completely, I don't know why there are two graphs here, here I lowered this quantity, instead of **120**, I reduced the number of available sodium channels and I have a kind of aborted action potential.

In this case, instead, it's the dual case and it's interesting to see because I think it can answer the intuition. Here you have a toxin, this famous **TEA** [corrected: TITEA in text, but TEA is more common for Tetraethylammonium], which blocks potassium channels and so at a certain point you have the opening of sodium channels, their inactivation, but you no longer have anything to bring them back to **-70, -80 millivolts**, to hyperpolarize them, and so the trace of the membrane potential remains stuck in what is called **depolarization block**, so it's a block of depolarization, here the neuron does nothing more because it's never given the chance to return to an initial condition where it was ready to fire again. I think the difference between these cases is that here I put a little bit, so instead of setting this value to zero I reduced it, I put it at **50%** and so if you don't have enough sodium you don't pull the membrane potential down [corrected: up].

Again these are trivial things and in theory, it's very interesting to get your hands on them. Experimentally you have to buy expensive toxins and use them with extreme caution, in a computer simulation you don't have to wear gloves, you don't have to wear masks, you don't have to declare you're not a terrorist, you could even be a terrorist.

---

### Beyond the Hodgkin-Huxley Model: The Diversity of Ion Channels

What I said before, and I won't expand on this topic in this course, is that in reality neurons don't only have voltage-dependent sodium and potassium conductances, so they don't have the famous *fast inactivating voltage-gated sodium channels* (so, rapid inactivation) and for potassium *delayed rectifier* (rectification and delay and delayed behavior).

There is a huge number of ion channels, right now I don't have the book or the figure with me, there are books just on voltage-gated ion channels, there are enormous families, they have a genetic counterpart, in different species there are variants and so not all channels have the same homolog in all other species. For the voltage-dependent sodium and potassium of the Hodgkin-Huxley type, yes, and that's why we're still talking about them.

### Electrophysiological Phenotypes: Tonic, Phasic, and Bursting Firing

It's possible that a neuron subjected to a current injection might either not fire (so, have no activity), or have **tonic** activity, or have **phasic** activity (as it's technically called). In particular, this has another name, it also has another name which is called ***bursting*** activity. *Burst*, I don't know how to translate it into Italian, it's a kind of shock, of period, of packets of action potentials that are fired and then there's an interval of several hundred milliseconds in this case without action potentials.

It's clear that no matter how hard you may try, when the current here is constant or it's a spontaneous activity, you will never have in the simulations I gave you or in the Hodgkin-Huxley model or imagining that there's only sodium and potassium of the type I told you about, you will never have a so-called **electrophysiological phenotype** like this. Why? Because it's the result of the presence of other types of channels and there are, as I said, different channels that depending on their... (I erased, here were the activation kinetics, sorry, the activation curves, the $m_{\infty}, h_{\infty}, n_{\infty}$), there are channels that have those curves *shifted* differently, or have different $\tau$'s.

### Modulatory Currents and Spike-Frequency Adaptation

And for example, in some cases, there are currents that are called, they are selective for potassium, which are called **muscarinic** (which always have the effect of opposing excitability). Any type of current you see that is selective for potassium, because of its reversal or Nernst, equilibrium potential (**-80**), if it activates it tends to discourage spikes, so it tends to slow down the frequency. Conversely, all the currents that are linked to sodium, to calcium, which have their reversal potential at depolarized values (**+20, +30, +100 mV**), tend to increase excitability. This is a very important concept that we'll see again in the description of synaptic receptors.

So again, depolarized or hyperpolarized, beyond one being negative and one positive, it's not that the algebraic sign matters, what matters is whether they are above or below the resting potential. Everything that is above tends to favor, to increase the frequency, to accelerate the frequency, what is below tends to brake it.

In this case, you see a phenomenon called **frequency-dependent adaptation**, in which despite the stimulus being constant, the neuron starts fast and then slows down, exactly like the ganglion cells and also the cells of the primary visual cortex that I showed you in the experiments of Hodgkin and Sido [?] tomorrow. The experiments of **Hubel & Wiesel** showed audibly, you could really hear a *trrr*, the *pitch* of the sound was representative of the frequency that was decreasing.

Here in this graph, I've simply put some names of conductances, for example of currents. There are some currents that tend to delay the onset of a spike, they are important because in some cases they are called **A-type conductances**, they tend to be potassium conductances but they have an inactivation. The famous hatch that seemed to be only a prerogative [?] of sodium, some potassium channels also have it, another family, another gene, another protein. And there are other currents, currents that are called *after-hyperpolarization*, which come after the hyperpolarization due to potassium, which also contribute to modifying the shape of the action potential or the dynamics of the membrane potential after the action potential. For example, in this case, you see there's a progressive increase of the inter-spike interval, it starts fast and after a short while, it starts to relax. This is the possible mathematical modeling and subject of other considerations.

---

### An Experimental Example: Human Neurons from Stem Cells *in vivo*

To show you that this is something seen almost daily in the lab, I'll show you a trace recorded years ago by a researcher in my lab when we were in Belgium (he's now an associate professor at the Polytechnic University of Milan, a bioengineer). And the type of experiment was a very interesting experiment, but in this case, I'm simply selling it to you as if it were a neuron.

The experiment was interesting because it consisted of taking stem cells, embryonic or induced pluripotent, differentiated from biopsies. These cells were, in a Petri dish, differentiated into neurons, so we had human stem cells that were differentiated and transformed into, reprogrammed into neurons and were, as a conclusion to something already particularly complicated, were first transduced with a viral vector that made them express a fluorescent protein, the famous **GFP**, which perhaps you've all heard of (Nobel Prize many years ago to an MIT scientist, Tonegawa), Green Fluorescent Protein, so that Daniele Linaro could see them under the microscope when he tried to introduce that famous pipette I showed you and he saw the cells as if on the surface of the Moon with that microscopic technique that I told you is called **DIC**, *Differential Interference Contrast Microscopy*, differential contrast microscopy based on infrared. If he looked for the fluorescent cells, by changing the light source, he saw the green ones and the green ones were the human ones.

And he saw them in the cortex of a mouse where they had been implanted, in an attempt, in the hope of understanding what happens, in cell therapies where stem cells differentiated into... think of Parkinson's, where a portion of the nerve cells dies, and I put back identical ones from the patient, because I take them from the skin, I make them pluripotent, and then I reprogram them to be neurons.

The experiment was very complicated because implanting human cells into another type of organism is a mess, because there's a xenotransplant where the recipient's immune system doesn't recognize the cell as its own and so it attacks it, unless it's a transgenic mouse that is immunosuppressed, so it has almost no immune system. Except that these little immunosuppressed mice are extremely fragile, you look at them and they die, unfortunately, so keeping them for nine months, the time necessary for maturation, which recapitulated the human embryonic stage, nine months to reach electrical maturity in an animal that could die at any moment, was complicated.

---

### Trace Analysis: Adaptation and Irregularity

And when he injected a current for... I think this is about ten seconds, this is a calibration bar, as they say, and it's **half a second**, so here it's one, two, it will be about fifteen seconds or ten seconds. If you squint, you'll see first of all that even though the current is constant, this isn't exactly a metronome. So the point I was making before, if you want to estimate the frequency here... you see, there are even some cases here, you can see by eye that there's a bit more space here, so the activity is regular, but like everything in biology, it's not exactly a metronome.

If you squint, you'll see that the very first action potentials here show exactly that **spike-frequency adaptation**, that adaptation of the spike frequency, which is also called **accommodation**, or *accomodamento* perhaps in Italian. And I may have mentioned it to you because it's a very typical thing for sensory receptors, i.e., the nerve cells that are linked to visual, auditory, proprioceptive, tactile, nociceptive (pain), etc., sensation. And I gave you the example that if a colleague of yours grabs you by the shoulders from behind, you probably have a reflex reaction called the *startle reflex* (I don't know how to say it in Italian), of jumping (Italian is not elegant for these things). But the clothes you've been wearing since this morning presumably don't make you jump, yet you have them on your shoulders, you don't feel them anymore.

This is because, evolutionarily, the world is not stationary, it's constantly time-varying, and perhaps I need to have neurons of all types, including sensory ones, that lose interest in what doesn't change over time. Here the current is constant. If, however, the famous tiger I've been invoking in class for years now were to enter (it hasn't entered yet), it would be a change, a notable non-stationarity, and so my neurons would have to sense the change and, for example, fire much more during the change, because the change contains the surprise, it contains information, and probably those of our ancestors who didn't have accommodation were eaten by the city [sic: tiger]... or by similar beasts.

---

### Adaptation as a High-Pass Filter

The interesting thing is that if you zoom in here, you can see this spike-frequency adaptation [corrected: frequency adaptation] better, and you can understand the story I was telling you about characterizing the frequency as the inverse of the interspike intervals. Paradoxically, here you could do it by plotting $1 / (\text{Inter-Spike Interval})$ as a function of time, and you would see that the concept of frequency adaptation is represented by a kind of dynamic of the frequency variable, of the observable frequency over time. It starts very high, almost **150 spikes/second** (you have one spike, two, they are the first two inter-spike intervals, they are these two points here), then things slow down a lot and within **two seconds** (which I'm not sure if it's this, no, it could be here). Within **two seconds**, so more or less here, you see that there's a kind of *steady state* of the frequency which is low, it will be on the order of **20 spikes/second**.

All neurons, particularly in the central nervous system, have a very low electrical activity because it would be metabolically extremely costly to fire at **100-150 spikes/second**. You see the little exercise from last time where I showed you how much a compartment of a neuron could eventually be messed up from the point of view of ion concentration. You keep eating bananas, potassium, and sugar, etc., for the ion pumps, etc., but... so metabolically it's expensive, and this adaptation seems to make sense both from the point of view of metabolism and from the point of view of information.

In engineering terms of filters, of filtering, those things (band-pass, high-pass, low-pass), so those that favor a particular spectral content or that favor slow transitions, fast transitions, what is this, what filter is this? If this were the impulse response of the filter... Ok, no, it couldn't be because it would be... if it were... you have to think in terms of spike frequency. What filter could it be? It's one of the three I told you, so you have a **33%** chance of guessing it.

Given that the important thing is to detect surprise, change, and if instead it's something that's constant, I want to lose interest. It's either **band-pass**, or **high-pass**, or **low-pass**, one of the three. Try, I hope, to get an... Pass? No. Why did you say band-pass? No, it's low-pass. It depends on what you're considering. The low-pass... so you have to imagine that here the response, this is the response to a step, so the input is a step, which in theory contains all frequencies because it has an ideal, steep transition, and so in frequency, it's everything, let's say, in Fourier there's energy at all frequencies. And the output of a band-pass would mean that it only keeps a certain oscillation, a certain set of frequencies, in the output. Here a step has a response of going up and then coming back down. Is it a low-pass? Or a high-pass? Is it a **differentiator** or an **integrator**? Which are other ways to call, in the end, the usual differential equation that for heaven's sake I'll ask you on the exam.

If I have a step and the thing is, if you squint, the output follows the input, but more delayed, this is a low-pass. Beyond the fact that electrically it's a low-pass, there I have that the response is something like this. So, how did [?] you get close? It's a band-pass... I wanted to bust your chops because in effect, in the end, it's a **high-pass**, because for band-pass you would have had to tell me where it cut off, it was the fact of where it cut off. It's a high-pass, a differentiator, so the band-pass is a bit more complicated to see from the point of view of whether it's an integrator or a differentiator.

I call it a differentiator or integrator because if you mathematically take the integral, so I'm thinking of doing the integral from **0** or from $-\infty$ to $t$, where $t$ is an independent variable and is one end of the integration. In fact, I'm trying to, for each $t$, to express, to calculate the area, so the area under here is **0, 0, 0, 0, 0, 0**. At a certain point here the area becomes... it jumps and becomes small, as time passes the area becomes larger and larger. So this curve here is the result of temporal integration. Whereas this graph here, ideally if this were an ideal step (so a function... I should call them distributions, I'll take a break now, so a Heaviside step), the derivative should be a **Dirac delta**. But if you think of this as perhaps a ramp, the derivative is non-zero only where the argument changes in time, so here it's zero and here it's zero, here it's zero, here it's zero, and only where... when it changes in time is it non-zero. So this mechanism of frequency adaptation does a high-pass, it does a derivative, and derivative means emphasizing temporal transitions.

I'll stop for ten minutes. Thanks.

---

### Biophysical Mechanisms of Adaptation: Calcium-Dependent Potassium Currents

... ...

Good. So, the interesting thing about this phenomenon of adaptation or accommodation, which results from a functional point of view in a high-pass filtering, in a differentiator, in something that prefers changes (like in this case there was no current before and then there is current, here the response was "oh what a surprise" and then "how boring" and I go to *steady state*), depends on conductances, so on membrane channels, which are **potassium conductances**, and so far you probably could have gotten there, you could have anticipated it, because if at this particular current the frequency of the first spikes... I'm not getting the subjunctive wrong. So if at this current, the theoretical current without adaptation would be this, around **100-120 spikes/second** and then the frequency decreases, it means there's something that tends to pull the membrane potential down or anyway to oppose the current that Daniele Linaro was injecting, and it was a positive current. If he had decreased the amplitude of the current the frequency would have decreased, he didn't decrease it, the charge balance equation says that all these currents, including his, add up together, so there must be something that tends to have a negative sign, and the only thing I've told you about, besides chloride, that has a negative sign is the potassium currents.

---

And indeed this could have been due as a mechanism to a particular type of voltage-dependent potassium current, which is called the **muscarinic current**, it's also called $I_M$. Actually, in this case, we know that for this particular time dynamic, and perhaps if you take a look you might realize that here it's as if there were two time constants, two decreasing exponentials. There's a very rapid one that drops suddenly, in the order of **10 milliseconds, 20 milliseconds**, and then there's something that further brings it down over the order of seconds. So this is unlikely to be the $I_M$ current and is instead a current, it's not voltage-dependent but it's dependent on the ion concentration, and it depends on the ion concentration of intracellular **calcium ions**.

So as I imagine it, it's a channel where it's selective only to [sic] channels, to potassium ions, that this is outside and this is inside, there's a lot of potassium inside, so if these open, and this gate opens to follow the electrochemical gradient, the potassium ions tend to exit. Am I wrong? Is there a lot of potassium inside? No, it should be right. And this gate doesn't open because this channel senses the membrane potential. It opens because there are sites here where calcium ions bind and the more intracellular calcium ions there are, the more this channel is active.

Now why on earth should calcium ions accumulate inside the membrane [sic] of a neuron? I told you that more or less the dynamics of calcium currents are similar to sodium currents, so from a voltage-dependent point of view. There are in particular two types of calcium currents, one is called *low-voltage activated* and another is *high-voltage activated*. Anyway, they have a... those activation kinetics are different. Anyway, it does more or less the same, it plays the same role that sodium plays. So every time there's a damn action potential, just as sodium ions enter the membrane, calcium ions also enter. You know that calcium inside is practically zero, so as soon as some enters, it's a big phenomenon, so much so that calcium acts as a so-called **second messenger** in an immense quantity of biochemical reactions, of intracellular biochemical pathways.

And it's the key to linking the chemical world to the electrical world, because every time there's a spike there's an influx, a *puff* of calcium currents, to the chemical world. In this case, it's actually a kind of negative feedback, negative because these calcium-dependent channels are sensitive, selective to potassium and so the effect is to reduce excitability.

---

### Detailed Spike Analysis: Changes in Shape Over Time

One can notice... ok this would have been, but I should have removed it from the title, so I must remember that if I do this, then this can't appear here, so it's already a *spoiler*, either they are calcium-dependent potassium currents or sodium-dependent (the effect would be similar, there are both types, so calcium-dependent potassium or sodium-dependent potassium, it doesn't matter much), and so I would have asked you "what is it that acts as a brake on excitability?", and you would have answered, I'm sure, "potassium conductances". Chloride could also have been, but chloride is a bit particular because its reversal potential is roughly similar to the resting potential, and so even if it opens, the potential is there [sic], and so the famous *driving force* of the currents, $V - E_{\text{Nernst}}$, is practically zero, because $V$ is more or less around its potential. It has another effect, but we'll talk about it later, we'll talk about it for the description of synaptic currents. In this case, therefore, it's the potassium conductances that have this blessed low reversal potential, at **-80, -90 millivolts**.

The interspike intervals increase [corrected from text: decrease] over time, because there's something that acts as a balancer, it subtracts from the current that Daniele Linaro injects. And another thing that happens, which is clearly visible by taking the first spike (this is a real spike, which looks a lot like a fake spike, and it's interesting as a thing, from a mechanistic point of view, that this is possible). If I compare the first spike to the last spike, you see there are several interesting things. The first is that the **duration** seems to broaden over time. I'm zooming in on this, I'm looking at the very first spike here, and the last spike, or one of the last. To the naked eye at this zoom, they all seem the same and in fact, they are more or less all very steep. But just like in the Hodgkin-Huxley model, if you simulate it, the shape of the spikes changes very slightly. Here too, it broadens. The *reset*, if you like the **hyperpolarization**, seems to be less marked, it doesn't reach down here. Here the Y-axis is always the same. And another thing you see is that the **slope**, the derivative of the membrane potential, $dV/dt$, seems to be steeper [corrected from text: less steep], so there's a residual depolarization. The *slope* also seems steeper at the beginning and bends later.

And I want to point out that when I talk about the *slope* of the membrane potential, in effect I'm saying of the action potential, I'm asking what is the derivative. I know that the derivative is a function of time and that there I should specify the slope of the tangent line at a particular... I don't care, I'm saying that I'm in fact considering the time derivative of the membrane potential because I'm looking at it and because I'm not particularly surprised that it's more or less *steep*, so the angular coefficient of that line which, by Taylor, is the first derivative.

This thing here, for the charge balance equation, is exactly the sum of the currents. Now in the initial part, only sodium is open, so I can think that the *slope* *upstroke*, so the initial hit, the rising phase of the action potential for that time duration is only due to the sodium currents. So I, in effect, with the *slope*, with the derivative, am reading... this is a constant value that surely shouldn't change (it could change but on very long time scales if there were expression or synthesis, gene expression... genic, this expresses [sic], or so it either removes or inserts new copies of sodium channels, but this happens over the course of minutes, hours, days), what I see is the combination, this for me is a single quantity, so it tells me about this because $E-V$ is what it is but it's not so interesting, and this is the coefficient that changes particularly.

So looking at that I can imagine that after a while the **inactivation of sodium** has increased, so there are fewer available, non-inactivated channels. The famous $H$ has become lower, it's no longer at **0.6** at the beginning of the spike, it starts to be a value like **0.5, 0.4, 0.3**. And so all these features that are seen in an experiment of progressive depolarization mean that the potassium currents, *delayed rectifier*, can't reset anymore (so they must also be inactivating). The sodium currents, because of the *slope*, are manifesting a residual inactivation that I can't get rid of, and in addition to this inactivation of the sodium channels, there's also this further calcium-dependent potassium current that generally lowers the frequency.

So this is a real neuron, and to dissect it without using toxins and identify the currents, there are also these methods. Another very interesting method is to try to take a more complicated mathematical model that includes other currents, other mechanisms, and try to *fit* it mathematically. Is there a set of parameters that gives me the model's response identical to the experiment? If yes, as a first approximation... there would be a very long discussion to have, but I won't give it to you in this course. Perhaps you are familiar with the *fit* of a mathematical model to data, you could have local minima, in the idea that you have for doing the *fit* you are minimizing a cost function. This is everyday in the context of *machine learning* where the cost function, or it's no longer called a cost function, it's called a loss function, is minimized for a particular set of parameters, but this set of parameters might not be the only one, it might not be unique and it might not be the global minimum. And so if you have a model that does the exact same thing, it might not be the explanation, you might then want to open the model and say "show me what the value of $G_{\text{Na\_bar}}$ was that the FIT procedure discovered", but it might not be indicative of what reality is. So what people do is with genetic optimization algorithms (which have nothing to do with genes, it's just a mathematical name for some styles of minimization, of optimization of functions, functionals), generate a family of solutions, which instead in *machine learning* is not done with gradient descent, but that's another story.

---

### Ion Pumps: Active Transport

The **ion pumps**, I mentioned them to you, are mechanisms of active transport, and we haven't talked about them, we don't talk about them because from an electrical point of view, they don't contribute dramatically. They might start to contribute dramatically in contexts of this type where I have the ion concentration of some species inside the membrane, inside the neuron, for which there are also ion pumps that extrude, that kick out calcium ions (and now I don't remember what they pull in to be balanced), and so in theory, one should put them in if one wanted to mathematically describe this relationship as well.

They are in general... they are modeled, they are considerable as fixed current generators. In particular, the so-called **electrogenic pump** (electrogenic means it creates a current), depends on ATP, it's called the sodium-potassium pump, it exchanges **three sodium ions** that are inside the cytoplasm, kicks them out, and pulls in **two charges, two potassium ions**, so it's not perfectly balanced, and obviously, it needs energy because there's a lot of sodium outside, so how do I take it and kick it out? I have to use ATP, and I think here there's a... I found this guy who uses a *game engine* for 3D games that you all like a lot (I like them less because I'm from the Pac-Man generation) and he uses them to do visualization, if you like, at a very high level, it's not a molecular simulation, so it's not exactly the truth, but it's a *cartoon* that I like a lot. I asked his permission... I think on Twitter, I asked his permission, he said yes, to be able to show it in class.

And I'll show you one now of the sodium-potassium ion pump, and then I have, I think, a couple of others of GABAergic synaptic receptors, where the quantity is chloride. I think his *commentary* voice is on, and if it's too loud, I'll lower it.

> *[Video Commentary]*
> *This is the sodium-potassium pump. It's a protein complex with a crucial job, to restore and maintain the neuron's resting potential. The pump has an alpha subunit, here shown in pink, and a beta subunit, here shown in yellow. There is also a small gamma subunit, but you can't see it from this angle. To start, three sodium ions from the cytoplasm of the neuron bind to the pump. Using energy from ATP, the pump undergoes a shape change and releases the yellow sodium ions to the extracellular space. This conformation allows it to bind two green potassium ions from outside the cell. The protein then gets dephosphorylated and the pump switches its conformation back to the original state. It then releases the potassium ions inside the neuron and the cycle begins again.*

The thing that particularly inspired me is to be able to give you a kind of *depiction*, a graphical, intuitive representation of what it is for a modification of a three-dimensional conformation, also due to biochemical reactions like phosphorylation and dephosphorylation (but I don't care), one could have, against a chemical gradient, by a kind of molecular motor, an ejection, an exchange of ions that actually pass through this structure that allows the passage of ions in a membrane structure that, as I told you the other time, the first few times, is strongly hydrophobic, energetically impossible, otherwise, for an ion to penetrate this layer of the membrane. So a lipophilic and hydrophobic layer, made of fats, of lipids, but where water molecules cannot go.

It inspires me, but it's clearly not a molecular dynamics simulation, which is a simulation technique where the atomic positions, at the atomic level, of all the molecules are used, and things move, however, for femtoseconds, for picoseconds. Here it would probably already be at several hundred milliseconds. And for example, there's no description of electrostatic interactions. In another of the videos, he says that if he had put in his green molecules, which are potassium here, they stand for potassium, if they had had to repel each other as they are positively charged, he wouldn't have been able to do the simulation. Because obviously, it's computationally extremely heavy. Anyway, if you like it, I think I put a link on Teams to one of these videos, and I hope this *cartoon* can give you a mechanistic idea of how these mechanisms, in this case, an active mechanism, allow this game, this electrical dance.

---

### From Distributed Permeability to Discrete Channels

Now before taking a break in half an hour... I'll take a look because the topic is a bit heavy... I wanted to go back to the microscopic aspect, a little less phenomenological, of membrane permeability. In the era of Hodgkin-Huxley, I told you, it was thought that there was some kind of transporter exactly like the sodium-potassium transport pump, the sodium-potassium electrogenic pump, something that had an electric charge, because somehow it seemed it was something that depended on the electric field, on the membrane potential, but it was thought to be a uniform, distributed property of the membrane.

We had to wait until the Eighties, when these two gentlemen, **Erwin Neher** and **Bert Sackman**, two German physiologists (they are still alive, they are now emeriti, they have been retired for several years, Sackman, but also Neher, are very high-level scientists, after the Nobel Prize for discovering the existence of ion channels as discrete entities, they continued to do research at the highest level, particularly Sackman, on cellular neuroscience).

Anyway, to cut a long story short, this is a very interesting thing, just like the excitement I showed you, my excitement I showed you for the story of digging up the *thickness*, the thickness of the membrane on a purely electrical basis (because I know the formula for the capacitance of a capacitor, so I don't have a powerful microscope to see how thick the membrane is, I make an electrical measurement and I see it), so too in this case, we didn't see the membrane channels. And reconstructed as I showed you just now with the ion pumps, we didn't see them the first time, the first time we got an echo, an electrical image of a consequence of them opening and closing, and it's all due to the invention of a technique called the **patch clamp** technique, where *patch* means patch and *clamp* means clamping, block.

---

### The Patch Clamp Technique and the Giga-Seal

Where instead of using a glass pipette, making it much, much more pointed in what is normally called *sharp electrodes* (they are glass capillaries, borosilicate, which are heated in the center and several cycles of heating and pulling, heating and pulling are done, because when it heats up, as you know from the glass masters of Venice, glass, particularly borosilicate glass, melts... besides burning your fingers if you accidentally touch it), if you start to heat a lot and pull, you can have, as in some cases of artificial fertilization, *in vitro* oocytes that you often see or sometimes saw on television time ago (then for political reasons I think you don't see it anymore), instead of having a very, very pointed thing that penetrates the lipid bilayer as if it were a very fine needle into a wide-knit wool sweater (it doesn't break it, it goes right in), they had the idea of not making a *sharp* pipette, but of making it quite, not macroscopic, but quite wide.

Here the opening is on the order of one **micrometer**. We don't really talk about opening, but we talk about the electrical effect of resistance, when I take this pipette, I put it in a bath with electrolyte, I pass a current and I see what voltage I record. The wider the tip of the pipette, the lower the resistance.

It's called *patch clamp* because they had the intuition that if I... if one could manage to place it on the membrane **without penetrating** (so in all the previous lessons I told you poke, impale, penetrate, stab a neuron, thinking and hoping to give you the idea that the electrode is put inside the belly of the neuron). Here it's different, I leave it on the surface and this patch, this piece of membrane is blocked mechanically, it's a mechanical *clamp*, it has nothing to do with the *voltage clamp* technique of Hodgkin and Huxley. In that case *voltage clamp*, the *clamp* also had the meaning of blocking, but it was blocking an electrical signal, blocking the electrical properties, here *patch clamp* means mechanically.

And by doing so, there are different configurations they invented, but in the configuration that is represented here, they saw that simply by recording, as a first approximation, they saw that the current they could measure were very, very small currents compared to the currents one measures when penetrating the membrane, and one can do it because the signal-to-noise ratio changes and becomes very favorable, because here if I am a charge carrier, if I am an ion, I can't escape here in the gap in between. It is said that there is a **seal resistance**, that is, the mechanical contact between the pipette and the membrane is so tight that there is a resistance inside here to get back out (where I have the reference electrode) that is a resistance on the order of **gigaohms**, so *billions* of ohms, enormous, for all practical purposes infinite.

---

### Single-Channel Currents: Stochasticity and Quantization

If so, it means that if I have objects here that start to open spontaneously, since there's the electrochemical potential inside, sodium, outside, potassium etc., if it opens I see the electrical echo, I see a current that opens. And the surprising thing is that they saw two surprising things.

The first is that these currents occurred **randomly, stochastically, randomly**, it wasn't a deterministic thing, it wasn't a periodic thing, it was "here it opened, it was open for a bit, then here it starts from a zero *baseline* and the downward deflections you see...". It means it was an outward current in some way, but it doesn't matter, inward and outward doesn't matter. In some cases, the duration of the opening of this mechanism was a bit longer, then it was shorter, a little bit longer. Here it was almost a very brief opening that at the sampling time of this amplifier, of this recording, it barely registered. Here it's much longer, a few tens of milliseconds, etc., etc. So stochasticity, which makes one think that they are single molecules, that it's not a population effect. They are sitting at body temperature, **37 degrees** or **35 degrees**, I think these experiments are anyway done at a temperature very far from room temperature.

And the second thing is that the value of... so when these signals occur, the amplitude values are **discrete**, it's either **0** or **7 picoamperes**, there's no **6.5, 3.1**, so contrary to what you would see... You would see if, using Hodgkin and Huxley's toxins for example, you measured the residual sodium current, then you would see that at a certain point this current, in the end, we plotted it, when I plotted $M, H, N$ in fact it was that. There was something that varied continuously, here instead the experimental trace shows that there is no continuity, there are discrete [values]. In the end, it's like in quantum mechanics where energy is quantized, here the conductance is quantized, and it was such a revolution that these two guys won the Nobel Prize.

Because it was understood that these events, the fact that the amplitude distribution was strongly bimodal (meaning the histogram of the amplitudes is either **0** or **7 picoamperes**), meant that there were discrete states, it was either all closed or all open, and it was **one**, one channel they were seeing. Now, clearly, with hindsight, "yes, what else could it be?". But it took **30 years**, from the '50s to the '80s, for these guys, playing with pipettes over a Bunsen burner, pulling them, to get something that... for binding to the membrane, it's not a big problem. As anyone who has to wash dishes by hand knows, oil goes onto glass easily, you can't get it off. So when you put a piece of glass near a membrane, the membrane, for electrostatic reasons too, which are little understood, attaches easily. But the fact that there was such an accurate condition of a *giga-seal*, they say *seal*, *gigaohm*, for the gigaohms here, was revolutionary.

---

### Patch Clamp Configurations

There are actually different configurations of patch clamp that these gentlemen proposed. The one I showed you is called **On-Cell**, I lean on it, "eavesdropping" if you like, but "eavesdropping" there are holes, so something gets into my ear, it's not just vibrations.

But if, having arrived at this *on-cell* configuration, through the pipette connected to a small tube (in post-covid times we probably can't do it anymore, but normally the experimenter has their little tube attached to the pipette and sucks lightly) from a kind of mouthpiece, and this mouthpiece leads to the application of very intense negative pressure pulses, very intense because the cross-section of passage here is tiny, where I put my mouth is relatively large, it could be a couple of millimeters, then there's, say, a meter of silicone tubing and here it's one micrometer. So correctly I should say that the pressure is the same, but the force... pressure is a force per unit area, so if the area is very small, for the same pressure, it's an enormous force.

You actually manage to break the membrane, it's as if you were taking the cap off this patch, it simply dissolves and disperses into this pipette which I'd point out is enormous, it's like... it's the volume of a tennis court compared to a ball. The ball is the cell and the content of molecules, ions, etc., is that of a tennis ball. The pipette contains liquid in a volume as enormous as that which could be represented by a tennis court. So ok, yes, there are pieces of membrane and junk from the cytoplasm that fly inside, but after a few milliseconds, they no longer matter.

And I see it electrically, I can realize the fact that I've arrived in this configuration, because while here the potential I was recording might have been the potential of the extracellular medium, as soon as I "uncork" this system, inside here it's **-70**, so on the oscilloscope, instead of seeing **0**, after I suck, I see that *boom!* the waveform of the potential of the pipette I'm recording goes down very rapidly and indicates to me that I've entered, that I'm in **Whole-Cell**. Typically an experimenter, especially at the beginning, gets very *pumped* when they manage to do this procedure here and the cell doesn't die, because the thing that sometimes happens is that the seal isn't perfect and when one applies a negative pressure, simply the membrane here breaks, but the cell isn't attached, so one continues to see **0 millivolts**, which is the potential of the extracellular medium.

There are other ways in which, without sucking, by simply retracting the pipette (the pipette is attached to three piezoelectric motors, which are along the XYZ axes, so it can be moved in space), one simply retracts it and a little piece of membrane remains stuck to the pipette. If one is lucky, one could take this pipette and put it in another container where the composition of the liquid present there has concentrations of ions or drugs controlled by my choice. And in this way, I can expose the intracellular part to the concentration I say, and so, for example, study this type of channel, potassium-calcium dependent, which sense the ions inside, but I inside a cell will never be able to change things, I instead want to change them at will. I can do it if I have this **Inside-Out** technique. The *inside* of the membrane is *out*. And there's another technique that's a bit more complicated that, in other terms, so after the *whole-cell* is established, one can have the patch of membrane that was removed in an inverted configuration where **Outside-Out**. The *outside* of the cell remains *outside* the pipette.

---

### From Single Channel to Macroscopic (Population) Current

This is simply nomenclature. The interesting thing is that these two researchers started to test not only ion channels, but they studied their voltage-dependence and they saw that what is normally a population characteristic, which you see below. Here, this is an example of a *voltage clamp* experiment that Hodgkin-Huxley might have done. You give a step, *voltage clamp* means you impose the potential and measure the current. You measure the current that the amplifier must constantly deliver so that the potential remains constant. If the potential remains constant, $C \cdot dV/dt$ is **0** and the current I have to inject is representative of the ionic current that the neuron is evoking at that moment.

And so you see here, this, even without knowing, this is... ok it says potassium, this is a response of the usual boring differential equation, etc., which is the dynamic equation of $N$, there's only $N$ in the potassium channels, and I give it a step, these open, they have the usual charging curve with the delayed kinetics that characterizes potassium of a few tens of milliseconds or whatever it is. This instead is sodium, so here I'm observing in effect $m^3 \cdot h$, I'm observing them together. In fact, I see an activation and a cumulative [sic] inactivation, sorry, an immediate rapid inactivation that follows. In fact, I see that when I give it this step, the sodium first activates and then inactivates. The potassium, instead, only activates. This is the trace of the potential step I'm changing, first I hold it at **-80 millivolts** simulating a resting condition and then I give it a "flick" [sic], let's say a step, and I hold it at **-30 millivolts**, for potassium I do it at **0 millivolts** and I verify that this is what Hodgkin-Huxley described.

If, on the other hand, I do it with these electrodes and these experiments by Neher and Sackman, where I can isolate, also due to the size of the *patch*, one channel or two channels, a very small number of channels, I can measure the so-called **single-channel currents**. And when I measure them, I see that the behavior is stochastic, and if I do it over and over again, I see that statistically this sodium channel, which is always the same (but you can think of it as representative, this is the **ergodicity hypothesis**, as representative of a population. If I take a single Italian, even if it's a flawed example, I can think that if I ask him a series of questions, the time average can be exchanged with the ensemble average over a set of Italian people I could interview by asking them only one question. This is called... this property that is hypothesized to exist, it's not easy to verify, is called the **ergodicity property**: a system is ergodic if you can exchange the time average with the ensemble average, just to be clear, the integral over time with the expected value for those of you who remember probability theory).

So here it's as if I were seeing a population of channels, because it's a single channel repeated over time, but it's as if they were $N$ channels if they are independent. Here I get independence for free because now I do this trial, then I let some time pass, I do the trial again, there is no dependence, no memory in time, if I wait a sufficient time for the transients, if any, to be forgotten. And the same thing is therefore representative of statistical independence.

And you see that the opening of the sodium channel happens statistically only in proximity to this step, while that of the potassium happens repeatedly throughout the experiment in which I'm holding the cell depolarized or a bit more depolarized. It turns out that if I **algebraically sum** these curves, I get the ensemble average. So if I do the algebraic sum, which means the arithmetic mean, so I take the population average, I get the macroscopic behavior. And it makes sense, because I normally, from Hodgkin-Huxley onwards, I only measure the macroscopic effect of the population, I always see, *patching* a neuron, a choir of voices. But if I could isolate them one at a time, I would hear that, for example, this one is particularly out of tune, it's not that it has a characteristic that first it opens, then it inactivates with a kind of exponential time constant. This exponential time constant comes about because some of these occasionally open again later, but mainly they open here and then tend to stay closed. When I do the algebraic sum of many repetitions, I see that the sum of the events tends to become smaller and smaller in a gradual way.

---

### Noise as a Fundamental Biological Feature

If you remember, I mentioned to you that in the Markovian description of ion conductances, voltage-dependent or non-voltage-dependent, there are two interpretations. One is the deterministic interpretation, the law of mass action, the other... and differential equations come out that are always the same, but in the other interpretation, I have to think that those arrows between the states have the meaning of **transition probabilities**, not conversion rates. And I want to see if by chance I can learn something from this.

It's interesting to learn something from this because, for free, I see that I have a **source of noise**. This is a nice *paper* from several years ago, a *review*, a compilation of a lot of previous studies in which the authors summarized and listed all the cases where the functioning of the a system has, in favorable or unfavorable terms, a certain type of noise: the sensor noise in the case, for example, of phototransduction or mechanotransduction; a noise due to this opening and closing of ion channels, which are not deterministic devices. They *flicker*, like a screen *flickers*, like an untuned screen flickers, in an unpredictable way. Now I'll show you other examples.

Another example of noise is that of **vesicle release**. Professor Zoli probably told you the little story where when the presynaptic button is invaded by a depolarization, by an action potential, the synaptic vesicles (*kiss and run*, it's called) fuse from the intracellular side of the membrane, they release a diffusion of what was contained in the neurotransmitter molecules that were contained, etc., etc. This is on a microscopic, molecular scale, it's not that all the vesicles do the *kiss and run* (the *kiss and run* must also be the one at airports, where one *kisses* and then leaves). Here they *kiss* the membrane and then leave, it doesn't happen deterministically, sometimes things don't work, sometimes you have a *failure*, a failure of release, sometimes the neurotransmitter doesn't bind right away, it diffuses in the intersynaptic space and is dissipated.

Furthermore, there are a lot of conditions where, without doing anything, without a presynaptic potential, the synaptic button is as if it were incontinent and it leaks a little bit of neurotransmitter molecules, so the vesicles fuse on their own spontaneously because there are fluctuations in the concentration of potassium [sic], sorry, fluctuations in the concentration of **calcium**, of calcium ions in the synaptic button. The mechanism is so sensitive that it only takes a few calcium ions to enter to activate the mechanism, so it's a relatively delicate thing, it's not that you need very robust signals. So synaptic release is also subject to spontaneous activity, to noise.

And all this noise, even if it makes one think, particularly an engineer, that it's something unfavorable, it's probably a fundamental ingredient of the functioning itself. So the signals you go to record are not only noisy because the amplifier you are using is noisy, there is electrical noise (it's called, for example, **Johnson noise**, it's related to temperature, to resistance, there are noises related to the use of operational amplifiers), ok, but biology itself, one might say, "sucks", it's *sloppy*, it's imprecise. But this imprecision is probably a *feature*, not a *bug*, it's a characteristic, not a flaw.

---

### Variability of the Neuronal Response: The Effect of the Stimulus

And I'll show you an effect that was described several years ago and that is disarmingly simple. With a computer simulation, like the one I gave you of the Hodgkin-Huxley model, you don't see this.

Every time you take a neuron (in this case it's a pyramidal neuron from the somatosensory cortex of a rat, it's the same type as those we use in our lab for some experiments of another type), if you give a step of current (now you know it *ad nauseam*) the neuron starts to fire in a more or less regular way... maybe you can glimpse, maybe not, that there's frequency adaptation... here you see many jumbled traces, deliberately *ad hoc* to create an effect, because it's the effect of a **repetition**.

You do this stimulation which lasts about a second, about **700-800 milliseconds**, then you wait a few seconds so that the membrane potential has gone to rest, you do it again (you certainly don't wait an hour, you wait some fraction of a second) and you repeat it, for example, **25 times**. Each time you superimpose the traces and you realize that it's as if... you will never see this in a model because the model is deterministic, if you repeat the same simulation, the same thing comes out every time. In the case of a neuron, no.

I'm somehow trying to tickle you with the fact that beyond the fact that from one moment to the next you have other things to think about, you probably have stimuli that have consolidated in your memory, your foot started to hurt, your butt started to hurt because the seats are uncomfortable, so your state is not stationary. But if I were to repeat the same thing to you **25 times**, maybe with exactly the same words, the same intonation, etc., in your auditory cortex, I wouldn't see the same *spike train*, I'd see a slightly different spike train, as I see here.

The interesting thing is that the first two or three action potentials are relatively ok, reproducible. As you wait, there's a big... in the sense that the time at which, the instant at which they are fired (you can see it well here, in this, which is called a ***raster plot***. *Raster*, I don't know where it comes from, I think it comes from how old televisions worked, which had the electron beam that moved in lexicographical order from right to left). Here, each repetition, I put a little bar at the instant of the peak of the membrane potential, so where there's a spike, I put a bar. And you see that the time at which the first, the second, already the third, is fired, ok, seems to have a remarkable reproducibility compared to the last one. How many are here? One, two, three, four... I almost have trouble counting them. Suppose the tenth one is completely *scrambled*, out of phase, messed up (it comes to mind, so the word *scramble* is used because it has to do with gambling, like with a deck of cards that you shuffle to randomize it), almost on the same order of magnitude as the time between one spike and the next, of the same inter-spike interval.

So it's a remarkable thing and one says "but what the hell, but how is it possible? I have an absolutely advanced brain etc. and I have elements that are so *sloppy*, so irreproducible, and yet I seem to have a remarkable performance in the world".

If instead of giving a DC stimulus, so a constant stimulus, you give a stimulus that's always the same, but that varies over time (this is technically a, it's called, realization of a stochastic process, or some way to generate random numbers, I fix the so-called seed of the random number generation and I generate the same thing). So I have something that fluctuates, as the reality of the tiger entering could be, of the fact that the world is non-stationary, so it's very different from the stimulus I've sold you so far as useful, because for this, for an RC circuit, I even have an analytical solution. When I do this, you see that it practically becomes an extremely accurate thing.

---

### Noise and Creativity

So it's a kind of noise generator, and the story of the noise generator, I like to think it could be linked (no one knows and it's very complicated to know) to **creativity**. How do ideas come to your mind? Beyond a deep cognitive aspect, partly linked to free will, partly linked to the unconscious, to awareness, whereby if someone tells you "think of a European city", something probably comes to your mind, can you introspectively describe the mechanism? No, it just came up. Probably there is no free will because you have sources of noise (for example, synaptic release, etc.) that provide an input similar to this and some random thought comes out, some...
clearly on the basis, correlated with what the activity or the structure or the memory or the synapses are, etc., etc.

So this characteristic of surprising, of being able to give unpredictable answers, can be an *asset*, it can be a useful thing from the evolutionary point of view of survival. If I have something that always responds in the same way when the tiger enters or when I see a particular apple and I never eat it, ok, maybe I'll go extinct. The one of us, of the *ancestors*, of our ancestors, who by pure chance had a whim to take the apple and didn't die, started to propagate this characteristic of ion channel noise. In reality, I don't think there are ion channels without noise, I think it's the nervous system that has evolved to use, to make good use of the noisy characteristics.

Now let's take a 10-minute break, and in the last part of the lesson, I'll tell you how, by digging up that stochastic description, how to explain or how to describe this type of electrophysiological signal. Thanks.

---

### Note on Transcripts and Feedback

Thanks. ...relatively easily the *transcript*, the *speech-to-text* transcription. When I do it, obviously it's something that isn't readable because I pause and it's not great, but I've seen that by processing them, feeding them to a *large language model*, threatening that if it summarizes (I've removed that from the *prompt* now but I would have been very angry), the result is that it even writes the equations for me with LaTeX, which is a formatting method, and it makes little paragraphs for me.

Take a look at it because at first glance, but I don't have time to review everything, it seems to be useful, it seems that the text is reworked, it's not bad. I don't think it can be considered a lecture note because it's at an informal level, there are no figures, etc. If any of you can take a look at it, you can tell me "look, after reading the first two or three pages it becomes complete hallucinations". I've glanced at it and it doesn't seem so, and I was particularly surprised by the fact that when I speak and say... it writes the mathematical equation for me... it's surely not just a reorganization into paragraphs because I did *prompt engineering* telling it "you are a neuroscientist, you have bioengineering experience" so I gave it a lot of stuff. But if you can give me *feedback* on whether it works or not, because it's a potentially interesting thing. I've also read in the literature that the *transcript* is considered very useful by students, but that almost no one looks at it during the course. This is ok, but...

### Cell-Type Specific Reproducibility: *Fast-Spiking* vs. Pyramidal

Ok, the interesting thing is that not all cells do this trick. Here it would seem that on the left, for this stimulus, the noise generator is on, while here on the right the noise generator is off. There are other types of cells, which are cells of the **inhibitory interneurons** that are called ***fast-spiking*** because, for the same injected current, they have a spiking activity, an oscillation frequency much higher than a pyramidal neuron. So this one is excitatory and this one is inhibitory. For the same current, this one fires **4** spikes, this one here for the same current fires **3, 6, 9**, it fires about ten, and what is this? **25 milliseconds**... it must be **2.5 seconds** [sic] ...so it fires probably **4** times as much, **3, 4** times the same frequency [sic]. So, by the way, their frequency-current curve is much steeper because then maybe it saturates too, but compared to pyramidal neurons, for the same current, at the same point on the x-axis, the number of spikes per second is much higher.

Here you see it's the same experiment, done among other things by a very, very smart Italian, Alberto Bacci, who is in Paris now though. So in this case, there are **15 repetitions**, unlike the pyramidal neuron, an excitatory glutamatergic cell, where by the second, third, fourth action potential the *jitter*, the flickering is dramatic. Here at the third, fourth, fifth, nth action potential the error starts to accumulate, as if there were an error accumulating, but it's much lower.

Why does this happen? It has something to do with the dynamics of the channels. Here, by comparison, the fourth spike, this is the *raster plot* of the fourth spike, practically all the times over **15** successive repetitions the neuron fires precisely. Here, no, here it's completely irregular, *scrambled*, altered, irreproducible, it doesn't reproduce itself.

---

### Stochastic Models: The Role of the Number of Channels (N)

So, the way to describe these things, to try to interpret them and eventually build theories or models that help us interpret electrophysiological signals like these, we remember that individual channels, like the ones I showed you, are discrete entities and when I consider them at the level of the sum of the membrane's effect, which you remember beyond the concept of Kirchhoff's laws, of the parallel [components], of the conductances, in the end, I'm summing the currents in this charge balance equation.

Now this graph here is slightly difficult to look at, at first glance you say "ok, here is one channel, here are 10, here are 100". Every time I make one of these graphs I'm rescaling the Y-axis, that is, here the individual channels have a small conductance, the current they pass when they open is a few **picoamperes**, whereas when I have **10**, if all **10** open, probably here the majority are open, I should have **10** times that few picoamperes. Suppose **5 picoamperes**, here it would be **50**, so the distance between here and here is not the same distance as between here and here. I'm rescaling the axes to show you that, it's true, the quantities are noisy but very, very small, because if I have a good, very large population, the noise is negligible, the fluctuations are there but they are very negligible.

The important thing for me was to try to clarify that when the number starts to be... it doesn't need to be infinite, it's enough for it to be on the order of hundreds of channels, it's as if the fluctuations can be negligible. In black here I've put the case $N$ tends to infinity, which I haven't drawn for $N$ infinity, but for example, I invoked it using the equation I already know I can use, the deterministic equation. $N_{\infty} - N$. I used this one, which I know holds in the deterministic case, that is, in the case where there is an enormous number of channels, so much so that I even describe them with the law of mass action, just as I describe the reactions of sodium chloride binding, dissociating, etc., in a way, not *bulk*, it's wrong to say *bulk*, in a macroscopic way, with an enormous number of particles, of molecules.

### *Channel Flickering* and Finite Number of Channels

And this **channel flickering** is worth trying to understand. This is not an experimental trace, it was me who used the Markovian kinetic scheme, and now I'll show you how, to describe it.

The idea is, if I can figure out, with paper and pen, more or less where this noise comes from, I could understand if by chance this noise, as it seemed to be here, maybe depends on the fact that here the trace of the membrane potential during this periodic activity doesn't get particularly hyperpolarized. Here, because of these fluctuations in the input, cases are frequent where there are these *swings*, these elongations, these stays of the membrane potential at more hyperpolarized potentials, where the membrane channels are predominantly closed. It's as if, I'm thinking, could it be that this noise is greater the more it depends on the state or depends on the membrane potential? Is it a noise generator that depends on the potential?

And if so, it could be interesting and intriguing to understand why *fast-spiking neurons* have a different behavior from pyramidal neurons. Ok, they have a different set of channels. Or they have a very large number of them, or maybe simply in the axon of these neurons, here there's a point, for example, in the **axon initial segment**, which is the initial segment of the axon, where the action potential is generated, where maybe here it's not that you have $N \to \infty$ channels. You have a number that's perhaps small, or you have some type of... in the axon or in the dendrites, you have a concentration of ion channels in a reduced number.

If they are reduced, they start to fluctuate, or rather, they always fluctuate, but if they are reduced, like a small choir, you clearly hear the out-of-tune voices. If it's an enormous choir, no, and it's always the same thing, in the choir too, the voices add up.

---

### The Stochastic Markov Approach for Single Channels

Summing algebraically, when I talk about an average, the average in my town is the summation from $1$ to $N$ (twenty?) of $x_i$ and then I divide by $N$. Do you agree that even if I don't divide by $N$, roughly the mathematical operation is the same, I'll just have a scale factor. If the fluctuations of a choir, of a population of channels, somehow become a little less negligible when I take an average of an enormous number, it means that when I have the charge balance equation (which is gone), which was a summation of currents, there too I have a sum of effects. If the channels are few, it's as if I had an average but of few elements, so it becomes noisy. Now maybe it will make a little more sense.

There are two ways to approach this problem. The first is the **Markovian** one, the second I'll tell you later, maybe next time. And this is the simplest and it is: I already know that an ion channel is a system that goes through a series of morphological, functional configurations, for which a number of states exists with a number of transitions, and there is probably only one (in experimental reality, it's typically like this), there is one state in which the ion channel is open, it's conductive, and then the elements that are on these arrows of this Markovian kinetic scheme are either constants or they are voltage-dependent or they are dependent on ions, on ion concentrations, or they are dependent on the concentration of an extracellular neurotransmitter in proximity to the receptor. In the end, the receptor/ion channel are the same beast, more or less as a first approximation, not all synaptic receptors are ion channels, I'd be talking about the so-called **ionotropic** receptors, not metabotropic, but we'll see that next time.

So I already know this type of description, I've used it and I had told you "ok, here for each state write a differential equation etc. etc.", but wait, no. Here I think that now this is the description of **one channel**, not of a population of channels. In the chemical case, it's not a kinetic scheme that tells me how a solution in which there is sodium and chloride... no, I'm thinking of two molecules, one of sodium and one of chloride, they bind or dissociate. Even intuitively, you imagine it as something that *flickers*, that is subject to probabilistic, not deterministic, events.

So I can think of associating (I'll show you how it's done in the non-deterministic case, in the microscopic case), I associate with this channel... with this $i$-th state of the channel, I think of having $N$ of these ion channels, suppose they are all identical, each of these travels between these states, jumps according to what the voltage-dependent properties are or whatever they are, and for each one, I have a binary variable that I call $S$, state (or I don't know why it's called $S$, I believe in accordance with a nomenclature and a notation from statistical mechanics, but it doesn't matter). This is a Boolean [sic] variable, it's **1** with probability... so it's a random variable, and it is... it takes the values **1** with the occupation probability of this state, otherwise (so with probability $1$ - the previous probability) **0**.

And so far, one says "ok, in the end, I imagine I want to write the current through this channel and I want to put in it for a single channel, I don't want to put $m^3h$, I want to put this $S$, when $S$ is **0** the current is **0**, when $S$ is different from **0** the current is ionic, for that single channel, it's non-zero".

---

### The Fraction of Open Channels as a Random Variable

But I don't just have one of these channels, I have several. In this horrible graphical way, it's as if I'm imagining them as systems that are identical and work in parallel, they don't talk to each other, there's no so-called **cooperativity**, that is, the fact that ion channel number **22** out of a billion is in the open state doesn't change the occupation probability of another channel, even if nearby. They all read the same control variable, they are all intercalated in the membrane and they all "fire" [sic] at the same transmembrane potential, but there's no additional interaction. I think I'm telling you this for the second time in this course because the concept of cooperativity is generally very important in biology and it seems that at least ion conduction in vertebrates, in the central nervous system of vertebrates, rodents, primates, human and non-human, doesn't seem to have these characteristics of cooperativity.

So what I can think is that there isn't a single $S$, there are many, $S_1, S_2, S_{1000}, S_N$. Each one has a probability of being **1** or **0** according to the probability of that channel being open or closed, so they don't talk to each other.

So what I can do is that the famous **fraction of channels** in the open state, which I sold to you before in this way, a few lessons ago, now I write it according to the definition. How many channels did you say there are? There are $N$. Ok, so if I take these Boolean variables, these binary variables (sorry, not Boolean, binary), stochastic, random, and I sum them together, I have something that at most becomes $N$. But it's a random variable in turn, it's a sum of random variables, and $1/N$ normalizes the sum and makes it become a fraction. Ok, so in theory, if I had some contraption, some mathematical machinery that describes these random variables for me and makes them go from **0** to **1** according to the non-deterministic Markovian kinetics of this kinetic scheme, I put it inside, I replace where I had in Hodgkin-Huxley, I replace the $N$ ($N^4$ if you like, but all of $N$), I put this in, and in theory, I should be able to simulate the excitatory activity when the channels are stochastic.

And in theory, it's the simplest thing in the world, you don't have differential equations, you just have to generate on the computer some kind of mechanism that becomes **1** with a certain probability and **0** otherwise, and it, let's say, *flickers*, it describes when this channel is here, if at the next step it stays here or it makes the transition, if it goes here, if it comes back... that is, you have to implement the Markovian kinetic scheme in a different way from the equation of mass action, from the law of mass action. For the rest, you put it inside the same currents.

---

### Simulating Stochastic Transitions: Probabilities and Random Numbers

I'll pick up this discussion on transition probabilities again. You see it's not particularly difficult. Here I told you one or two weeks ago (when you're having fun, time flies, I can't remember if it was last... whatever last week was), in which the arrows of that damn kinetic scheme, even the one from a moment ago, are to be interpreted as **transition probabilities**.

Here you have the probability of making the transition from $A$ to $B$ in a time interval $[T, T + \Delta t]$, given that you were in state $A$ at the previous time $T$. Conditional probability where we talk about an interval because continuous-valued random variables do not have non-zero probability over an integration interval of zero measure. If you ask me "what is the probability that the phone rings right now?", at this instant of zero measure, in this time interval of zero measure (because it's not an interval, it's a point), it's zero.

Here I say it's an interval, I call the interval... I take it in $\Delta t$, suppose one second, half a second, one hundred milliseconds, whatever it is, and I say that this $k_1$ and $k_2$ are the transition probabilities [per unit time], so that the probability of passage is given by $k_1 \cdot \Delta t$ + higher-order infinitesimals. This thing that seems a bit... that can be intimidating, simply means that I assume that this probability is a complex function as you please, but that it certainly cancels out when the interval... [it's] a function of $\Delta t$ and when the interval is zero the function cancels out, because the probability for $\Delta t = 0$ is $0$ (the phone didn't ring and even now it hasn't rung).

And this is its Taylor series expansion to the first order, and I say that what I put on the arcs of that kinetic scheme is the first term, the first order. The $0$-th term is $0$ because the probability for $\Delta t = 0$ is $0$, so it's a function that, evaluated at the point where I'm doing the Taylor series expansion, is $0$. And in a bit, I won't care about the higher-order infinitesimals, but an approximation... written like this, however, it's an exact expression.

This object here, I can simulate it on the computer in an ultra-simple way. You may or may not know that, for centuries now... no, for decades, computers can simulate... all the video games you play work with a **random number generator**. There is, obviously, an active academic research field, in particular also linked to cryptography, to the fact that you, in theory, must, to protect communications, you should try to be able to generate something truly random. And this is obviously very complicated because computers are deterministic, because von Neumann architecture, Turing machines, are objects described by laws, they are not *wetware*, they are *hardware*, it's not wet, it's a silicon thing that works, among other things in a bad regime where the transistors are... but it works and we even have *large language models* and *deep* networks that classify kittens or cute... dogs.

So random number generators are intrinsically deterministic objects that, however, have a behavior almost indistinguishable from the random case. One thing that enlightened me years ago was when a professor in class showed me this graph where he told me: "If by chance you have some way to generate a function $f(x)$, which is made like this, a periodic function (so these are lines), it's related to the remainder of the integer division... $R$... I don't know, some $x$-bar which is this interval". If you have something like this, it's a deterministic function, in which, that is, and it is a function because if I enter (so a function is a unique *mapping* where one element is associated with one and only one element, suppose it's here, one and only one element), so this is the output and this is the input. In this direction, it's deterministic, but if I tell you I got this value out, you can't go back to what the value was that generated it. So in particular in the case of a sequence, a function similar to this or conceptually similar to this. I think these classes of random number generators are called congruential, something like that, there's this word *congruential*, and I don't remember anything else. It becomes, to all intents and purposes, unpredictable what the value of the previous sequence was. And typically these methods cause the output value to be recycled to the input and a sequence is generated. And this sequence is almost indistinguishable from a stochastic process.

---

### Algorithmic Implementation: Generating Stochastic Events

What you have in Python, Julia, Matlab, whatever it is, you surely have some function, either from a library or whatever, that generates a **pseudo-random** number for you. It's called pseudo-random because, unfortunately, complete randomness is achieved with hardware techniques that I won't tell you about. You have something that you invoke, and it generates a number between **0** and **1**.

When you generate it, you might ask yourself the following: suppose $R$ is one of these random numbers and it's generated between **0** and **1** (suppose **[0, 1)**, it doesn't matter, they can be open as intervals). Could you answer the question, what is the probability that $R$ is $\le 1$? Can you tell me what it is? **100%**, **1**, perfect. Ok. And can you tell me what is the probability that $R$ is $\le 0$? It's between **0** and **1**, so **0**. Ok? Let's say this shouldn't be enough for your intuition to say, to say... "yes, it doesn't matter". No, it's open. You're right, but it's not essential. Here it is...

This ingredient should make you suspect, if I need to synthetically generate a probability that something occurs, so I can generate, pretend to simulate a coin, the toss of a coin whose probability is **0.5**. Can I simulate an event on the computer that happens **50%** of the time that I call that function? Here you have a clue that if the thing you put here, so if you use this, if you make a comparison $R \le \text{something}$, it would seem that what you put here is effectively the value of the probability of the pseudo-random event that you are synthetically generating. If I put **0**, it's **0**.

Now the next step would have been to ask you what is the probability that $R \le 0.5$? I must add, I must emphasize one thing, that random variable is **uniform**, it means that the probability density function (here obviously you are more shy) is constant between **0** and **1**. It's not the probability distribution, it's the density function, its [sic] derivative, but anyway, it doesn't matter. It means that there's more or less the same probability that it will take any value between **0** and **1**.

And it's effectively true, if I run it, I take Python... no Python 3, it screws me now because I don't remember from memory... Obviously, it has to do the update now. Interesting. `python3`. Let's see what a fool I make of myself. And I do `import`, how do you say it, how do you do it... `from numpy`... no. `import`, how does it go, `numpy`, thanks. `numpy as`, how do you say it, `numpy` as, how do you do it, `np`, `as np`, thanks. Like this? Go, *sorry* thanks. And `np.` there's no completion... yes, there's `random`. Ha ha. `random.uniform`, damn you for changing the... I hate this, I hate Python, I hate Python. It's too annoying, that is `np.random.uniform` and probably I have to give it `0, 1`, *low, high*.

If I did it a large number of times, I accumulate these things in a vector (if I were cool I'd do it, but I won't because I'll definitely not make a fool of myself), and I made the histogram of all these terms, you would see that roughly all the values have the same probability of occurring. In fact, they are spanning between **0.1, 0.08** which was small, **0.9, 0.09, 0.2, 0.3**, etc., etc.

So this is the fundamental ingredient. And in this case, when I say "what is the probability that $R$ is less than a certain quantity", in theory, graphically I should say, I'm asking what is the integral between **0** and **0.5**, so what is the area here? And the area here would be exactly **0.5**, so again it seems to work, whatever I put here is equal to the probability of the event I want to generate. So if I want to generate an event with probability $k_1 \cdot \Delta t$, I have $R$ generated and then I ask, `if R <= K1 * delta_t`, then print "the event happened", otherwise "the event didn't happen". Remember that it's a probability of an event, I want, like a coin, I want for example to simulate if a coin tossed in the air results in heads or tails. Suppose I want heads, I must have some *if-then-else* that tells me, I compare $R$, the algorithm is simple, I take $R$, I generate it, I compare it with a quantity I call $P_0$. The times this logical condition is true, it happens with probability $P_0$, it happens $P_0$ times percent out of one hundred times. If you want, you can try it.

And it's linked to the property, so the probability density function is linked to the probability distribution function because it's its derivative, and so to go from one to the other there's an integration operation, and here it's assumed by convention that before **0** it's **0** because I don't have values of $R$ less than **0**, and so it's this area. This area fills up proportionally, given that it's a rectangle of unit width... I forgot to say that this, being a probability density function, means that the total area under it must be **1**, because probability is one of Kolmogorov's axioms, it must be **1**, and so the height of this rectangle must be such that multiplied by the base, which is **1**, it must give **1** area, and so the amplitude is **1**. Knowing the amplitude, I can play this game of the integral and a fraction, and it happens with a certain fraction, so **0.8, 0.8** etc.

---

### Simulating a Two-State Channel

So this is the algorithm, and with a wonderful computer simulation from a few years ago, which I have to redo because it's pitiful, I can simulate... Simulate exactly the current, now I'll show you, but in the end it is: I generate the random number and depending on the state I'm in, if I'm in the open state I can only make a transition to the closed state, and if I'm otherwise... I'm not in the open state... I can only make a transition from closed to open. So $\alpha$ and $\beta$ (you see here $\beta$ appears and here $\alpha$ appears) are not all the possible cases.

And what I can generate in this very ugly, aesthetically very ugly, very ugly representation is the opening and closing of an ion channel, stochastic, which has two states, which has a single state, only one of the two that is open. And the thing I did between one and the other, maybe you can see it, maybe not, one of the two probability values per unit of time, so $\beta \cdot \Delta t$, in this case, is **0.01**, in this case, it's **0.1**, so I close much more easily, much more frequently, if I open, I close right away. In fact, you see that regardless of which one I defined as open or closed, here the openings are sporadic, here instead, openings and closings are much more frequent.

### Computational Complexity of Stochastic Models

So in theory, I could put something like this, at this point I can only do it numerically with a computer and run a simulation, and simulate... here's the disadvantage, I have to simulate every single sodium channel, every single potassium channel. In each of the two cases, I'll have $\alpha$ and $\beta$ that depend on the potential exactly with the Hodgkin and Huxley formulas, on those kinetics, $\alpha$ and $\beta$ are complicated functions of the membrane potential, but I have to track them all, that is, in memory, I have to keep track of the state of all these damn... I can't use a single variable for all of them, I must have $N$ variables, $N_{\text{sodium}}$ for sodium, $N_{\text{potassium}}$ for potassium.

So computationally it's easier [sic, likely means 'conceptually simpler'], it allows modeling, describing, exploring the stochastic component, but it's extremely complex, extremely heavy from a computational point of view because if you want to explore a typical case you might have some tens, hundreds of thousands of sodium ion channels or potassium ion channels. And you have to do this algorithm hundreds of thousands of times per unit of time, because you have to do this at every time step. If the time step were **0.01 milliseconds** or even less, there you go, welcome to a world of considerable complexity.

---

### Linking Stochastic and Deterministic Models

I don't remember what I have to tell you... if any of you have had him, it means I'm old enough to have colleagues who were young, who are no longer young either.

I wanted to try to give you an insight, an intuition, and an explanation of why when you treat the deterministic case, exponentials come out, continuous things come out, but in particular, exponentials come out and time constants come out. When instead in the case... I have nothing that tells me there's a time, that there's a time dynamic with a time scale. I have $N$, **100,000, 10,000**, or even **10, 20**, entities, each of which *flickers* and fluctuates between, for example in this simple case, the open state and the closed state.

### From Transition Probability to Occupation Probability (Master Equation)

I want to do it in a simpler way, so that here, with paper and pen, we can manage to do something. So the kinetic scheme I have in mind now, I think I'll want to use $A$ and $B$ to be able to write $dA$ and $dB$, to be able to say, pardon, not to write $dA$ and $dB$, because I don't want to treat it deterministically, but I need to write the transition probabilities. But I want to do one more thing, I want to write the **occupation probability**, which is not the same thing as the transition probability. The transition is a conditional probability: I'm in a state, for example open, and I can only close, what's the probability? If the probability is **0.5**, it may be that **50%** of the time I don't close, I stay in the state where I am. It's different to talk about occupation probability, which means "but what is the probability of finding that channel in that state at time $t$?".

I'll show you where the exponentials come from. So here I've written it, this $P_A(t)$, I've written it as... I'm alluding to what is the probability of being in state $A$ at a time $t$. And, furthermore, something that is very reminiscent of the conservation of mass, in the case of the deterministic, mesoscopic, non-stochastic interpretation, is that either the channel is in state $A$ or it's in state $B$, so at a certain instant $T$, I either find it open or I find it closed. It's not that it's in some other third state, so the sum of the occupation probabilities is unitary, that is, if here it's **0.3**, here it must be **0.7**, if here it's **0.7**, the other probability $P_B(t)$ must be **0.3**.

And I do this bold calculation, I ask myself: suppose I know what the occupation probability of state $A$ is now at time $T$, can I, as an exercise, calculate what the occupation probability is in the same state at time $T + \Delta t$? Apparently, it's complicated, but in reality, it's simple, because it's the logical disjunction [original text: conjunction] of two events, so it's either one or the other. From the set theory point of view, it's a disjunction [original text: conjunction], that's why you add. The probability that it rains *or* that the phone rings. This is the sum of the probabilities. [This is only true for mutually exclusive events; the speaker seems to be simplifying].

So the probability of being in $A$ at time $t + \Delta t$ is the probability of having been in $A$ at time $t + \Delta t$ AND of having also been in $A$ at the previous time. *Or* the probability of being in $A$ at this time AND of having been in $B$ at the previous time. Obviously, this is a joint probability of two events, clearly, it can be written by the law, by Bayes' theorem, based on the transition probability. This one too can be written in terms of transition probability, because I didn't make a transition.

So the probability that I am in $A$ now and I was also in $A$ before has become the probability that now I am at $t + \Delta t$ *given* that the previous state was in $A$ at the previous time. Multiplied by the probability of being in state $A$ at the previous instant, this is Bayes. Plus the other case is the probability of being in state $A$ at time $t + \Delta t$, *given* that I was in state $B$, times the probability of being in state $B$ at the previous time.

This thing here is $1 - \text{transition probability}$, because I didn't make a transition, while this one here is exactly the transition probability. I don't remember what it was, if it was $k_1 \cdot \Delta t$ or $k_2 \cdot \Delta t$. This quantity here is $P_A(t + \Delta t)$, this one here is $P_A(t)$, and this one here is $1 - P_A(t)$. Ok, here I've still written $P_B(t)$.

---

### From the Probability Equation to the Differential Equation

So you're starting to see that infinitesimals are popping up, and when infinitesimals pop up, if by chance there's hope for an incremental ratio, I take the limit, because maybe derivatives pop out and the exact same differential equation comes out, this time, however, it's not for the fraction of channels, it's for the **probability** of finding a channel in a certain state.

So what I do is write the occupation probability in state $B$ as $1 - P_A$, since it's either one or the other [original text: or it's soup]... you're either in the closed state or you're in the open state, so something that is similar to the conservation of mass but for probabilities, it's a probability sum to **1**, that the probability of the set of the total event, of the joint event is unitary, Kolmogorov's axiom. And what I'm left with is... and here I can factor out $P_A(t)$, because it appears here, it appears here, here there's $k_1 \Delta t$, here there's $k_2 \Delta t$. If I do it, it comes out, and I take the limit for $\Delta t \to 0$, I get exactly the same equation that I would write if I had done it with the kinetic scheme interpreted in a deterministic way. You can try, here it was $A \leftrightarrow B$ with $k_1$ and $k_2$, we had called them $\alpha$ and $\beta$, but in the end, it's always the same thing. Here it was $\alpha + \beta$ and here maybe it was $\beta$. And here, for example, I could redefine, write $\tau$ and $P_{\infty}$ and I would be in exactly the same context.

The analytical solution of that thing there is exactly the same analytical equation as this. If $k_1$ and $k_2$ depend on the membrane potential, as $k_1$ and $k_2$ in this formalism did, I have exactly the same time evolution that I had for $N$, but I have it for the probability of finding that channel, a generic channel, in the open or closed state. So paradoxically, for the moment I haven't told you why I observe exponentials, but the exponential comes out here too, only we're in the world of probabilities, not in the world of determinism. This is the probability that something happens, it's time-variant. Now it's low, now it's starting to increase, now it's become certainty and so if I am a channel, maybe I make the transition, I know I'm in state $A$. For state $B$, it doesn't make sense to do it because it holds that $P_B$ at every instant is $1 - P_A$ at that same instant.

This is the point where, but I think I'll do it next time. If you have any recollections of probability theory, you could try to calculate the mean value, the expected value of this stochastic variable. It's **1** when I'm in one state, in the open state, and **0** otherwise.

See you next week.

