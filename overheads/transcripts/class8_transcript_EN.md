# Receptor Kinetics and Synaptic Transmission

With today's lecture, I would like to finish the part on synaptic transmission. And so, the topics I would like to treat are, once again, **kinetic schemes**, this time applied to the quantitative description of synaptic receptors, post-synaptic receptors, which are therefore operated, **gated** by ligands, not necessarily by potential. I would like to show you how a simplification of the model allows one to acquire an intuition regarding the physiology of synaptic signals and tell you, at a somewhat quantitative level, about both so-called **short-term synaptic plasticity** and **long-term plasticity**, and we will see the two differences.

So, in the conclusion of the last lecture, the element, therefore the **take-home message**, the element to take home, was that the same formalism, the same conceptual scheme applies also when describing how two neurons are connected together and one adds in parallel, due to the conservation of charge, of the balance... Therefore the conservation of charge, of mass, the charge balance equation: another branch where basically this type of branch represents currents that are mediated by receptors, which are activated by the presence of neurotransmitter in the **cleft**.

Here, with this formalism, I am actually reasoning by having a neuron as a **point-like** element, and this is not the case. I believe that by the end of the day we will have moved on to another description where spatial extent, the morphology of the neuron, counts, but for the moment everything is in a point. And so, if in this point there are also receptors that are turned on when there are neurotransmitter molecules, well in this case this branch would have a change in the **conductance** of that path, of that **branch**, of that component.

And as I insisted quite a bit last time, the type of value of the Nernstian potential or equilibrium potential is what depends, clearly, on the ionic species to which that type of receptor or channel (so remember, **ionotropic** or **metabotropic**) is selective, and it is what decides if the synaptic coupling is excitatory or inhibitory. And we saw that in the case of an electrical synapse, which we will no longer consider, things were slightly different, in the sense that it was not a branch only in the receiving post-synaptic neuron, but there was also a resistor joining the two independent circuits which were two distinct cells.

## Description of Synaptic Current

So, concentrating only on the description of chemical synaptic transmission, I would like to solve, with the same formalism, the problem of how I describe the current. I describe the current as the change over time of a conductance, for example for simplicity dictated by a difference between two conformational states of a protein, of this membrane receptor, which for simplicity can be either **Open** or **Closed**.

And by doing so, in fact, I am making explicit what the value of the conductance should be; I call it **synaptic conductance**, which is what changes over time. And it changes over time when, if and if, another presynaptic neuron decides to "spit out" (*sputacchiare*) neurotransmitter molecules, because in the synaptic bouton, which is juxtaposed to the membrane of the post-synaptic neuron, vesicles fuse and the neurotransmitter molecules contained within these vesicles diffuse. But all this mechanism does not interest me for the moment; we will examine it slightly later when we talk about so-called **homosynaptic** plasticity.

Here I just want to understand what the dynamics of these receptors is, and therefore this conductance will be again linked, proportional, to the number of post-synaptic receptors in an open state. So it is always the same little game: there will be a maximum value $\bar{G}$ and a state variable representing a fraction. I have resumed describing things here in a deterministic way, in a mesoscopic way, as a population; here there are very many post-synaptic receptors and this is a description without noise if you wish, only in the deterministic case (you know how to eventually translate it into the stochastic case).

> *Student intervention:* "Could you put the code back for a second? Which code? Yes, naturally. Oh, I apologize. The... I believe they are... The first one might be an O and the second a zero. O, zero. What is the probability that out of 25 letters and 10 digits the first two were exactly those? You can... It is the same probability as any other combination of digits, but obviously... Thank you."

## Transition Rates and Dependence on Neurotransmitter ($T$)

So, in the simplest case, channels can be **Closed** or **Open**. I am taking the values of the transition **rates** in this case already with the idea that I know how they must describe things. In particular, they will not be voltage-dependent; they could be, some are, in some cases there is a dependence on potential, but it is made explicit in a different way and I will tell you about it (it is the reason why in this period of the year, with your study fatigue, they foist things rich in magnesium, magnesium ions, on you, and we will comment on why briefly), but for me the transition **rates** are effectively constant, unless, in the case of the transition from closed to open, there is in the **synaptic cleft**, in the inter-synaptic space, there are neurotransmitter molecules.

So $\beta$ is a constant, it is a number, it is the inverse of a time, in fact it is a **rate**, it is a speed, so many transitions per second. And $\alpha \cdot T$ is a product. While before, in the description for example of voltage-dependent conductances, we wrote (then I will pull up the screen, it doesn't matter, I'll write it here), we wrote $\alpha(V)$, so we said it is some very unsightly function in the case of the Hodgkin-Huxley model of the potential, here I make the dependence explicit. I say that it is a linear dependence, proportional; so it is a product between a number that dimensionally is not only the inverse of a time, but is also the inverse of a time times a concentration (because multiplied by the concentration the unit of measurement terms, concentration-concentration, cancel out and it returns to being a **rate**, a probability of transition per unit of time, a transition speed) and with this symbol $T$, with these brackets, alluding to the extracellular concentration of neurotransmitter molecules ($T$-transmitter).

So it is slightly different from what we saw before, but here too $\alpha$ and $\beta$, in this case only (sorry, only this transition probability which is $\alpha$ times something), this thing here depends on time, because the concentration of the neurotransmitter in the **cleft** depends on time. In most cases, if the presynaptic neuron does not speak, does not emit action potentials, these will not give, will not **trigger**, will not initiate any synaptic release and therefore the concentration of neurotransmitter in the inter-synaptic space will be zero.

When instead an action potential arrives and the vesicles fuse and the diffusion of neurotransmitter molecules begins, then there will be a change over time which, however, is not necessarily... it's not that it then remains constant, because the synaptic space is small, it is on the order of a fraction of a nanometer (or pardon, tens of nanometers), but it is open. Furthermore, therefore, the diffusion of neurotransmitter molecules continues and the neurotransmitter diffuses away from the receptors. And furthermore, there are glial cells that are there ready to recycle, suck up, manage the recycling of neurotransmitter molecules.


## Neurotransmitter Dynamics and Flow Modeling

And now we will see one of the ways to describe it, because otherwise this neurotransmitter profile in the inter-synaptic space might have a... yes, let's imagine it can rise rapidly and then decrease. So the increase is probably due to the diffusion kinetics of the vesicles and the diffusion coefficient of these neurotransmitter molecules in the medium, the extracellular *medium*, but at a certain point it turns off: the concentration returns to zero with some dynamic that I cannot indicate a priori. It depends, as said, on free diffusion and *re-uptake* by, for example, glial cells.

But the strategy is this: I say that the conductance is active only in the **Open** state, by convention (by my convention), and again with the same little game I would write that $\frac{dO}{dt}$ (you see, there is a small arrow exiting this state $O$) exits with a speed $\beta$. And this flux depends on how much $O$ there is, so it is minus because it exits: $-\beta \cdot O$. It is a proportionality, I remind you, by the law of mass action, but it is reasonable because if I don't have $O$ (if here I don't have a term that is proportional to $O$) it wouldn't make sense for it to be a negative term, because integrating each member I would have a quantity $O$ that over time tends to decrease, even going negative. While this way I have the famous exponential arcs: you see the minus sign, I am calm, I have no problems and therefore there is a saturation effect, of shutting down to zero that maintains physical consistency.

Now, physical consistency, in the course of probably this hour and the next, at a certain point I start to loosen it because I need to be able to simplify this description to extract something that I don't see instantaneously from here, I don't see immediately.

The other term, you see, in this container (again I imagine them as if they were containers, tanks of liquid) there is this tap, this flow, which is all the more intense the more $C$ there is, the more channels/receptors are in the **Closed** state, proportionally to $\alpha \cdot T$. You see that $T$ acts as a kind of tap: when $T$ is 0 the tap is closed, here this arc from $C$ to $O$ is no longer there; it is there only when $T$ is non-zero. And when $T$ becomes/changes (it can have some gradation, suppose from 0, 0.5 millimolar, 1 millimolar, which is the peak neurotransmitter concentration in the inter-synaptic space), then this transition speed, of increase of $O$, obviously tends to increase. I call it speed of increase because here I am describing the time derivative, which is a speed of appearance, of apparition.

### Rewriting the Equation: Canonical Form and Time Constants

Actually I can write it for $C$ as well, but these two equations are linearly dependent, so in the end it suffices for me to remember the conservation of mass ($C + O = 1$), so I write $C$ as $1 - O$. Same thing we did in the generic case of voltage-dependent conductances and we can arrive at this form:
$$\frac{dO}{dt} = \alpha T(t)(1-O) - \beta O$$
which is related exactly to the same rewritten form where I had a state variable quantity, "infinite" version (as was $m_\infty, n_\infty, h_\infty$) and I also have the $\tau$. In this case I called it $O_\infty$ and $\tau_O$.
$$\frac{dO}{dt} = \frac{O_\infty - O}{\tau_O}$$
This is the time constant with which this equation changes and $O_\infty$ is the steady-state value. I told you as a practitioner, as an engineer: I can consider this system a box where the output is the state variable, the input is this $O_\infty$ and in the end $O$ follows $O_\infty$. How does it follow it? With this kinetics, with a little bit of delay.

The interesting thing is that this time constant, moreover as it was also in the case of voltage-dependent *gating*, is concentration-dependent. When there is no neurotransmitter concentration, the denominator is smaller... No, I'm wrong. So, pardon. If $T$ is 0, this time constant is $1/\beta$. When $T$ is non-zero (yes, that's right), when $T$ is non-zero, the denominator tends to increase ($\tau_O = \frac{1}{\alpha T + \beta}$). So 1 divided by something that is larger tends to be smaller.

It means that in the rising phase, in the so-called **onset** phase, of opening, the currents (which are ultimately proportional to $O$, the synaptic currents) go up much more rapidly. It is always an exponential arc, I am calm, you see here there is the minus sign, but this $\tau$ is much smaller when $T$ is non-zero. $T$, note, can only be positive, so when $T$ is non-zero, here the denominator becomes larger, so this $\tau_O$ becomes small, so it grows very rapidly. And then when it decreases (suppose it decreases because $T$ turned off, because $T$ is zero), then in fact only $\beta$ remains and it is a quantity, seeing as there is no longer this additive term in the denominator, it is a slower quantity: it goes up rapidly and goes down more slowly.

Which, if I recall correctly, is similar to the type of **synaptic potential** (here it is the potential we are recording and actually it is not the current, but in the end synaptic potential and synaptic current are the same thing except for an integration, integration by the charge balance equation, the work of the equation satisfied by the membrane potential). You see that here the rise is much steeper, much faster than the descent; the descent, or also called turn-off kinetics, is much slower. Here with these considerations we have this consequence effectively **for free**. And again it is implicit in having made one of the two kinetics, the opening kinetics, concentration-dependent.

## "Square Wave" Approximation of Neurotransmitter

You could have said: "Okay thanks, it's intuitive. In the following I have a bit of a problem solving a differential equation of this type". Which yes is indeed true and is first order, but it is not with constant coefficients, because by definition $\tau_O$, which is a coefficient, and $O_\infty$, which is the known term, the forcing term (from which the particular integral actually descends), are functions of time. And as said I don't know what the profile of neurotransmitter in the inter-synaptic space actually is.

I can measure it; in some cases there are synapses that are so massive that somehow I can measure the neurotransmitter concentration as it varies over time. I could even use the receptors themselves as **reporters**, as antennas: since they open and close depending on whether there is neurotransmitter or not, I could see from them what the profile and duration is, doing a sort of deconvolution operation, of **reverse engineering**.

However, for simplicity, to fix things, I say that this $T$, function of time, is a particular function of time, simple for me: and it is a **piecewise constant** function. Piecewise constant means that suppose the activation of the presynaptic neuron (so the moment the presynaptic bouton "spits out" the neurotransmitter) starts at one millisecond, goes up very rapidly in zero time, reaches a maximum value, $T_{max}$, which can be one millimolar, and then goes back down after a millisecond. So one millimolar for one millisecond. This is the way I have to, again, try to understand something and extract intuitions, simplifying my life.

In fact, as said, this would be something that changes gently, it goes up maybe rapidly (but certainly not in zero time) and goes down in a slightly slower time, probably because it is linked to lateral diffusion and to a *re-uptake*, a recovery with endocytosis by the neuron itself and by other cells like glial cells.

If I do this, however, this differential equation also becomes one with piecewise constant parameters. So I know I can solve it from here to here, where in fact $\tau_O$ is only $1/\beta$ and $O_\infty$ is 0 (you see, at the numerator: if $T$ is 0, $O_\infty$ is 0), and if $T$ is a constant value $T_{max}$, again, this is a number and this other is a number. So I can write the solution as two distinct terms and in the end I always find the same exponentials, it is not a big surprise.

### Comparison with AMPA Receptor

If I compare, therefore, the solution of this equation with piecewise constant parameters (and this black trace you see) is compared in blue with the current, obviously normalized, which is in fact a real **AMPA receptor**. A receptor that, as I told you last time, is a glutamatergic receptor: it binds to glutamate, obviously it also binds to the substance called AMPA with which it was selected. Glutamate binds to many receptors: it binds to AMPA receptors, NMDA receptors, it also binds to metabotropic glutamate receptors.

For AMPA (and I recognize it from the fact that the turn-off kinetics is fast, it is on the order of 1-2 milliseconds, perhaps I called it the **Ferrari** of synaptic receptors), it is an ionotropic synaptic receptor: rise and fall are immediately consequences of an ionic flow through the pore that is created in that receptor. And you see, the agreement is not perfect: in the rising part yes, in the falling part no. But it might not be so crucial to capture exactly the waveform of this potential, of this temporal change of synaptic conductance.

And anyway, to make it short, do you want something more accurate? Now eventually I will provide it to you, but the "open and closed" type of description is probably not the best description. It may be that, as also in the case of voltage-dependent channels, there is something more (remember it was $n^3 \cdot h$, it was $n^4$, assuming therefore accounting for the existence of different subunits; now in that case the subunits were independent). In short, it can be arbitrarily complicated if one has the mission to understand and replicate the experimental behavior exactly.


## Advanced Models: AMPA Receptor Desensitization

If one has to do something like this, one might prefer a much more accurate description. This is a description of the **AMPA** receptor which, as you can see, is not "closed-open": there are three states indicated by the letter $C$ and they are **Closed**, in the sense that the channel is not permeable to ions. By the way, the AMPA channel is permeable to a mix of sodium and potassium ions and the reversal potential is $0\text{ mV}$, I believe I said that last time. In states $C_0$, $C_1$, and $C_2$ the channel is not open, the conductance is 0. The only state where the conductance is non-zero is this state $O$.

And there is an additional state called $D$, where again the conductance is zero, and it has this letter, this label $D$, because experimentally it was understood that there is a need for all these transitions. Some of which are dependent on the presence of neurotransmitter in the inter-synaptic space, but not all; for example, these where from the Closed 2 state one escapes somehow into this sort of trap, are not dependent on the neurotransmitter concentration, they are fixed. And this state is called the **desensitized** state.

Indeed, when you take this kinetic scheme and activate it (so simulating, imagining that there is a train of these little pulses, of these little rectangles of neurotransmitter concentration in the inter-synaptic space, which are regular: here I believe they are one every $10\text{ milliseconds}$, so 100 times per second, 100 activations, **$100\text{ Hz}$**), it means the presynaptic neuron is firing at $100\text{ Hz}$, which is a lot, it is a very high activity frequency. Perhaps a cortical neuron could fire at $100\text{ Hz}$ for a very short time, do a **burst**, a packet of spikes. Forget about seeing $100\text{ Hz}$ for a neuron, for example a pyramidal one, holding it for a few seconds. It doesn't happen: the neuron gets tired, it turns off, there are frequency-dependent adaptation phenomena, as I mentioned to you, etc.

### Comparison Between Simple Model and Model with Desensitization

If you use this [simple] scheme, the amplitude of these maximum peaks of the synaptic conductance remains practically the same. Again, the important thing is that you don't go and activate this faster receptor with a frequency on the order of or faster than $\beta$. $\beta$ is a frequency (a rate), $1/\beta$ is the kinetics, it is the turn-off time constant. So what I am saying off the cuff, by eye, is that this $\beta$ somehow tells me what the time scale is in which the tail is exhausted. Today we will talk about tails (I don't know if I talked to you about my bank account at the university that I pay myself every month... Yes, we will resume it. Exactly, it was about adaptation, **spike frequency adaptation**. Here I am basically thinking of the same thing). So the tail is finished here and if there is a further pulse of neurotransmitter in the *cleft*, so another spike, ok, the system is ready again.

In this case instead [with state D], if you (and it is not particularly complex, the only difference is that here it is a single differential equation, here it is 1, 2, 3, 4, 5 states; they are $5 - 1$, they are 4 simultaneous differential equations that you have to solve numerically. You can potentially write it, you know that it is possible to describe a system of more than one differential equation as a single differential equation not of order 1 but of order N, in this case it would be of order 4, but it doesn't make much sense), in the end you would want to have a value to put inside here and to simulate this synaptic current. In this simulation $V_m$ remained fixed because here the membrane potential is not changing, it is as if it were a **voltage clamp** experiment.

And here you see that, since I am going so fast (so I repeat, here it is on the order of a pulse every $10\text{ milliseconds}$; I notice it because here out of $20\text{ milliseconds}$ there are two, so roughly it must be that), then the inactivation makes itself felt and therefore the amplitude of these conductance peaks (this is the post-synaptic conductance, I emphasize it because we will see something similar in another context) shows a kind of "tiredness".

The interesting thing is that it is possible pharmacologically, for the specific case of the AMPA receptor: there is a substance called **cyclothiazide** which, if you apply it, you remove the occupation probability of AMPA receptors of the desensitized state. Effectively the AMPA receptor, even at $100\text{ Hz}$, continues to function without getting tired.

## Physiological Meaning: Runaway Excitation

Probably nature thought (beyond cyclothiazide, which I believe is an artificial compound, I don't know if it exists in nature, or perhaps it is an agonist in this case, instead of an antagonist, somehow it tends to potentiate the effect of these currents), but in general the fact that glutamatergic synaptic receptors, therefore excitatory, have perhaps evolved in some cases a kind of fatigue, could be as a possible interpretation (this interpretation might be worthless) be due to the fact that if I am a glutamatergic synapse it is a big responsibility. Because if I activate myself and propagate the excitation signal, it may be that this excitation signal can become pathological, can be explosive, can be what is called in jargon **runaway excitation**, excitation that "runs away" uncontrolled, or a **seizure**, an epileptic crisis: a global synchronization of all the cortical tissue.

Because the cortical tissue and many other areas of our brain and our nervous system have connections that are recurrent. I'll give the stupid example I often give, since you are particularly close and maybe you are a bit edgy today (I don't know why, again because of the fire alarm): if I were to, a stupid example, were to slap one of you and a **burst** of excitation... you turn to your colleague next to you and you start beating each other up particularly. If you were much further away there wouldn't be a brawl. It's a stupid example to say that if by chance there was a kind of muscular fatigue in your arms, in your synapses (actually it should be in this case in the receptors, so it would be the receiving part), but if there was a kind of fatigue in this sense, it could be that the collective effect of this reverberation, of this recurrence of excitation, could be sedated. And this seems that it could be one of the reasons for these mechanisms of **spike frequency adaptation** and desensitization.

The thing is different on inhibitory synapses and we will talk about it, because it is very interesting, because it is the exactly dual effect. If I am inhibitory, maybe there is a sense why I am receiving a lot of spikes, because the spikes evidently were generated by excitation. And this excitation perhaps puts me in the condition of having to inhibit the others and maybe my inhibition must not lower the voice, but raise it: my inhibitory effect must become more and more prominent if there is someone who keeps saying "fire, fire, fire". But it will be clear later.

### Intuitive Interpretation of the Kinetic Scheme

Here it is simple to understand because, again, I imagine it as if there were here a kind of plumbing between four tanks of liquid and here in fact there is a kind of leak. Okay, actually the leak... the arrow can also go up, but it depends on the values of this $R_D$, of this rate that returns up from the desensitized state. Because only when I am in $C_2$ can I open, otherwise no. So if I have something that makes me fall... I imagine it again with a lot of channels, of receptors that all work in parallel and many are at a certain point moving from left to right because $T$ has become non-zero. So these arrows are possible, but here one falls into the desensitized state, subtracting oneself from those that can instead reach the open state.

You have to imagine it like someone flipping a coin. Ok, the probability there is 0.5, but it's not that heads comes up every time. So suppose the probability is 0.1. One out of 10... what does it mean? Time? Ok, they are probabilities per unit of time, so it makes no sense to say 0.1; I should say $0.1 \text{ per millisecond}$. Every millisecond there is a probability of only $10\%$ of making the transition and so maybe the majority of these receptors (which I imagine all in parallel, independent, etc.) don't see the desensitized state, but someone does. So collectively the fraction of channels that are available to go into the open state reduces. It is even simpler than the case where the kinetic rates are controlled by an external variable, like the potential or like the concentration of a ligand. Simply the transition happens.

How does it happen spontaneously? The fact that here it closes spontaneously if there is no more... Actually it closes even if there is neurotransmitter, but in fact one sees perfectly: when there is no more neurotransmitter there is this **recovery**, this kinetic closing of the channel.


## The Role of Magnesium and NMDA Receptors

Why do you take magnesium? Or why is magnesium given to hyperactive children? The time I said: "Is it possible that it's like this?", that is, psychiatry in general (so I take responsibility for this), psychiatry gives molecules randomly like this, saying: "Yes, but because I know... I made up the little story (now I'll tell you another one), the little story of AMPA receptors, and so if I put a little... so I lower the glutamatergic tone and people are less stressed." In the end, for the moment it is like this: besides obviously medicine and psychiatry having a huge base of observations, trial and error (not even the "trial and testing" of Galilean memory). I have a thousand patients in my office, I saw that those to whom I gave benzodiazepines of that type or the antidepressant, ok, are a bit better, and so I say: "Ok, you seem similar to me". But there is no knowledge as an engineer would want it, to say: "That parameter should have been $20\%$, now it is $26\%$, I have to reduce it". There is not this in biology yet.

With bioengineering, particularly with **neurotechnologies**, this is getting closer. One says: "Ok, I don't know what the right temperature is, but maybe I put some neuroprosthesis that with a closed-loop control dynamically adapts an activity, brings me back, removes the Parkinsonian tremor because it tends to decrease the affiliation of a particular population". I don't know which one would be physiological, but I can measure the tremor; the more tremor there is... so this is a direction more based on principles than based on experience.

So, does anyone know why you take magnesium if you are hyperactive or to calm down? You might think, and you wouldn't be far from the truth, that it is because there is something to do with the **glutamatergic system**. Ok, but damn, it is only one way, one mode of communication between cells (the most important mode of the nervous system perhaps, agreed), but it's not that my whole nervous system, if you lower $\bar{G}$ a little bit, then I become a different person. It's a mess, we don't know.

### The Voltage-Dependent Magnesium Block

Anyway, the particular fact of magnesium, do you know it? It is due to the fact that there are glutamatergic receptors, still **ionotropic**, called **NMDA**. They are called NMDA receptors because there is a substance called NMDA (of which, as for AMPA, I don't remember what the acronym stands for) which acts as an agonist. They are glutamatergic receptors though, so when there is glutamate in the inter-synaptic space, these open. But they have, in the part... in their domains (it would be wrong to say in the intracellular and extracellular part because it is everywhere), they have a **sensor of the membrane potential**.



So I repeat: they are receptors that are placed in the post-synaptic neuron, they are intercalated and therefore on their own (like the membrane channels sodium, potassium, calcium, etc., are voltage-dependent) this one too somehow has a dependence on the potential. The potential is of the post-synaptic neuron in which they are placed, but they open when there is glutamate.

An easy way to describe them, without complicated kinetic schemes, is to think that there is certainly a state variable between 0 and 1 (the fraction of open channels) and this for example evolves with a simple two-state open and closed kinetic scheme. But the maximum value ($\bar{G}$) is not a number: it is a number times a quantity, and this quantity is an instantaneous function of the membrane potential and the presence of **extracellular magnesium**.

That is to say, if you have 0 magnesium (these are the small circles in the graph) or $0.01\text{ millimolar}$ of extracellular magnesium, you have that this $\bar{G}$ is not constant, as you would expect (given by the conductance of the single channel times the total number of channels). Well, in this case, in a phenomenological way, in an effective way, you see that it depends (I am looking only at this curve here for the moment) it also depends on the **post-synaptic transmembrane potential**.

If this is very depolarized, then yes, basically it is constant. However, as it starts to be hyperpolarized, this value of the maximum conductance of post-synaptic NMDA receptors tends to decrease. And it decreases badly, it decreases down to $25\%$ or to zero. So it means that if there is no magnesium there is a very notable dependence on the membrane potential... Ok, no, said like this it is wrong. The receptor also depends on the membrane potential and the more magnesium there is (passing from circles to diamonds, to squares, to triangles, so the extracellular concentration is passing from $0, 0.1, 1, 10\text{ millimolar}$), this dependence tends to be there anyway and tends to shift towards more depolarized values.

And the **range** of this sensitivity is exactly in the range where synaptic integration and the action potential occur. It's not that it has a range for which physiologically it doesn't happen; it is right there. So evolution made it there.

### Coincidence Detection

I'll say it again in another way, so maybe you can anticipate: but what the heck is this for? The conductance... suppose there is a necessary physiological quantity of magnesium. So if you take magnesium orally, you are moving on these curves, so you are at resting potentials for example, you are **turning off** glutamatergic synaptic transmission, regardless of whether there is neurotransmitter in the inter-synaptic space or not. The interesting thing is that this changes: so this curve is no longer 0, it becomes 0.75, 1, etc., up to a maximum, when the potential is particularly depolarized.

So, for this receptor to conduct a current:
1. There must be neurotransmitter molecules in the inter-synaptic space, so that the "closed-open" game flips and that variable $R$ (or $O$) becomes non-zero, because otherwise this current is null.
2. And then here there is a product, so logically it is a kind of **conjunction** (AND): two things must hold for one to see an NMDA current. There must be the neurotransmitter (so the channel flips from closed to open) **AND** the post-synaptic membrane potential must be depolarized (to remove the magnesium block).



So I am the presynaptic neuron and I "spit", I speak, I am excited and I speak, and speaking I spit glutamate. And the post-synaptic neuron hears it, it binds to the receptor, but on its own, to have this current, it must be depolarized, it must fire (or almost). It occurs to me: what is this thing here? It is an operation, it is a remarkable computation. It is a **temporal coincidence detection** of two events. The presynaptic neuron is firing **AND** the post-synaptic neuron, on its own business, is depolarized.

So this correlation is probably not a coincidence, because the NMDA receptor is extremely permeable to **calcium** ions, it also has a reversal potential around $0\text{ mV}$ (so anyway it is certainly excitatory because it is greater than the resting potential, $-60$), and it is involved in a whole series of phenomena linked to **synaptic plasticity**. This detects coincidences between pre and post activity.

And obviously, if by chance every time I, the post-synaptic neuron, am active when the presynaptic neuron wants to talk to me, perhaps there is some reason why the two of us become connected, why we move from a **correlation** event to a **causation** event. Maybe it is necessary, as for two concepts in psychology that are similar and co-activate, it is necessary that maybe if they always co-activate, that a kind of reinforcement exists between the two and that it becomes that when the presynaptic neuron fires, then the post-synaptic neuron also fires.

But this thing here in theory precedes, because again the membrane potential can or must be depolarized and could be depolarized for other reasons, for the activation of other synapses. It is not necessarily that I speak to you and every time I must excite you; you can be excited on your own and if temporally there are these co-activations, that is this synchrony, perhaps it is good that our synapse too, our communication channel, becomes privileged.

However, it has always made a big impression on me that one gives magnesium to a hyperactive child because this way this sigmoid **drifts** (shifts) and therefore tends to be even less excitable. If you don't give magnesium, so if there is little magnesium, either this curve is practically horizontal or it is anyway very shifted to the left and therefore even at resting potentials, if there is a little bit of glutamate, this has a depolarizing effect on the post-synaptic neuron. Does it make sense? This starts to require a bit of attention because, again here I am talking about the presynaptic neuron exciting the post, and the post on its own business being depolarized. I know from experience that people get a bit confused: "But how? Glutamate gives a depolarizing effect...". NMDA, given this reversal potential, is a depolarizing current. Yes, this is an extra thing, it is an extra coincidence detector.

## Metabotropic Receptors and Complex Kinetic Schemes

Ok, now in the case of **metabotropic receptors**, the discourse, even if we won't deepen it that much (you can try to write the equation), nothing is... It allows, with the same language (it is very powerful, because it is phenomenological: in the end I don't have to specify that this is the G-protein that got activated, it is simply $G_0$). A quantity that describes a density, a concentration of objects in a particular state that can be free, or here it could be bound to the receptor.

And I can translate into a kinetic scheme what is for example the metabotropic behavior where the neurotransmitter, when it binds with the receptor in a *naive* state that is not bound, not only gives (so it doesn't open a channel, the conductance here... the $O$ pops up here, so yes, in the current expression I will put the fraction of channel in that state)... before getting there I have to struggle a bit.



[Image of G-protein coupled receptor signaling cascade]


Here $R$ represents the state of the receptor (for example I am thinking of the **GABA-B** receptor or the metabotropic glutamate receptor, **mGluR**) when it is bound, in a state that is bound to the neurotransmitter molecule. And on its own business, you see here, it can even go into a desensitized state and flip and can also go back, can free itself (since there are no... since all these transition probabilities are constant, so they are not further dependent on voltage). And I need receptors in this state **AND** there must be intracellular **G-proteins** in the free state to be able to go into a conformational state of "receptor and activated G-protein" combination, which after a while detaches.

And this G-protein in an activated state decays on its own business and returns to a ready state, but binds to a channel that was (a channel for example potassium) that was in a closed state. Maybe $N$ G-proteins are needed (see that three-dimensional animation I showed you last time by this guy on X or Facebook or Twitter or whatever): only in this case does one pass to an open permeability of a potassium channel.

And so all this mess translated into a system of differential equations represents for me and allows me to describe and understand metabotropic activation. The only thing that happens is, imagining that each of these steps has one or more differential equations, you can imagine that every time there is a differential equation you have some kind of time constant, of temporal kinetics, therefore a kind of **delay**. Which rightfully justifies the fact that ionotropic receptors are Ferraris (the neurotransmitter binds, the pore opens and ions pass immediately). Metabotropic ones are the most ancient from an evolutionary, phylogenetic point of view (and they are probably... phylogenetic means on species, on different species of mammals, insects, invertebrates, vertebrates, whatever you want, they are the oldest) and they are **slow**.

So if the tiger enters, perhaps it is convenient for me to evolve synapses in my visual system that can pass information quickly, rather than stuff that here maybe activates after $50\text{ milliseconds}$. The tiger might get the better of me.

The interesting thing here is that they could, for these schemes (which you could also read as biochemical kinetic schemes that some of you might have done), it lends itself, and in biological reality it is so, to a whole series of **positive feedbacks**, amplifications. So I have maybe only one molecule of neurotransmitter. This is particularly interesting for the olfactory bulb, for the olfactory system where maybe even just one molecule of jasmine odorant (of the flower, oh well, of the rose) I take only one and I have to amplify it. And instead of doing an amplification job that exists in the central nervous system at the level of spikes, at the level of electrical activity, I can do it at the level of biochemical reactions. Just as I can make a kind of *safety* mechanism, of safety, of a brake where if I have too much activation I can block it. And so this with AMPA... the AMPA receptor doesn't have it (ok, it has this desensitized state, but it is simple from the point of view... it doesn't allow... it allows only maybe to disfavor, not to amplify weak signals).

After the 10-minute break I will show you how, to try to simplify things, what advantages there are if, instead of thinking of a train of these little pulses, of these little rectangles (each placed at a time when there is a presynaptic action potential), I swap them, I replace them with **Dirac deltas**. And you will tell me: "But this doesn't exist in nature, concentration doesn't reach an infinite value in zero time". Yes, but mathematically the effect is similar, it is very similar. And we will see it later.

I'll stop for 10 minutes, you know where to find me if you have questions.


## Approximation with Dirac Deltas and Impulse Response

Good. So, here I have resumed this kinetic scheme of the post-synaptic receptors of chemical transmission where there are only two states, **Closed** and **Open**. Again, this is the equation I write when I describe the change over time of the fraction of channel in the open state. And the thing I can observe is that if I, instead of putting $T$, an arbitrary function (which somehow accounts for this graphical summation, graphical superposition of functions that are practically zero everywhere, except where the little rectangle is), I can write it instead as a sum of individual functions, where I have to change the argument anyway depending on the index of the summed element. That is, I can imagine that that train of pulses I showed you before, of rectangles, is describable as a summation and then here there is a translation, a *shift*.

Those of you who had the courage to watch those few introductory mathematics videos will have seen that in a very intuitive way I show you (perhaps it was even with some Jupyter Notebook or Google Colab or whatever it was), I show you that it was possible to understand that if you put, change the argument of the function, subtracting or adding, the function moves rigidly on the X axis; if instead you put plus or minus something here, the function translates from top to bottom, so towards or along the direction of the Y axis.

Here however I change, I am no longer interested in using the little rectangles, I want the **Dirac deltas**, because I like them. Or rather because I know how to behave when they are put on the right hand side of a differential equation. I know this because engineers, when they calculate the impulse response, are in fact having an impulse on the right part of a differential equation of a system of this type, and they have one. Ok, here instead of being one they are an arbitrary number, they are $N$, and they are each placed at a different instant $t_k$ ($t_1, t_2, t_3$...).

So the situation is this, in which I have different instants. Not necessarily for the moment... not necessarily these times representing the activation times of the presynaptic neuron, not necessarily must they be uniform, that is integer multiples of some quantity; they can also be random. As long as I call them $t_1, t_2, t_3$, have written somewhere where they are, I can write concisely. So writing sum of these terms, $\delta(t - t_k)$, in fact I am expressing in one go the graph of this function that has 4 (in this case there are 4), 4 Dirac deltas. This is a very convenient thing and it holds because where the Dirac delta is not present (like where the step was not present) the function is zero, so I can literally sum it. And doing the summation, unless I am... so I have to *shift* things, but if not... here it is a bit more difficult, but if I am careful to put them at instants that are more distant than the interval of that millisecond of this duration of the neurotransmitter, of the neurotransmitter profile in the inter-synaptic space, the amplitudes do not overlap for me. The sum always happens between a quantity that is 0 everywhere and a quantity that is 1 (or $T_{max}$, where was it? $T_{max}$) only in one point. I'm making it very long for those of you who perhaps haven't seen Dirac deltas, but this is very convenient for many reasons.

### Multiplicative Terms and Receptor Saturation

What I heard written here and $C$ I wrote as $1 - O$, which by definition (so in this case the equation is still exact) and $T_{max}$, which was the amplitude of these Dirac deltas, but since the amplitude was the same for all, I brought it outside the summation.

Looking at this equation I don't know how to solve it because here it is not a system... beyond the fact linear or non-linear (in this case it is a linear system, it has constant coefficients), but here the input is a **multiplicative** input, one says, because the state variable $O$, here multiplies the input. While I am used, in systems theory, to having a term that exhausts (which appear as they wish) all the terms that are the state variable, plus something, and this something does not contain the state variable. This is typically the simple version, of which I know how to manipulate, I know how to write the impulse response, I know how to solve analytically with the convolution integral or with various heuristic techniques. When I have something like this I don't know how to do it because, or rather I could do it but it is more complicated, because it is a multiplicative input term.

I make you reflect that this term $(1-O)$ has the function of, for free (it emerges from this kinetic scheme), taking into account what is the **saturation of post-synaptic receptors**. As I told you, the presynaptic bouton "spits out" the neurotransmitter molecules and these bind to a number that is however limited and finite of post-synaptic receptors. When a neurotransmitter molecule approaches a post-synaptic receptor and finds it occupied because there is already another molecule that is bound (so if they are almost all open, open in the sense that they are bound to a neurotransmitter molecule), $1-O$ is very small. And so this term here, whether there is this input or not, whether there is or not, is not taken into consideration, there is no space, there are no other *slots*, since the majority of channels is in a state (of post-synaptic receptors) is in a state bound to neurotransmitter molecules.

However, a situation like this happens in two circumstances.
The first is that there are so many vesicles in the presynaptic bouton that in an instant I flood you and saturate all the receptors (as if they were seats, perhaps this is a more fitting example, as if they were seats in a classroom: a huge quantity of students enters, the seats are those and in one go they sit down). Another possibility (this is usually not the case, also because metabolically it is complicated to regenerate in the presynaptic bouton a notable quantity of neurotransmitter, so most of the time the quantity is limited, it is small of neurotransmitter; on the other hand it is an electrochemical conversion and so it could be, and it is, metabolically expensive to synthesize the neurotransmitter, package it in vesicles, it can be tiring).

The second scenario, which is a bit more realistic, is that the doors open repeatedly with a certain frequency and every time new students enter. So if the *firing* activity of the presynaptic neuron is at a certain frequency, this frequency again has to do with $\beta$, with $\beta$ is a kind of term of comparison. Then obviously I say it because $\beta$ is the *rate* with which the neurotransmitter unbinds, so with which you perhaps have finished sitting and leave. So if I start to have an activation that is on the order of $\beta$, then again I can have that I saturate, there are no more free seats. So this $1-O$ is absolutely convenient for me. But this happens again not so frequently, because again it means that the presynaptic neuron is firing at frequencies comparable to $\beta$, which putting the appropriate... putting the appropriate numerical values means on the order of a few hundred Hertz, a few hundred spikes per second, which is practically very very rare. Yes, it could be for very short... for packets, for *bursts*.

But from here on I am making assumptions, approximations. So I am aware of the fact that not only was this a simplified view, but here I am saying: "You know what? I'm just throwing this away". I say there is only 1. So it's as if I said: the majority of receptors are almost all free, so they are almost all closed, always. And you could tell me: "But no, they are not". Yes, because anyway the fraction of receptors in the open state, when it increases, increases and becomes at a peak value for a short time. So if the presynaptic activity is not very frequent, the neurotransmitter always finds free receptors.

So I am writing that I approximate and I am putting here a term that finally is quite simple to handle. Have you ever seen a differential equation of this type where the forcing term is a Dirac delta? Electronic engineers, those who have the idea of the impulse response should give a positive answer. Maybe you have never seen it with a train of pulses, so with an adder of many shifted pulses, what is called a pulse train.

I will show you shortly, comparing it and doing paper and pen (or marker and blackboard, if the marker writes), I show you that if you allow me this simplification I can extract some information that is basically here. That is, I can have on one hand an expression (but this is not particularly interesting: instead of solving this differential equation, I can have an algebraic, iterative equation, which tells me what the amplitude is at the instant of the $k+1$-th spike, knowing what the amplitude was at instant $k$ and knowing how much time has passed). But this could have a value of the type: if you want to run simulations, you want to have a method to accelerate times. Because if a pyramidal neuron receives something like 10,000-100,000 synaptic inputs from other neurons, it might be hot to have 100,000 (even maybe there will be 50,000) differential equations of this type to solve numerically instant by instant.

## Dynamic Steady State and Frequency-Current Conversion

But the most interesting thing, of which I just want to show you how one arrives at it, is this expression here which says that more or less at **steady state**, at the regime state... And you can tell me: "But what regime? I have presynaptic spikes and from the post-synaptic point of view the potentials (so the currents or the post-synaptic potential, the change in conductance) is something that goes up and then goes down, goes up and goes down. Where is the regime?". There is no fixed regime, it is similar to what is called permanent sinusoidal regime, in which there too there is no fixed regime, there is no constant regime, but it is a kind of dynamic regime that I show you.

And from this (now I tell you what this *steady state* is) I obtain this information which is very important: apart from $\alpha$, $T_{max}$ and $\beta^{-1}$ which are numbers, here it tells me that at *steady state* a synapse, a post-synaptic receptor, a family of post-synaptic receptors, are producing a current for me (ok, this must be multiplied by $\bar{G}$ and multiplied by the *driving force*), but anyway on its own it is a current **proportional to the presynaptic frequency**.

Which is a very interesting, very powerful thing. Somehow, despite being a strong approximation, it tells me that **a synapse is an object that converts frequencies into currents**. Which is what it does. And this conversion is linear, it is proportional. And a light bulb might light up in your brain and say: "Ok, if I have to develop some intuition on what a synapse does in a large network of neurons, maybe I could say that its value is testifying to me what is a value... so it testifies to me with a continuous analog value over time, of what is the frequency of a digital event and extracts the frequency for me".

So this is what I am saying and we haven't talked about it, we will talk about it next year, combined with what neurons do. In the end we saw it only from the numerical point of view, we saw this when I showed you that for the Hodgkin-Huxley model, for example increasing the current (my injected current, but it could be the synaptic current injected by another neuron), when this current is increased the firing frequency increases. It doesn't matter that it increases linearly.

So in a certain world (for which this slide makes half sense now and the other half makes no sense because we haven't seen it in this approximated way), the so-called frequency-current curve, which perhaps I asked you to try to play with, perhaps I asked your predecessors of the second year, converts a current into a frequency. So it converts an analog level into a pulse train. While the synapse does the opposite effect: it converts a pulse train into an analog value. And so maybe at a certain point one might (but the temptation doesn't come now, it will come in a year) to say: "But ok, what if by chance I connect this arrow to this...". Because in the end synapses are objects that have an effect on other neurons and let's say the frequency activity of this arrow actually I could attach it here, because in the end it is neurons that produce these action potential trains, so it's not that synapses invent these action potential trains for themselves. So I could see and study (and it is done in a self-consistent way) a kind of combination between a synapse and a neuron which could be representative of many neurons.

### The Bank Account Analogy

So, what I would like to do is, before moving on to synaptic plasticity, I would like to put a damn black slide because I don't want to turn off the projector which I don't know how to do, so give me a second. This is not very elegant... ok... ok. It's brief, it's a little game, but once in a lifetime the story of my bank account must be clarified mathematically.

So you have to allow me to make an approximation, a simplification. I am thinking that the presynaptic spike train over time is at **uniform frequency**, that is, this time yes, the Dirac deltas must be equispaced. So if I were "cool" I should write that $t_k$ is proportional to some $\Delta T$, for example. It is an integer multiple of some period. And this period, $\Delta T$, could be described as $1/f$, since frequency and period are one the inverse of the other, in a context where there is a uniform frequency, one says, because it doesn't change over time.

So the equation was something like this (it doesn't matter about $\alpha$, it doesn't matter about $T_{max}$ and $\beta^{-1}$, it doesn't matter), it was something where there was, as a function of time, a differential equation describing $dx/dt$, there was a $-\beta x$ plus, let me call it, $A$, then there was the sum over $k$ of these $\delta(t - t_k)$. It doesn't matter much, it is much of a number, now you will see because it is not particularly important.

So the first thing I can say is that when I am not close in proximity to a Dirac delta, this quantity disappears, because it is zero everywhere, practically it is zero always, almost always, except when there are exactly those Dirac deltas. So in that case, when $t$ is not... is different from $t_k$ ($k$ from 1 to whatever is $n$), the equation becomes very simple, and it is the usual, very boring equation:
$$\frac{dx}{dt} = -\beta x$$
And I know how to solve this because $x$ (apart from the initial condition, which here I leave indicated the constant to be identified) is $e^{-\beta t}$. I am happy because there is a minus sign and so there is the minus sign in the exponential, so it means it is a decreasing exponential.

So somewhere here (so this would be the graph, so this is somehow the presynaptic input appearing in that equation, presynaptic input, and this is for example the value of $O(t)$, or of $x(t)$, whatever it is; in the end $x$ and $O$ are the same thing except for a scale factor which I don't have now, it is not important to recover in its precise value). So at a certain point every time there is a space between two, so the interval where there is no Dirac delta, starting where I start (whatever the initial condition will be, I don't know, fixed at whatever it is), I know how to write the evolution over time because it is an arc of exponential that goes down.

Here perhaps you see the little game I want to play. It's as if it were... so that is the university gives me (because there is a plus) a quantity of money at the end of every month. But fortunately I don't behave like this: I don't spend proportionally to how much I have in the bank account. I have fixed expenses, sometimes I exaggerate with Amazon, but surely it's not that I do it proportionally: "Now I have [a lot], so I double". No. Here instead by the law of mass action yes: I am... the higher the value of $x$, the more rapidly it must go down, in fact I have an exponential.

In the case of my bank account, here it is as if I no longer had the dependence on the value of the bank statement, on the total value of money in my current account, and so the solution would not be this, it would be a straight line, it would be something of the type constant $(t_0-t)$, something like that. So it is something that instead of going and then slowly saturating, would go linearly, so much so that I can go not only... I go into the red, I can go down arbitrarily, my debt can increase arbitrarily, it is proportional to how much I spend. So it's not this, but the story of the bank account perhaps remains in your mind.

Note: I made this same analogy for the accumulation of calcium in the cytoplasm of a neuron when talking about frequency-dependent adaptation mechanisms. Here I am talking instead about the fraction of post-synaptic channels in an open state. The math is the same, but they are two different concepts. In a first case it was a concentration of calcium accumulating (this could be a kind of influx of calcium, calcium channels opened, there was a *puff* of calcium). Here it is another thing: that is an opening, a quantity of neurotransmitter that arrives all at once in a moment of time and between one moment and another this is the kinetics in question because it is a synaptic receptor that from open becomes closed.

### Integration of the Dirac Delta

The second step is to say: but when I am around $t_k$, how should I behave? So here I can apply the integral on both sides and I apply it across a Dirac delta. I apply it between... suppose this is $t_k$, the $k$-th instant. Using this nomenclature, this formalism of mathematicians, I write (removing this), I write that I do the integration between $t_k^-$ and $t_k^+$. What I should say more correctly is that I am imagining doing this integral, $t_k$, between two points, so in an interval, between two points that are across a Dirac delta, any one (then you see that the thing always repeats). After that I should do a limit procedure, saying that this on the left, this left extreme goes to $t_k$ from the left and this on the right decreases going to $t_k$ from the right. But it doesn't matter that much, because the only thing I have to reason about is that here I have the integral of a derivative. Integral and derivative cancel out because one is the inverse operation of the other.

So let me cancel this... so here by the fundamental theorem of calculus, I know how to write this because I know what the primitive is. The primitive $x$ I calculate at one extreme minus calculate it at the other extreme: so $x(t_k^+) - x(t_k^-)$. I like this because in the end I know what the value of this $x$ is immediately before and I imagine that immediately after there is some kind of jump. In the end the university pays me a salary, here I certainly go up, by I don't know how much, but I go up (unfortunately I always have little, but I go up), that is, after minus before will be some quantity.

This piece remains [the term $-\beta x$]. This is the most annoying piece, because I have to invoke a thing that annoyed me as a student, which is a fairly heuristic thing, that is, I invoke the **continuity** of this function $x$. You tell me: "But you haven't solved this equation, how do you know that $x$ is continuous?". I assume that $x$ is continuous and here basically I am saying that I am doing the integral, that is, I am calculating the underlying area of a function that is ok, that is not pathological. This [the delta] is a pathological function, but fortunately it cancels out with the integral, we will see it in a moment. If it is a normal function, when I do the underlying area it is easy, and it is easy to understand that when I restrict the integration extremes a lot (mathematicians say "when I do the integration on a domain of measure zero", the same thing), the area becomes zero. Because there are no particular discontinuities; at most there could be a discontinuity of the first kind, like this one, but for mathematics this is not particularly dramatic, there are no Dirac deltas.

So this quantity here I know is zero. But there is obviously a heuristic that can disturb you because again I don't know it in advance, so I have to assume it and then I have to verify that it is so. Another way to see it is that this class of differential equations ($dx/dt = -\text{blah blah} + \text{something}$) have the output that is said to be always more continuous than the input, because in between there is an integration operation. The integral, unlike the derivative (which highlights transitions, highlights discontinuities, highlights changes), the integral softens everything, let's say it does *smoothing*, it "smooths", as they say, slows everything down. So that's why I assume that if the Dirac deltas which are ugly beasts (they are strongly discontinuous, go to infinity in zero time; worse than this there could be multiple derivatives of this Dirac delta, they are even worse, but they are not here), if it is a Dirac delta, the $x$, which is in fact a kind of integrated version, at most will be a step. That is, I am imagining in another context, hoping that someone can resonate, that I should talk about distributions, which are these special functions which are the Dirac delta, the step (often called, indicated with $\theta$, the so-called Heaviside function), they are all functions that are... they are not special functions, and I know that in fact the integral of a Dirac delta becomes a step, and the step is a function that has only a discontinuity of the first kind.

Anyway, to make it short, in that case I removed that term which was quiet, quiet, so here I literally write 0. Plus this: it is simple because the Dirac delta is by its definition a function (there are various ways to define it, even one way is using the inverse Fourier transform, or the inverse Laplace transform, or taking a step whose amplitude is inversely proportional to the duration, so if you do the limit, when you shrink the duration, the amplitude goes to infinity), but anyway, there is another definition that I like more because it is axiomatic, where there is not much to understand, which says: the Dirac delta is that function which, if you put it under the integral sign, and the integration extremes comprise it (comprise where the term $t_k$ is, where the Dirac delta is centered), extract the function that premultiplies this Dirac delta. One. Another way to say it is that the integral of that Dirac delta (sorry, here I remove the summation because I am considering only one Dirac delta at a time), another way is that the Dirac delta is that function whose area is always unitary, that is the integral across where the Dirac delta is defined is one. However you want to put it, here you write $A$.

And this is a dramatically easier thing to do than solving, even if it is linear, piecewise constant, the equation satisfied by $O$ for when the pulse, when the concentration of a neurotransmitter is not described by a series of Dirac pulses, but by those little rectangles. So here I gained that I have a simple rule, because this means that **after** (so bringing this part to the right), **after = before + a factor that is always constant A**.

So the university pays me the salary, or vice versa, they are the vesicles full of neurotransmitters that have flooded the inter-synaptic space and have bound to these receptors of which I am describing the fraction. After which, however, here between one and the next it continues to decrease, it continues to decrease exponentially. Here I continue to have the *income* from the university and here I decrease.

### Dynamic Steady State

At a certain point (and this obviously holds strictly only if the Dirac deltas are equispaced, now here graphically I made it return because I am lazy, I made it return after two or three... after two or three Dirac deltas), at a certain point you see that the next Dirac delta happens, arrives more or less always at the same moment. So it is as if there is a repetition, so it is as if this goes down and then comes back up. In this time, which I know to be $1/f$, this value here I know to be the value that was... the same value that was here. Suppose I call this... what do I call it? I call it $\bar{x}$ (x-bar). I call this $\bar{x}$ because, again, it is a kind of dynamic equilibrium. It continues to repeat in the same way at a certain point. It does so because it continues to insist on the residual tail, but at the same moment. So this point here I call $\bar{x} + A$. This decays to $\bar{x}$ and this point here will be again $\bar{x}$ plus... point... plus $A$.

If you want this is a hypothesis: I am hypothesizing that there exists a regime where this thing goes up and down and at a certain point it always does the same thing. That is, dynamically it is as if these peak amplitudes (or you could see the amplitudes instead of... the minimums, or you could see the intermediate amplitude), more or less are the same thing. And the only question is: but what is $\bar{x}$? $\bar{x}$, I know how to write it, because it is $(\bar{x} + A)$ (which is the initial point... I am thinking of this... this $k$ is the value that the function assumes when $t$ is 0; $t$ in this case is the interval, it is as if it were a... I had taken this variable $t$, now this is 0 and this is $1/f$). I know that $1/f$ has passed, and $1/f$ milliseconds before this value was $\bar{x} + A$. And so here I write: times $e^{-\beta \cdot \text{(how much time has passed)}}$, $1/f$.

From here it suffices for me to make $\bar{x}$ explicit, because for example I want to write what the expression of this bottom value is, $\bar{x}$. But I could have written it differently, I could do $x$ again, the peak or the intermediate value. So what I do is write:
$$\bar{x} (1 - e^{-\beta / f}) = A \cdot e^{-\beta / f}$$
I brought this $\bar{x}$ to the left side, factoring it out. This multiplied this exponential, so I brought it to the left and $e$ appears here in this parenthesis. And on the right I write, remains $A$ times $e^{-\beta / f}$. So $\bar{x}$ becomes a kind of function of $f$:
$$\bar{x} = \frac{A \cdot e^{-\beta/f}}{1 - e^{-\beta/f}}$$

Now I won't stress you further, even if, believe me, if you try to do it yourselves, to reason... Theoretically you could also try to write some code to realize the fact that with this rule (so 1 plus 2), which incidentally is similar to what you would write for intracellular calcium in frequency-dependent adaptation, but it is a different thing... here we are talking about synapses and we are not talking about my bank account because I don't spend as much as, proportionally to how much I have. With these two rules you could realize that when there is a uniform regime, a uniform frequency of presynaptic activation, at a certain point this transient, if you squint, you see that there is a kind of band. And it is shown that this band is on the order of $1/\beta$, but it doesn't matter. At a certain point you have a regime and you could describe this regime as the mean value, the peak value, whatever it is, is a function of $f$.

I won't do this, but if $f$ is particularly large, if the frequency is high (which in your mind might say: this phenomenon occurs rapidly, because high means high compared to $\beta$, that is it means that the new Dirac delta arrives before the tail has exhausted itself completely)... Fortunately I am thrifty, so the next salary doesn't arrive finding me exactly at... it finds me that... so that's why I manage to save, I hope these stupid things can be useful. So here in fact it is telling me that I am saving at a certain point, but not that much... it's not that my bank account increases... Ok, my bank account could increase if the income was higher than the expenses. But here I have that the loss is proportional to how many receptors are in that state, by definition of mass action. That is why there is a steady state.

And so when $f$ is sufficiently large compared to $\beta$ (you see here that you have $\beta$ divided by $f$), you can approximate the exponential with the first Taylor term. I won't do it, you can try and you see that you get an expression where $\bar{x}$ is equal, is proportional to $f$. And this is a thing, I repeat, interesting because it tells me that this band, roughly, goes up or down depending on how much $f$ is.

Obviously in the real case, it is not true that synapses are always driven, are always activated by a train of action potentials at uniform frequency. Indeed they never are. However something similar in a regime that is not deterministic but is stochastic, so where moments are unpredictable but happen with particular probability distribution conditions, the same type of reasoning can account for the fact that the synapse somehow responds proportionally to what the frequency is. This is a more advanced topic so it makes no sense to stress you now. Is the green light still on? Yes? Thank you.


# Synaptic Plasticity and Learning

Now, it seems (by now we have known this since the 70s, so it's been more than 50 years, from the 70s and also from relatively recent discoveries) that synaptic transmission is, compared to all the biophysical and biological things I have described to you up to this moment, one of the sites where experience or electrical activity in general changes the structure. So I am talking about **plasticity**, I am in fact talking about... I am evoking the theme of **learning**, which is not necessarily equivalent to synaptic plasticity.

This is a very interesting, very profound thing, because learning memories, concepts, and being able to recall them, does not arise immediately from the consequences of saying: "Look, $T_{max}$ could change over time, or $\bar{G}$ (which was the maximum conductance or the maximum number or the maximum conductance of post-synaptic AMPA receptors) might not be fixed, it might change over time." Maybe it changes very slowly, it doesn't change on a time scale of milliseconds, it changes on a time scale of minutes, of hours.

You see me, you learn (I hope) and something remains in your mind (if nothing else the story of the bank account, which I hope, together with the story of licking your sweat, is by now indelible). That certainly did not happen instantaneously. It may be, however, that now you have dedicated a gene expression to me in which new proteins, new synaptic channels, new synaptic receptors have been inserted into the post-synaptic membranes, because that concept had to be strengthened.

At the basis of this (which I repeat is not so obvious based on these considerations) there are two phenomena called synaptic plasticity (so it is not necessarily equivalent to learning, but it is its probable cellular substrate). And they are of two types; the ones we examine today are of two types: it is called **homosynaptic** and **heterosynaptic** plasticity. It has nothing to do with sexual preferences, but as in that case, here it has to do with something that does not include the presynaptic neuron... sorry, does not include the pre- and post-synaptic pair, while heterosynaptic means that it includes the activity between presynaptic and post-synaptic neuron. It will become clear in a moment.

1.  **Homosynaptic plasticity** is also defined as **short-term plasticity** (STP). And I will show you and we will comment on and study together what short-term depression and short-term facilitation are.
2.  While for **heterosynaptic plasticity**, it is also called and attributed to characteristics of *long term plasticity*, **long-term plasticity**, and it has to do with a (or it is necessary, a necessary condition for its expression) a co-activation or a relationship, a correlation between presynaptic and post-synaptic activity. This is not present in homosynaptic plasticity, which is more solitary, while heterosynaptic means that there is the one who speaks and the one who receives must do something (like the story of the NMDA, of the NMDA receptor).

And so we will talk about long-term synaptic plasticity dependent on *timing* (depending on the timing of spikes) and what is called redistribution of synaptic efficacy.

## Short-Term Plasticity: Inertia and Limited Resources

One important thing to realize (probably an engineer doesn't realize it because for him synaptic transmission is a gain, it is a value similar to an amplification factor, or to the value of a resistor on a wire, the resistance of a wire which is that, and ok, I put a signal, the signal at the destination becomes amplified or attenuated, but it doesn't change over time, what changes maybe is the signal). Instead synapses, which are objects, as said, are probably the most complex organelles that exist in biology, out of all living beings, have a certain dynamics, latency, inertia. It takes some time for neurotransmitter to be expressed, synthesized, so these vesicles are packed, are then anchored in the presynaptic membrane.

A disgusting and stupid analogy again is that of the **llama**, the animal llama which I believe (some do it, I haven't seen it done) spits. The synapse spits. If you ask the llama to spit with a certain frequency, and the frequency is very high, the llama might have a dry mouth and simply have no more saliva to spit (a disgusting thing, with respect for llama animals), to make you participate in the fact that this is not a system... it is not a wire, it is a dynamic system.

Then just as I could be with my voice (I have a bit of a cold but I think I'm managing), where now I am ok, but at the third and fourth hour my voice with activity will be much lower. This regardless of whether you speak, are attentive or not. It is only my problem, a problem linked to my ability to *replenish*, to restore my resources in generic terms (I won't say what they are), the resources for my neurotransmission.

### Synaptic Depression (Short-Term Depression)

And so these synapses have fatigue, inertia or **depression**, therefore depression of responses during repeated activation. So if the presynaptic neuron, on its own, has an activity, a frequency of, I don't know, 50 spikes per second, it may be that they are too many for the synapses. They may be too many for the synaptic vesicles even before the receptors. The receptors maybe desensitize on their own, but I might not have any neurotransmitter left to release because I really [lost] my voice.

### Synaptic Facilitation (Short-Term Facilitation)

However, as sometimes happens to me (surely it happens to you too), the same dynamic system can instead show a dual behavior of not a depression of responses, but a **facilitation** at times. And I imagine that the hope is that you can do it also during the colloquium course in bioengineering, where I ask students to make a presentation at the exam, to organize themselves in groups during when speakers come and therefore to start putting yourselves out there, to give presentations. It is useful, job interviews and other things, I don't have to tell you. It might have already happened to you for example in your bachelor thesis defense that at the beginning you were quite nervous and then you warmed up.

So this context of warming up or depression has only... it is on the short term, because if you leave me a little bit of time without speaking, my voice returns. Vice versa, if you are nice and pumped up because you are in the middle of your presentation and I put you to sleep, you return to a certain baseline value of anxiety versus confidence. So it is independent of what the *audience* does, it is independent of what the post-synaptic neuron does. If it were dependent it would be a process of heterosynaptic plasticity, because it would involve both the speaker and the listener. Here it is only the speaker: if the speaker speaks too much, speaks too frequently and gets depression or vice versa gets facilitation.

## Experimental Evidence: Pyramidal and Interneuron Connections

This thing of short-term depression is actually a relatively recent observation from the years 90-2000, but it is not a surprise because people who studied the neuromuscular junction knew it all along, since the 70s, that when the release of acetylcholine produced a muscle contraction, at a certain point the nerve fibers of the neuromuscular junction could simply run out of acetylcholine. You can try, even if from the muscular point of view you also have the onset of lactic acid, and therefore you also have other mechanisms, even other desensitization mechanisms of nerve fibers (so somehow also of axons), but somehow if you try to perform a muscle action "give and give", at a certain point you can't do it anymore. I repeat, in that case it is more due to lactic acid in the muscle, but the concept might be familiar to you.

So this is an example, not in the neuromuscular junction, but between glutamatergic synapses between two pyramidal neurons of the rat somatosensory cortex. Here you see two cells, one is green (they are three-dimensional reconstructions), one is green and one is black, they are very very close, their somas are very close. The experimenter had two pipettes, one in the soma, in the belly of the black neuron and the other in the belly of the green neuron and in one of these (as in the experiment I showed you last time), instead of just giving a "flick" (*schicchera*) to produce a presynaptic spike, the experimenter gave a train. I believe they are ten, three, whatever it is, plus he let elapse perhaps 800 milliseconds or thereabouts, and then he gave a further flick to emit a further action potential. And he measured the post-synaptic membrane potential.

The thing is more interesting compared to last time. Last time one could only see that, basically by making a spike fire, you saw if the post-synaptic neuron was connected or not (which had notable importance, it allowed in part to infer the connectome at the cellular level, I told you). Here, surprisingly, people (I don't know why it took until '97, the late 90s, why people didn't just give a pulse... people maybe gave two, but give a train and maybe change the frequency too, so maybe you can study the frequency-dependent behavior of the synapse).

To make a long story short, what is seen in the post-synaptic neuron is a progressive decrease. These are excitatory post-synaptic potentials, here the cell is simply recorded in so-called *current clamp*, at -70 mV, at -60, whatever it is, and again here the amplitude of these events is tiny tiny, they are synaptic events, they are very very small. It gets tired. But if you wait a bit, so there is a pause period, you see that the amplitude of the response to the nth spike, the subsequent one, is much greater than the amplitude of the last one of the train, not yet completely restored, not completely recovered compared to the amplitude of the first one. The amplitude of the first one is higher because the experimenter had waited a few minutes before resuming stimulation. So in fact this trace I am showing you is the average of several repetitions and between one repetition and another the experimenter waited maybe not a few minutes but certainly a few tens of seconds.

### Target Specificity: Depression vs Facilitation

The interesting thing is that if this same neuron (suppose the black neuron, which is this one here that I made fire, the one that is presynaptic) by chance projects (here I don't think it is here, maybe it will appear on the right, but anyway), if it projects not only to another pyramidal neuron, but projects to an **interneuron**, to another neuron in particular they are called **basket cell** (they are basket cells, like a basket, I don't know), in their vicinity, they are typically inhibitory neurons. I am a pyramidal, I am excitatory, I project (I repeat this is **Dale's law**: if I am excitatory, glutamatergic, all my synaptic boutons are glutamatergic), so I release, I spit to that other neuron with a synapse to that other pyramidal neuron and the synapse however depresses.

And it has another post-synaptic target (it's still me, so I still release glutamate), but the synapse towards that other neuron, look what it does? In fact it **facilitates**. It is not so much different in amplitude with respect to the same order of magnitude of a fraction of a millivolt, but it is a completely different behavior: it tends to be, to facilitate, as... so not to turn off progressively due to fatigue, but to "get pumped up". And if one waits a certain interval of time, practically the amplitude of the response (so the response to the nth spike) becomes comparable or roughly comparable to the first one.

* **Depression (Pyramidal-Pyramidal):** Here I have dry mouth and I have no more saliva, I have no more resources, so the neurotransmitter vesicles have probably run out, they are empty, they haven't had time to restore themselves yet. Yes, the mechanism is very fast, but it might not be so fast at this frequency (which I believe is... Well, there will be 4 or 5 spikes in 200 milliseconds, so it's 25 Hz, it could be. I am thinking if this was instead 1000 milliseconds it would be easy... so it could be, so from 200 to get to 1000 I have to multiply by 5, so they would be yes, around 20-25 Hz). So we are not talking about hundreds of Hz as in the previous case where AMPA desensitized. Here already at physiological frequencies, 25 Hz is ok, actually even 20 Hz is ok, synapses have a strongly time-variant behavior.
* **Facilitation (Pyramidal-Interneuron):** And in this case ditto, here it is not dry mouth, here it is probably (this I am giving you two explanations) here it is probably the **accumulation of intracellular calcium**. You know that the release of these vesicles depends on the influx of calcium, it may be that there is residual calcium in the synaptic bouton, so it persists and if it persists it might make release easier when there is a further influx of calcium.

The real question is: but why do two synaptic boutons coming from the same axon, of the same presynaptic neuron, in one case do depression, in one case do facilitation? No one knows. There seems to be a specificity of the type of synapse based on the identity of the target, of the interlocutor.

## Functional Interpretation of STP Specificity

Again, with an interpretation partly probably superficial: if I am excitatory and I project to another excitatory neuron, perhaps it is better that my communication channel, since it is glutamatergic, loses strength. Because in the case of recurrent connections it could easily explode; this could be a **positive feedback** and could explode. So if I have a mechanism whereby I get "dry mouth", my voice goes away immediately, it may be that I manage to interrupt eventually a **seizure**, an epileptic crisis.

Perhaps this is exactly for the same reason [that the opposite happens with the interneuron]: I, glutamatergic, excite you who are inhibitory, who are GABAergic, and your role is probably to act as a peacemaker, to inhibit everyone. Because maybe I might have gone crazy. Also again, if these frequencies are not extremely high, it might make sense that I continue to facilitate response after response. Because while here I discourage excitation (the emission of a spike by the post-synaptic neuron), here honestly a 2-3 mV EPSP does not make a neuron fire; at most if I am at -60 mV it makes it arrive at -65 mV. Ok, if there were more neurons simultaneously maybe I would be close to threshold and could fire, but it may be that if this first EPSP made me arrive at the threshold, the last one certainly won't [in the case of depression], so I can interrupt like this, even without going completely voiceless.

Here [in the case of facilitation], if the EPSPs are, again, are excitatory post-synaptic potentials (they depolarize the neuron), but the post-synaptic neuron is inhibitory, at a certain point (and this is seen experimentally), if I continue, at a certain point a synapse... I manage to make the inhibitory neuron fire. So I have to fire many times (and I am not talking here about temporal summation, but I am simply talking about this facilitation). It may be that therefore it is evolutionarily profitable that, when I am perhaps gone crazy, the synapse with an inhibitory interneuron is particularly activated, so the GABAergic inhibitory neuron activates, fires and since he by trade releases GABA, he silences the whole network.

Again, the interesting thing is that this is calcium-dependent and seems to be again... here is the graph where the pyramidal neuron with respect to the interneuron has even potentiation and then even a spike; in the case of pyramidal-pyramidal a depression.

## The Experimental Challenge: Connectivity and Multi-Patch

This was quite an important discovery. I repeat, it is trivial in the case of the neuromuscular junction, but no one for some reason had observed it between neurons of the central nervous system. One of the reasons is that, as I told you last time, since these pyramidal neurons (or the pyramidal neuron and this cell which is probably not a *basket cell*, but is a **Martinotti** cell, it is called, it is a GABAergic interneuron that stays near layer 5; this is layer 5 where the somas of these cells are), the **connection probability** is very, very low.

So people at least must have two pipettes, two electrodes, two amplifiers, two micromanipulators. And I told you last time that if you have only two micromanipulators, two pipettes, two amplifiers, you are destined to find few connected pairs. If instead you have 4 or 6 or 8 or 10 or 12... if you have 2, 3, 4, 5, you have a number that changes quadratically [exponentially in speech, but implies combinatorial] of possible pairs: $n \cdot (n-1)$. If with 2 you have only 2 possibilities, with 3 you already have 6, with 4 they become 12: you have already increased by an order of magnitude.

This guy, **Henry Markram**, who was my advisor at my second postdoc at EPFL in Lausanne, had a setup... he is an artist, he has the hands of a violinist. Because to have this ability to (despite them being controls, so you perhaps would be more impressed by a *gamer* who plays rapidly), the same class, he managed to put the pipettes very rapidly into the soma of these two neurons without losing the other *patch*, without introducing vibrations. And he had four, and so he had more than the others and managed to find easily and describe this phenomenon that maybe the others saw once every two or three months.

### Dendro-Dendritic Synapses in the Olfactory Bulb

The interesting thing is (maybe I have to take a break, so I simply tell you that) also in the **olfactory bulb**, in cell types called **mitral cells**, one has the same thing. A mitral cell has a target, has a behavior (which is poorly seen here because it is crappy) depressive, and in others it has a facilitative behavior. The interesting thing about mitral cells (maybe I was talking about it with one of your colleagues last time) is that here the synapses are **dendro-dendritic**: so they are not the neurotransmitter vesicles at the end of the axon, but they are in the dendrite. Simply another example.

I'll stop for ten minutes, sorry if I have enthusiasm for synapses. Thank you.


## Kinetic Modeling of Synaptic Depression

So it is worth it, why not, with the usual kinetic schemes (this time applied in another context) to understand something about it. Because again, if we manage to put in a slightly more formal way the phenomenon I described to you... Up to now I simply told you what people saw: the fact that it is known that it is partly linked to the depletion of vesicle resources (that there are no more vesicles because they are in limited number) or there is an accumulation of free calcium in the presynaptic bouton that changes the offset, makes release much easier; these are things that came later. And the advantage of being able, for example with kinetic schemes, to give a quantitative representation, because then I can open it, I can do everything, I can partly simplify it and extract again some intuitions, like the one that a synapse converts a frequency into an analog level proportional to it.

Here I am thinking, so I describe only **short-term synaptic depression**. And in fact, even if it is wrong (because we don't know if the phenomenon I sold you up to now as short-term depression is totally a presynaptic fact... intuitively I told you that vesicles are limited, so I am stressing the fact that it is only a fact linked to the presynaptic bouton; currently we don't know if there is also a dependence on the post-synaptic neuron, in particular on the population of post-synaptic receptors; probably they are two combined things).

### The Three-State Model (R, E, I)

So much so that in the literature people said: "You know what? I generically describe what are the **resources** for neurotransmission". You will tell me: neurotransmitters. I call them resources, so the *reviewers* of an article cannot attack me, saying: "I didn't say it is a presynaptic phenomenon". For simplicity we can imagine that we are talking about synaptic, presynaptic vesicles.

And so these presynaptic vesicles, or rather the neurotransmitter contained in them, can be found in three states (again here you see the phenomenological approach, I am not describing in a biophysical way). I am simply saying: look that they can be in a state:
1.  **Recovered ($R$):** Also called *ready*, *releasable*, ready to be released, restored.
2.  **Effective ($E$):** In an effective state, where they have been released and are available to bind to post-synaptic receptors.
3.  **Inactive ($I$):** And then they can be in an inactive state, whereby, for example, they are diffusing laterally in the synaptic *cleft*, in the inter-synaptic space or are in the *re-uptake* phase, of fishing back, to be recycled and put back inside, re-packaged in vesicles.

There are therefore three states and three transitions, they are not reversible. So if you are ready to go, when there is... this is an activation probability, the release probability, or a release *rate*. Historically it was called $U$ as **Use**, in a barbaric way, but it was so. So when a vesicle is ready for use, if with a probability of use (clearly this probability is normally zero and becomes non-zero when there is a spike)... In fact this thing I am telling you is the story of the neurotransmitter rectangle, but with all the trimmings, much more sophisticated: there it was a rectangle that goes up, reaches one millimolar, $T_{max}$ and goes down. Here I am saying: with this thing here, I make that $T_{max}$ dependent on previous activity, on previous history.

So here when it is used, when there is a spike, it passes from "ready" to "effective": the neurotransmitter is released in a very very fast way. And then in an equally fast but slightly slower way (with a kinetics that here I am writing as one divided by a time constant to be able to skip a step and in fact introduce because it is convenient for me intuitively to speak of time constants, of time scales), with which an **inactivation** phenomenon occurs ($1/\tau_{inact}$). Or even with which a phenomenon occurs whereby vesicles that are in a state no longer functional, no longer useful to bind to post-synaptic receptors, have a period, a **recovery** time scale, *recovery* ($1/\tau_{rec}$), which again accounts for this transition with a *rate*, which is a rate, so it is not a time, it is the inverse of time, it is a speed.

The thing I notice is that obviously (otherwise we wouldn't have seen it if the llama or if Michele Giugliano didn't need a certain long time to restore the neurotransmission vesicles, saliva or voice, we wouldn't have seen, we wouldn't have appreciated short-term depression). So it is reasonable to assume, because it is a consequence, that this arrow here, this transition is much slower. So the value here of this speed is small, that is the time constant is large: the time scale is long, 500 milliseconds, against this one, the inactivation time scale, which maybe is 1 millisecond, 5 milliseconds. The neurotransmitter disappears immediately, but then it takes a lot of time, 100 times more slowly to be recovered.

## Differential Equations of the Model

So the first thing I do, now I write a differential equation for each state. But you will see that it starts to get a bit hot because they are three differential equations (actually they are two because one is linearly dependent on the others), but they are ugly. I would like to have some expression that tells me: "Here, this is the amplitude of a neurotransmitter pulse based on past history". But let's start.

The first is, again, they are as if they were water tanks.
$$\frac{dR}{dt} = \frac{I}{\tau_{rec}} - U \cdot R \cdot \delta(t-t_{spike})$$
So does $R$ decrease or increase? It decreases with this arrow, proportionally to how much liquid there is, and increases, for this other incoming arrow, proportionally dependent on how much $I$ there is. So it is minus $U$ times $R$ plus $I$ divided by $\tau_{rec}$. So I wrote this as "1 over" because now it is easy to make approximations.

The second equation is for $E$. $E$ appears because this arrow enters and disappears because this arrow exits. The exiting arrow is easy: minus $E$ times $1/\tau_{inact}$. And $I$ plus is $U$ times $R$. Again you start to see that obviously there is some symmetry. What exits here enters here.
$$\frac{dE}{dt} = -\frac{E}{\tau_{inact}} + U \cdot R \cdot \delta(t-t_{spike})$$

So $I$ increases again because $E$ enters proportionally to $1/\tau_{inact}$ and disappears proportionally to how much $I$ there is with this time scale. So $dI/dt$ plus $1/\tau_{inact}$ and minus $1/\tau_{rec} I$. But for the moment I leave them like this.

### Simulation and Separation of Time Scales

And I show you what happens if I take those three equations and simulate them. Obviously the thing I have to do is take (otherwise the system goes into some *steady state* and does nothing interesting), I take $U$ and instantaneously make it become a certain value and then bring it back to zero. This simulates the arrival of an action potential for me and simulates the fact that the release probability, synaptic release which in the end is how much is described by this $U$, is very rapid. The release probability changes, normally it is zero (normally in reality it is not really really zero because there is a basal level... synapses are, I told your colleagues, they are a bit incontinent, they leak, so they release vesicles even when there is no presynaptic action potential). But at this level and in this deterministic context the lion's share is done by the so-called synaptic release evoked by presynaptic activity.

So what I did is set $U$ to some function that is 0 always, then at a certain point becomes a certain value, the numerical value doesn't matter, and then goes back down. And what I saw is that for free this system of equations brings me reason for one thing that is very simple: that is there is a **very rapid emptying of $R$**, of the $R$ compartment, and then there is a **slow recovery**. And I imagine it: the synapse has these vesicles, I let a few out, it is obvious that those remaining [are] decreased, they are no longer 100%, they are decreased. And due to this cycle, which I have for free with the kinetic equations equivalent to this kinetic scheme, it starts slowly, with the same time scale of $\tau_{rec}$, starts to return towards 100%. I expect it to reach 100%, because I would like, I expect that there are no other branches here. When $U$ is 0, ok, if I wait a very long time, 100% of the (so the occupation probability becomes unitary for $R$ or in other words, all vesicles, so there are many, independent, identical) return ready to act.

The thing I see is for example that the effective ones ($E$) are the echo of this decrease of $R$, because $E$ and $R$ have similarities. You see however that it is a little slower compared to $U$. $U$ is an impulsive signal, because I made it and I wanted to make it impulsive because I wanted to understand what would happen if I said "now release the vesicles and then I step aside". What you see is that the rising phase of $E$ is steep and then the falling phase is not instantaneous. It is not instantaneous because this time constant $1/\tau_{inact}$ is always the usual little game, the same differential equation is always that, they are decreasing exponentials. Ok, it is quite faster than this time constant, than this exponential that takes a long time to return to 100%, but surely it has a finite time, it is not the Dirac delta which is instantaneous.

So again, the thing that interests me is only... there are two things that interest me for my subsequent approximation: the inactivation, the inactivation process are very fast and the recovery, the filling of neurotransmitter vesicles is very very slow. Translated into mathematical terms, this $\tau$ has a numerical value much larger than $\tau_{inact}$. And the other element is that the activation of vesicles, this one here, is very rapid.

Since I am in a mood devoted to Dirac (who by the way was a very curious guy, very interesting, there are books on Dirac's life, he was a bit peculiar), I say that it is a **Dirac delta** centered, so shifted, with the minus (this means that I centered it at the generic instant of a presynaptic action potential). And then I specify the area of this Dirac delta: I say it is big $U$, so usage factor, but big $U$, because it is a constant, while $u(t)$ is a function of time, big $U$ is a number. I have to put it because otherwise I would remain with the area of this Dirac delta being 1, because it is exactly 1 and not 1.2 or 0.65 or 45. So this way I fix the idea and that is the area of the Dirac delta.

## Reduction of the Model: Separation of Time Scales

I need these two pieces of information and I manage to reduce three differential equations into a single differential equation, which I manage to unpack. Ok, to finish, this is $I$. $I$ is quite boring, it seems somehow to follow the same dynamics as $R$. What I am inactive, I remain inactive for a long time, just as much time as I take to recover. So it is as if there were only two mechanisms here: one, that of black and green which are very fast, and another time scale which is that of blue (ok, of red, but blue and red are roughly similar). This is a technique that is often used in various fields of physics and biophysics called **separation of time scales**. Here I am realizing that there are two phenomena that are on different time scales, so maybe I can consider one immediately at *steady state* and the other instead that still has to start doing the dynamics. Now I show you in what terms.

So I resume the equations and start considering this differential equation here, that of $E$. Here I simply transcribed it and one thing I do simply for convenience, I multiplied now both members by $\tau_{inact}$, so that here I get that type of form $\tau \cdot df/dt$, which can be typical, because I like seeing that here there is a time in the numerator and there is a time in the denominator. In reality, what I am saying is that if $\tau_{inact}$, compared to all other time constants in question, in particular $\tau_{rec}$, which is the most important one, if it is very very small, it means that this equation here (remember the engineer "squints", says this is a *black box* whose output tracks the input and tracks the input with this time constant). If this time constant is very very small, output and input are practically the same thing. In other words, this equation is already always at *steady state*, that is it doesn't change anymore, there is no more dynamics, I can cancel this derivative and I can therefore write that $E$, so this second member, is equal to zero, that is $E = U \cdot R \cdot \tau_{inact}$.

And I got rid of this differential equation. I repeat, it is an approximation because $E$ was not instantaneously a copy of $U$, it had a certain little tail, but here I, with this reasoning, say that it is sufficiently analogous to represent a transition.

If I want to put, I want to link the two things, the kinetic scheme, open/closed post-synaptic receptor, $T_{max}$ to this world, I must probably say: wait wait, before I do other things, $T_{max}$... and I must probably put it in analogy or link it to the peak of this little neurotransmitter pulse, because this has the meaning of neurotransmitter in an effective state, active, ready to bind to post-synaptic receptors. It is exactly $T$, but while there I had the little rectangle, here I have something that is a transient with an exponential that you saw numerically. So what I could do is I could say: I take the peak. So if you have a function that mathematically is written like this, then I see here now that the peak is the factor that premultiplies this mess here. But if I didn't have it, I could say, to put $T_{max}$ here, I could say that I take $E(t)$, which might not have that form after my approximation, I integrate it and then divide by $\tau_{inact}$.

If you take this expression here, do the integral from minus infinity to plus infinity, do the underlying area, you know how to do the exponential because the integral of the exponential is still an exponential, apart from a factor that cancels what one would get by differentiating the primitive function, and this is the term I put to compensate for the term you get by doing the derivative. You discover that by doing the integral and dividing by $\tau_{inact}$ you find exactly the peak, because in other words if you take just an exponential like this divided by $\tau_{inact}$ it has an area, it is a function that has a unit area.

In other words, I want to put this peak here, and here I am thinking: this looks a lot like a single exponential, but in general now here I made a mess, I no longer have an exponential, I have $U$, which is a Dirac delta, or $R$, which is a function of time, ok, this is a quantity. So what do I have to put here at $T_{max}$? With this trick I can say: do the integral of $E$. Here the integral is only... so it is only... ok, it is not very trivial, it is not very trivial. I say that, since here there is a Dirac delta, the integral pulls out the value of the function $R$, which is under the integral together with the Dirac delta, and gives me the value of the function $R$ at that point. This is again... is used is a heuristic made on the basis of the definition of Dirac delta and relies on a graphical interpretation. If I had written directly suddenly: "put here instead of $T_{max}$ put big $U$ times $R$ at the moment of the spike", you would have said: "Ok, where does it come from?". It comes from here. If you do this, despite this approximation, it continues to have a link with what is the peak of the neurotransmitter described instead by this three-state kinetic scheme.

### The Resource Equation (R)

This removed from the way (so again here I have the way to link the two worlds), I return because I was left with two differential equations, I would like to have one. So here is what remains, obviously it is an approximation, remains from the differential equation that was $dR/dt$. Now I use the famous, usual expression of conservation of mass. In all kinetic schemes, one differential equation is a linear combination of the others, because in the end either you are in $R$, or you are in $E$, or you are in $I$: they must sum to 100%. Sorry if not I don't see...

So I write 1 as... One of those quantities I write as 1 minus the rest. So $I$ I write as $1 - R - E$. And I know how to write it, $R$ I leave indicated, simply becomes $1 - R - U \cdot \delta \dots$ because $E$ is approximated. Obviously this continues to be an approximation because this is an approximation, but actually this is an exact expression, $I$ equals 1 minus the complement. So, in the last equation that remained, I had $R$, which decreased proportionally with $U$ ($U$ times $R$, here now I wrote big $U$ times Dirac delta $R$), and I had an incoming term that was due to $I$. But now $I$ I write as $1 - R$ (neglecting $E$ because it is very small in time), and so I manage to factorize and write a single differential equation where on the right hand side I have only $R$.

So I have no other unknowns, $R$ is enough for me, which is the quantity of neurotransmitter in the recovered state, which has a certain slow dynamics and which behaves... if you imagine that there are no spikes, this stuff here you remove it, I am distant from where there is a spike, so that term is null. So if this null term multiplies by 0 also this $R$... it is slightly more annoying than the bank account equation etc. that I showed you before, because here the input is multiplicative and here I have no escape, the input I have to keep multiplicative. What remains is:
$$\frac{dR}{dt} = \frac{1-R}{\tau_{rec}} - U \cdot R \cdot \delta(t-t_{spike})$$
Looking at it I can say: "But yes, sure", apart from the minus which makes me happy because in the end nothing explodes, it is a differential equation that normally has arcs of exponential (I don't know if they go down or go up, but they are always arcs that then saturate), and at *steady state*, if there are no inputs, this goes to 1. How do I know? If there is the *steady state*, $R$ is constant, so the derivative is 0, and this term here vanishes when the numerator vanishes, that is when $R = 1$. In other words, this piece here accounts for the fact that now $R$ starts where it must start ($R$ always between 0 and 1 because it is a fraction, and this is a constant that holds when I work with kinetic schemes). It starts from where it starts and then slowly, with the time constant $\tau_{rec}$, returns to 100%, which is what I saw in the trace (I don't remember if it was red or not, it was blue before). $R$, even if before it was the complete model, without having any approximation, that's what it did. It started, it was immediately, dramatically decreased due to use and then tended to recover.

Now, this term here, beyond the fact that you could say: "Ok, I turned off my brain as you told me to do, this thing came out so I believe it is equivalent", but intuitively this nevertheless makes sense. I cannot trivially do, to realize it, do the integral of both members. Here I would know how to do it (fundamental theorem of calculus, the primitive, ok); here I would do exactly the same thing (continuity, the integral on a set of measure zero and therefore it is zero). Here I am screwed, because here is the integral but there is $R$ and it is not done like this. It is done by separating variables, but we won't do it.

What you see is that here is a quantity all positive. The Dirac delta is stuff that goes to infinity. $U$ is a release probability, presumably it is a positive quantity. $R$ is a quantity between 0 and 1. Here there is a minus: when there is the Dirac delta, here there is a very strong subtraction of quantity and therefore $R$, which before was traveling at 100% because it was the *steady state*, starts instantaneously to lower. But there is $R$, that is, it does it proportionally with how much $R$ there is. That is if you imagine that there is not only one spike, but many presynaptic spikes, it is not that here you remove, remove, remove like Amazon expenses from my bank account. Here the expense depends on how much money I have: if this $R$ which is between 0 and 1 tends to become small small small, the quantity I subtract becomes small small. In the end if I have few neurotransmission resources in my synaptic bouton, here he says: "Look, you have to give...". Another way to read probability: the probability of 0.5 means 50% you have to give. No, it is 50% of those remaining, because it is multiplied by $R$. And it makes sense. If I don't have any, I give at most a fraction. It's not that I give an absolute number. The fact that there is "times $R$" means that I am giving a fraction of it.



## Dynamic Steady State and Frequency Dependence

Ok, this was the intermediate step that should have appeared earlier. Having done this reduction, a single differential equation (I won't bore you with doing pen and paper to see what it's like), I can take it and in an analogous way, similar to that of the bank account with this tail, I can that is say: if the presynaptic activation is a train of pulses at constant frequency, of which the period is $1/f$ (the frequency $f$), I can calculate what is at the **dynamic steady state**.

Before I had called it $O_{ss}$ (*steady state*) and it was the fraction of post-synaptic receptors. Here, somehow, I am thinking about what is the amplitude of these little pulses of neurotransmitter due to this mechanism of synaptic depression. And what I see comes out (I won't do it for you), an expression comes out that is not proportional to the frequency, although I can eventually do the Taylor series expansion here; it is a bit more complicated than before and accounts for the fact that if the frequency... (so first of all somehow, however it depends on the period; now sorry here I indicated the period $T$ and it is disturbing, I have to change this slide and put $1/f$ because otherwise you think $T$ is the neurotransmitter concentration).

This $R_\infty$ is the regime value, and it is the one, note, that I was putting here. So I put $U \cdot R$ here at $T_{max}$, so $R$ counts. And if you do exactly the same transition story (it is slightly more complicated due to that fact of this multiplicative term, where the Dirac delta cannot be done trivially with that integral), you find that however it is a dependence on frequency. So, and this is not surprising, the amount of neurotransmitter itself depends on the activation frequency: it is not a surprise, the slower you go, the more this amount of neurotransmitter will be ready, will be at 100%; the higher the frequency, there will be less and less neurotransmitter released into the synaptic space.

But if you look at the specific dependence on frequency, and you actually make this graph where on the x-axis you have the frequency (that is 1 divided by the period) and on the y-axis you have this quantity, you see that there is a region for low frequency where there is a dependence of the amount of neurotransmitter on the frequency. So it is a synapse that tends to become, to grow with frequency. However, there is a certain frequency in which a certain regime (here they call it **supralinear**, **linear**, and **sublinear**), in which the slope of this curve tends to decrease and at a certain point saturates. There is a frequency, for example, at which the synapse becomes insensitive to frequency, at least from the point of view of the amount of neurotransmitter.

### The Synapse as a High-Pass Filter (Differentiator)

This for example is seen here: this is an experiment in which the frequency of the presynaptic neuron was changed from 0 to 15 spikes per second, 30 spikes per second, 80 spikes per second. And you see, besides the behavior indicated here (and this accounts only for the *steady state*, for the fact that here it seems a behavior that has an integrative transient, here it seems to be a behavior, ok, proportional to the frequency, fine, there is as much more neurotransmitter the higher the frequency is, and then this neurotransmitter will further activate the synaptic receptors which also have a dependence on frequency).

But see what very interesting thing happens at a frequency, at a frequency jump, from a frequency of 30 spikes per second to 80 spikes per second. You have a... so practically the amplitude changes, but it changed little. But the transient signaled the variation in the neurotransmitter concentration. What I was obsessing you with last time saying: look that frequency-dependent adaptation is a filtering and it is a **high-pass** filtering, because nature is, I was saying, total, always continuous change, so the nervous system has accustomed itself, has evolved to discard things that do not change over time.

Here you have in fact the same system at the level of the single synapse, which is a crazy thing. Because not only is it crazy in itself (because at the level of the single synapse), but because every single synapse will potentially have a different history, a history dictated by the presynaptic neuron that controls it. And anyway this *high pass* behavior, of high-pass, of differentiator, changes as the frequency varies. If the frequency is low, here it is more similar to a slow integration.

So to make a long story short: synapses (whatever the *Large Language Models* and *Deep Learning* architectures say), in vivo, biologically, are not weights $W$ that are constant over time. They themselves have filtering properties. For those of you who are particularly interested (I am not sure if in the various *machine learning* courses you are told this), try looking for **KANs** (**Kolmogorov-Arnold Networks**). It is a different architecture proposed by people at MIT very recently, a few years ago, two or three years ago. And somehow the same concept of having filters and of... So it is not synapses as weights that change gains, but they are temporal computations that underlie the possibility of learning, the possibility of approximating functions. I am not saying that Kolmogorov-Arnold Networks implement exactly synaptic depression and facilitation, but they are similar.

Ok, here simplifying things it becomes a further law: for very high frequency (but I was interested at all frequencies) it becomes $1/f$, which is this $1/f$ trend, power trend, but which I won't talk to you about.

## Network Phenomena: Up-States and Down-States

Before taking the break I wanted to tell you briefly that this frequency dependence of the amount of neurotransmitter (which we have now seen thanks to this reduction of the model in a very in-depth way): practically we have extracted what is potentially a **computational primitive**.

If you imagine it simply as something that when there is a lot of activity turns off the voice (and therefore the synapse no longer transmits), you can immediately imagine or understand why if I plant (or rather if a PhD student who is now a researcher at the University of California in San Diego, Joao Couto), if I put **silicon probes**, electrodes, in the medial prefrontal cortex part of an anesthetized rat (and since each has more than one recording site), what I record is an alternation between epochs where there is activity (**Up state**) and epochs where activity disappears (**Down state**). Up, down, up, down.

And the same thing if I take a network of neurons *in vitro*, so not an *in vivo*, complex system, but I take dissociated neurons, the famous cell cultures, I put them in a Petri dish (maybe a Petri dish plus... maybe I already told you, with electrodes like a "bed of nails", where many electrodes detect the activity of many units). I can somehow intuitively appreciate why the activity consists of epochs in which all neurons fire, then the synapses get tired and these neurons detach. When the synapses start to *recover*, the neurons can connect again and the activity is no longer disordered but is synchronous (everywhere that kind of fireworks represents the top view of these electrodes). You see it well now that a so-called **super burst** arrives.

This is a network phenomenon, it is not the individual neurons that have on their own the property of being *bursting cells*, *intrinsic bursting*. It is at the network level. This thing of the violence of the slaps, of the smacks that I would start, here they start it by themselves: at a certain point here there is a lot of activity, but the neurons detach and are silent for a while, because the synapses got tired, they ran out. And you see spontaneous activity, you see that there are these waves of synchronized activity, which arrives spontaneously because:
1.  The system is at body temperature, 37 degrees, so there is channel noise.
2.  Synapses are incontinent, so there is synaptic release anyway and so post-synaptic neurons are a bit activated, a bit stimulated, even if there is no presynaptic activity.
3.  And last thing, connectivity (in the cortex I already told you about it, it is around 10%, etc.): in these cultures the probability of having two random neurons connected here is higher, and it is 30%, and it is a recurrent connectivity. I connect you, you connect... but at a certain point somehow it returns. It is a **positive feedback**.

This positive feedback thing, with simply the element of which we have potentially also examined the dynamic component (low pass, high pass filtering, which I put there for you, I don't expect you to digest it so easily), but simply the story that I get tired and I have no more neurotransmitter in the synaptic bouton to spit out, explains to me for free, instantaneously these phenomena that are very important. Because be it a network of neurons *in vitro*, but be it more importantly the cortex just now *in vivo*, it is a mode of operation of all parts of the nervous system during development and every night when you sleep (not when you dream, but when you do long-wave sleep, **slow wave sleep**). Your cortex synchronizes, synapses get tired and it turns off, it resynchronizes, etc. etc., and you sleep.

Let's take a 10-minute break. Thank you.


# Mechanisms of Long-Term Plasticity (LTP)

*(Interval / Answer to questions)*

I believe generally no, ok. Ok, so, in today's lecture and the last one, we basically brought up, even from a quantitative point of view, several concepts. The last of these was the **release probability**, which is the most ethereal one. What does release probability mean? Yes, ok, I can define it mathematically, but to what do I attribute it? This is more complicated.

Surely I talked about vesicles, I talked about the number of, or anyway I talked about the quantity of neurotransmitter that is released (so somehow how many molecules of neurotransmitter are inside each vesicle) and I talked about post-synaptic receptors.

In the case of **long-term plasticity**, it is conceivable that they can change in a persistent way (not dynamic, not activity-dependent like the story of short-term, homosynaptic plasticity, where somehow I told you that there is some kind of relationship between the frequency, the demand for use, and the quantity of neurotransmitter that is ready to be released). There can be instead a persistent, dramatic, structural change, which could have to do with:

1.  **Number of Receptors ($N_{post}$):** For example, going from 1 to 2, the number of post-synaptic receptors. This would lead to a change in synaptic efficacy, a change in the maximum synaptic current, because $\bar{G}$ (G-bar) would change, the total number of post-synaptic receptors which is encapsulated inside that $\bar{G}$ would change.
2.  **Number of Vesicles ($N_{pre}$):** Another thing that could change is, passing for example from here to here, is the number of vesicles containing neurotransmitter. So it could be that there is a structural change where in fact $T_{max}$ will change. True, $T_{max}$ is a dynamic quantity that depends (in terms of short-term plasticity) on the past history, but ok, you would have more. The quantity would change, a scale factor. Therefore the quantity would change or, if you want, we could say the release probability could change; you have more vesicles so it could change.
3.  **Single Channel Conductance ($\gamma$):** Another thing that could change (which here I don't remember anymore in this review... I believe it is, I don't remember where I took this graph from, I don't remember who is who, what the passage was, it was very elegant), you can also have a change not of the number of receptors, not of the number of vesicles, but of the single channel conductance, due to what is for example called **phosphorylation**. So some persistent structural change of the receptors that causes the pore to be somehow wider.

In all these cases this change has a counterpart of a persistent change that can somehow be considered a **substrate of memory**, a substrate of something that accounts for the fact that an animal (us) can memorize things and keep them for even 80 years. 90 years, less, depending on when one kicks the bucket.

## Hebb's Law: Causality and Correlation

A very important thing in the context, let's say, and in the end if you want a kind of guiding principle that came out, is that of the proposal, of the theory of **Donald Hebb**, at the end of the 40s (or 50s). He was a psychologist, but he had a clear understanding of the physiology of the nervous system, which back then at the time was not complete (it is not even complete today, but it was not as complete as now).

And Donald Hebb is remembered for a principle that in the end is distilled in this way (partly horrible, because it does not account for what could have been both his intuition and what was then the instance of things). He was dealing exactly with, he had in mind speaking of concepts, of memory, of cognition; he had an idea that there must be a neuronal correlate, and somehow he concluded: **"Cells that fire together, wire together"**. This doesn't work in Italian, that is "se sparano assieme, si... spagan... spaghettano", it doesn't come out in Italian. That is: **if you fire together, you connect together**.

And that speech I made before: it could be that there are mechanisms whereby if two neurons (which on their own are not even connected, or are connected but not necessarily the synaptic efficacy of one neuron is such as to recruit or influence, exciting and inhibiting it, the other neuron)... If by chance there is some temporal dynamic where there is a correlation, a co-activation (maybe in a certain temporal relationship, because if I activate now and another activates half an hour later, perhaps the causal link in our universe might not be useful to remember), and this could be a way to create memories by noting co-activations, correlations that could become, as said, **causation**.

He did not necessarily speak of concepts, despite being a cognitive psychologist, but he was literally linked [to biology]: he spoke of the axon of cell A. When this axon is close enough to the threshold to excite neuron B and repeatedly, persistently is involved in the process that leads cell B to fire, some growth process must happen. What I showed you before: for example an insertion of new post-synaptic receptors (in the end they are proteins, they are linked to expression: genes are expressed, proteins are trafficked, *trafficked*, they are brought to the right place). Or it has something to do with a persistent change, another type of mechanism of growth process of the number of vesicles. Or even a metabolic effect, a change, such as can be the phosphorylation of a receptor, of a membrane channel that changes its conductance, increases it, decreases it.

### Discrete States and AI Analogy

The interesting thing is: but if it changes with this story of phosphorylation? Does it change gradually? I am thinking again of **machine learning** where you have synaptic weights that in theory (maybe those of you who are more competent will know that it is not so), in theory are real numbers, they can change continuously. Now I potentiate you a little bit, I potentiate you even more. The story of phosphorylation could be something binary: either the synapse is not potentiated or it is potentiated. Or regarding post-synaptic receptors, in the end I can integrate an integer number of them, so from the point of view of efficacy a continuity might not hold.

Perhaps you know that with **Large Language Models** there is a memory problem. If you tried to play by downloading some weights, you download hundreds of gigabytes onto your computer and the fact of changing their numerical resolution is being explored. They are numbers normally represented with floating point at 64 bits (or 32). What happens if instead of having that type of numerical resolution that allows me to represent the value 51.68462... ok, integers? 51 or 50, you don't have the intermediate value, deal with it. It seems that it is possible. And it could be exactly what happens in the nervous system, where synapses in these persistent changes it's not that "I potentiate you synapse, I potentiate you in an analog way"; it may be that there are **discrete** states.

## Memory and Erasure (Kung Fu Upload?)

Anyway, whatever it is, this guy got very close. To this day we don't know exactly... we don't know for example how to alter memories. Some time ago (and I believe I have to give a talk for the general public in a few... maybe next week) I was asked: "Could we then learn new concepts like **Matrix** when they do the Kung Fu upload?". It is a fundamental problem that the representation of memories or representation of information is distributed: I don't have *here* the day of my graduation, *here* the marriage and *here* I don't have it. It is all distributed. I said this because we don't know how to create new synthetic memories or erase other memories.

Having said that there are (and if you want I can give you references) several scientific articles where, in the particular case of the limbic system (hippocampus and particularly amygdala), a rat, a mouse that has been exposed to what is called **fear conditioning** (Pavlovian type conditioning of fear)... obviously this has a very notable impact for PTSD syndrome, post traumatic stress, *whatever*, so anxiety disorders, panic and traumas. The possibility of erasing, of mitigating the activation, the co-activation of a particular environment (I get panic in a particular situation, I am in that situation and I suppress the amygdala) and in that way I tend to disfavor, or I tend to do *reversing* of what was instead a consolidation of a plasticity. We are very far from doing the Kung Fu upload, perhaps less so from curing anxiety disorders in that way.

## Lmo's Experiment (1973): Tetanus in the Hippocampus

In the 70s there was a key experiment (I don't know exactly if it was by chance or not). Norwegian researchers (**Bliss and Lmo**) took slices of cortex... if I remember correctly, they did it also on the intact animal, a rabbit, but typically they took a slice of **hippocampus**. And in a particular combination of the hippocampus you have basically a circuit that is **feed-forward**: you have fibers projecting to a population of post-synaptic neurons and there is no particular recurrence, reverberation, etc.

So if you place an ugly electrode (not a pipette: two twisted wires, two microscopic twisted wires, but they are still two wires) and you apply (here there is a kind of symbol of a coil, because in fact it is what is called inductive coupling, it is a galvanic stimulator where there is no continuous connection, because it would bring in noise; there is no continuous connection between the signal source and the electrode, there is a transformer in between, this decouples the DC for those who know what I'm talking about) and you give, you start giving a stimulus. In fact you are activating, and these guys did it at high frequency.

Normally if you activate it only once (again this is not a pipette in the stomach with those very fine experiments of pairs, of *pairs*, of couples that I showed you; here it is: I place an electrode that records from a **population** of neurons; today we do it in the belly of the neuron, it is also done outside, it would be like a microphone placed here and if you activate yourselves the microphone hears the integrated, summed version of all your voices). So if I with this "flick", a small electroshock, start doing it once a second, I measure in your population reaction the effect of this electrical stimulus that generates activation of synapses and this activation of synapses in some cases also generates spikes, but predominantly I manage to read from the extracellular point of view the so-called **Local Field Potentials**, which are an integrated synaptic activity.

And so I see that for 15 minutes, every second, every two seconds, I give a flick and I see an amplitude. Here it is normalized, it is millivolts or whatever it is. I normalize, I see that it is always the same. Then at a certain point, at time zero, I start giving a stimulus always exactly with the same channel. This for me was a bit complicated immediately to realize, but what does it mean? So it is something homosynaptic, because if before I activated the synapses with the same channel, now I start activating at high frequency: I am discharging maybe a facilitation or a depression, but it is certainly short-term stuff. Due to the particular anatomy of the hippocampus however, by stimulating these fibers, one activates both some neurons and other post-synaptic neurons and creates temporal correlations.

What is seen is that instantaneously after giving this **High Frequency Stimulation** (this high frequency stimulus, is also called **tetanus**; I believe it is called tetanus because tetanus has to do with the tetanus disease... now I don't remember the nervous disorder anymore, maybe you can help me, it should be some tonic activation. I don't remember where tetanus comes from, look it up on Wikipedia so you can know). So after that I resume giving a stimulus to read the system (here in the end it is always: I write and I read, so stimulus and I see, stimulus and I see). But the tetanus here was a high frequency blow and then afterwards I resume at one every second, one every two seconds, every ten seconds.

### Post-Tetanic Potentiation (PTP) vs Long Term Potentiation (LTP)

What I see is that instantaneously there is a kind of... what you would call facilitation. It says: "Ok thanks, here the synapses have facilitated and you with this reading pulse are continuing to see the effect of this accumulated calcium". And you wouldn't be too far off, it is called in jargon **post-tetanic potentiation** (*Post-Tetanic Potentiation*). So this disappears very quickly, over the course of minutes or tens or hundreds of seconds.

The interesting thing is that this potentiation, this facilitation, **persists**. And it persists for hours. Now here it is up to 60 minutes, so about an hour later (in fact you could object: "I want to see that after three hours it is still here"). And significantly, the amplitude of the synaptic responses is significantly greater than the *baseline*, than what it was before starting. So in a causative way I wrote into a piece of brain and this information persists.

This thing here was also done in vivo and it was the first, in the 70s, the first demonstration that there is an activity-dependent plasticity (because here I am the one who induced activity) and a mnemonic change is maintained... probably not structured... pardon, not mnemonic, a persistent physiological change of a physiological observable is maintained, which could be correlated to a type of memory.

If I tell you a phone number, you are not reverberating it in your head, but simply after a while you know it: probably an **LTP** (*Long Term Potentiation*) has been created in your hippocampus. There are mechanisms, now we know, of consolidation that also involve the cortex (and parenthetically *slow wave sleep*, sleep, and the activity of information exchange between hippocampus and cortex during sleep phases, during some sleep episodes, is precisely to consolidate and transfer to even longer-term memory the episodic memory of which the hippocampus seems to be the seat).

## Spike-Timing Dependent Plasticity (STDP)

So this plasticity was found everywhere. And in more recent years (and again it involved this researcher **Henry Markram** and also a Chinese researcher who had been in the United States for years at Berkeley, **Mu-ming Poo**), they are more or less at the same time, they published articles, one in *Science* and one in *Nature*, again resuming the story of synaptic plasticity, but in an extremely finer way. They used pairs of electrodes. So in the best way, so obviously only few musicians, experts, talented people also from the motor point of view, also of patience, of aptitude for experiments (which is not trivial, you can compare it with the aptitude for cooking: maybe some of you will have the recipe, but maybe they are poor, or vice versa they excel, they are intuitive, maybe they add salt... and these maybe placed the pipettes in a way: "In my opinion I won't take this cell, I'll take the one next to it because it seems more vital". How to know? And ok, they are Henry Markram).

And they found that this writing characteristic depends on **temporal information**, temporal information on a time window of milliseconds (now I will argue better and tell you what it is). And it had been partly anticipated by a theorist, by a physicist, who said: "In my opinion synaptic plasticity, also revealed in this hippocampal LTP with tetanus, must have some kind of time-based correlate, because otherwise I don't explain why I can learn motor sequences, sensory sequences, why I manage to learn a language, why I manage to learn to listen to music and interpret it, when the information is temporal".

And this is called **STDP**, **Spike-Timing Dependent Plasticity**, and it is both a potentiation and a depression, and now I will show you why. It has been found in many, so in the neocortex, in the hippocampus, in the olfactory bulb and in other parts, and surprisingly it depends on the **causal relationship** between presynaptic and postsynaptic activity.

Again, I am a pyramidal neuron in layer 5, I am excitatory, I release glutamate, I have my synapse projected there (it is a heterosynaptic plasticity, so what the receiver is doing also counts, regardless of whether he has his own axon that maybe returns to me, it doesn't matter, it is only the synapse, my synaptic bouton is here) and it depends, in a way that I hint at the possible biophysical correlates, it depends on the precise timing of when my spike was emitted (note: the spike was emitted but it takes some fraction of a millisecond to propagate to the synaptic bouton, but close parenthesis). The exact *timing* of my spike and the exact *timing* of the other spike, which again can be caused by me (because I have a nice effective synapse that is every time I fire he also fires) or he fires on his own.

### Causality (Pre-before-Post) and Anti-Causality (Post-before-Pre)

1.  **Potentiation (LTP):** If these two spikes are close, and they are close not... ok, I said it before, it's like NMDA receptors are coincidences, no, they must be close in a particular temporal causality relationship. **Pre first and then post**: then the synapse potentiates. I don't know if it potentiates on the presynaptic or postsynaptic side, but it potentiates.
2.  **Depression (LTD):** If instead the **post-synaptic spike arrives first and then the presynaptic one arrives**, there is a kind of anti-causality relationship. Because anatomically if one looked at the circuit, if Ramn y Cajal saw the circuit, he would say: "Information travels in one direction, so when the presynaptic activates, maybe the postsynaptic activates later". If instead it activates before, the synapse depresses.

As if there were a recipe to say: causality (causality which means cause-effect, the cause must precede the effect) is intrinsically hinged in biology. Which is cool.

And this is a graph taken from Mu-ming Poo's article in '98 (I met him, I was about to go do a post-doc with him at Berkeley, but... life's mistakes, several life's mistakes). What they did is: they had two pipettes, one in the soma of the presynaptic neuron and one in the soma of the postsynaptic neuron, and they gave flicks to make the neuron fire. So in the end they had two ways of using the system. Giving a flick in the presynaptic neuron, they could see what the echo was in the postsynaptic neuron, they measured the EPSP (excitatory post-synaptic potential). But they also had the electrode there, therefore they used it to read. So in a plasticity induction phase, it is said, they could inject a very intense current in both the pre and the post and they could do it at will. They could go "tac-tac" or vice versa go "tac-tac" with a temporal resolution of several tens or hundreds of milliseconds.

That is, they really buckled down on many experiments (not just one experiment: each dot, if I remember correctly, is a pair, is a separate experiment) and for that synapse, between two connected neurons, so quite complicated, they did that same particular temporal relationship. Pre before post, which is here, or post before pre (you see that post precedes pre) and they did it with "wait, do 200 milliseconds before, do 150 milliseconds before".

The cool thing is that you have an inversion around zero with a... let's say, these curves you see, these solid lines are in fact an exponential fit (it doesn't matter that it is exponential because in this case here it is not a time variable, it is an interval variable). If the interval is somehow negative, you have an **LTD**, a depression. Here is the change that was expressed after having done the induction many times pre-post, pre-post or post-pre, post-pre, it determined in normalized terms a subsequent expression of the EPSP that was smaller, at 80%, 70%, even at 40%. While simply changing the sign of this interval (so instead of post-pre, pre-post), it was instead a potentiation. Again, the interesting thing is that across zero, within very few milliseconds, you have a huge LTP or a huge LTD.

### Safety Mechanism (LTD > LTP)

The thing you might notice is that there seems to be more LTD than LTP. Again this is a thing perhaps linked to evolution, it is a kind of safety mechanism. I don't want that for some reason neurons fire trains of spikes pre-post and that it prevails in a completely random way. If you imagine making neurons fire in a completely random way, you can imagine that it is... it is equiprobable that it is pre-post or post-pre. So in that case there it could be: "Look, but if it goes wrong for you, potentiation wins; and if there is more potentiation there are probably more spikes, and if there are more spikes there is more potentiation and it ends up in a seizure".

So the fact that here the area subtended by these points in the depression side curve is wider is perhaps a safety mechanism to say: "Fire even randomly; I tell you that on average I make you turn off synapses a little bit, I don't make you potentiate them at will, so that there is a kind of perhaps homeostatic control that is rather safe".


## STDP Variability: Neuromodulation and Development

So the surprise is that there is a causal relationship and that it is timing, and that LTP and LTD are two sides of the same coin: so very little changes in the induction of plasticity.

There are over the years (this is a *review* article from a few years ago where it is called "canonical"), in other parts of the nervous system it is found that these curves, this dependence on timing (again this horizontal axis is always the pre-post interval: so if pre-post is positive it means that the pre... sorry, if it is post-pre, because post-pre if the time of the post is greater than the time of the pre, so it is subsequent, then I have potentiation; if instead it is negative I have depression)... However, if studies used **neuromodulation**, they simply poured dopamine or dopamine receptor antagonists, the type of profile completely fell apart: it became wider and it doesn't seem to have the LTD component anymore. Now I don't have the slide, but I don't remember if it is in the cerebellum, there is an STDP in reverse: that is, in this part where LTP should be there is LTD and in the left part there is LTP. So there is a zoo here too.

And an interesting thing is that during the developmental phases, it is not that this curve appears when the first neurons form during embryonic development. At the beginning (also these P13, P21 stands for days after birth, *post-natal days*; we are talking about a rat)... typically rats open their eyes and thus reach maturity of the visual system around P14, two weeks after birth. P13 and P21 are what are called very young rats, 21-30 are *juvenile*, 35-42 are also to do with the onset of puberty and therefore the hormonal effect. Again neurotransmitters, neuromodulation: the profile changes from being predominantly LTD to becoming LTP, even in a way independent of timing. So it seems that there is something, there is something very interesting.

### The Arrow of Time and the Eligibility Trace

And so synaptic plasticity is not simply the consequence of a co-activation. It depends how, it depends on the direction of causality. Years ago I read a book titled *"The Arrow of Time"*, written by physicists; it had to do with the irreversibility of the laws of physics. I'm not saying there is food for thought, but the fact that there is a direction of time here would seem to be captured by a biological mechanism.

So how does it work? It works that if you have two neurons (this is presynaptic and this is post-synaptic), if there is a *firing* activity of this type, somehow the synapse between the two must be able to read. It reads some information perhaps linked to residual calcium, perhaps linked in part to the coincidence between action potentials (which, now in this case there is no morphological structure, but from the Soma action potentials *back-propagate*, retro-propagate into the dendrites where the synapses are). But, however it does it, the synapse somehow reads that "pre before post" must be a potentiation.

So it is as if every presynaptic spike had a kind of temporal trace which is called **eligibility trace** (*traccia di eleggibilit*) which at a certain point loses memory. This is truly a hypothesis: it is a decreasing exponential, whose time constant here is the same time constant of this curve (of this curve here). So this arc of exponential changes because it is a dependence on the interval. Same thing if I take it and assume that there must be some biophysical mechanism: it is as if the synapse, once the post-synaptic spike arrived (so this neuron here), the synapse manages to read and says: "Ok, but was it still in time for this eligibility window?". If yes, then potentiate yourself (in fact the arrow should become chubby).

If instead the causality relationship is inverted, the eligibility trace must perhaps be associated instead with the post-synaptic neuron. And when the pre-synaptic spike arrives at the synapse, it looks at the eligibility trace that the post-synaptic neuron has and sees that yes, they are still legible, but it is another dynamic, another technique. So in this case depression occurs and the arrow becomes thinner, more slender, because the efficacy of the synapse (for example $\bar{G}$, or the release probability, or the phosphorylation of receptors, I don't care, one of those phenomena) is activated or depressed or potentiated.

## Beyond Pairs: The Triplet Rule

So the interactions are for pairs of spikes. If I ask you when (and this is the most appropriate situation) when neurons have a *spike train*, a train of spikes of this type, I just by eye cannot understand who is pre and who happens first. Well, I know who is presynaptic and who is post-synaptic, this yes. The blue one is always presynaptic (anatomically it has a synaptic bouton projected and touches the post-synaptic neuron, so there is no escaping this). What I don't know is if, for example, this spike here, the third one, happens before or after... do I have to consider it as having happened before or after these spikes of the presynaptic neuron.

So, in this case, people doing experiments (doing the experiment) saw that, for example, in this circumstance, the synapse potentiates. And obviously the question is: how come? Before it seemed to be an interaction of pairs of action potentials and now one of the proposals (I simply tell it to you like this, briefly, without going into detail) hypothesized that actually the synapse is not based only on two spikes and on two eligibility traces, but counts **triplets**.

So if you think that every post-synaptic spike somehow interacts with a pair of presynaptic spikes (and you represent this sentence mathematically and put it inside a simulation), you would manage to express, to explain the experiment.

### Dependence on Firing Rate

The snag is actually resolved because things do not stand so simply on an aspect of only temporal causality. There is also an influence of the **firing rate**. Now I would like to get there. So, when you are in this type of regime, instead of in a type of regime where the frequency is much lower... and in a regime where the frequency is much lower, probably the temporal information makes sense, because there is sufficient silence to say: "This arrived before and this arrived after, and this is in particular referred to this, because it is too distant from this other spike". If instead the frequency is higher, it is not.

This mechanism was (so either timing or *firing rate*) was again expressed not in words, but in a mathematical description by two researchers, **Pfister and Gerstner**, years ago. And they showed (this is a graph that I plotted some time ago) that the time dependence... the two curves are obtained by keeping the pre-post interval fixed (I keep this always fixed, suppose a millisecond, or post precedes pre). So here there is temporal information. And at low frequency (at low frequency means that I do this pre-post event, I do it rarely)... but I can do it also at higher speed, maintaining the same phase between the two stimuli.

If I do it and go slow, around a few pairs of spikes per second, the black and the dashed remain different. One is potentiation and the other is depression. The interesting thing is that at a certain point, at frequencies not particularly so high (around 30-40 spikes per second), what was supposed to be an LTP starts to experience a dependence on frequency; so at the *pairing* frequency, LTP also starts to experience a dependence on the *pairing* frequency. But in the end, exceeded a certain critical frequency, **everything makes LTP**.

So, in some cases, if synapses are of this type, when neurons fire (for example in the visual cortex), if the firing activity is very high, you have that both from the presynaptic point of view and from the post-synaptic point of view, I always have a potentiation. So if by chance anatomically there are both synapses in both directions, both potentiate and you have a bidirectional *motif*. Vice versa, in other areas of the cortex where the code is said to be sparser (in other words neurons fire at much lower frequencies), you continue to work in a regime where timing counts.

This thing here, which is called **triplet rule**, has partly reconciled (and there is still a piece I won't tell you about, which further reconciles with a lot of experiments) where people started to say: "But wait, timing is not enough for me, it must be frequency too... but how?". Simply considering triplets captures both timing and frequency in a way that *fits* the experiments. So it is a very powerful description. Again, here is a case (of which however we do not go into mathematical detail) where having systematized, formalized, modeled the phenomenon, allowed to say: "Look, but it doesn't work... however if I add this it starts to work and I see and explain the experiments". Then with the same model I can say: "Wait, I have never tried it with that other stimulation protocol, for example Norwegian, with high frequency tetanus... let me try". It turned out that it *fits* and works even in a context that was never the one on which the parameters of this model were identified.

## Interaction between LTP and Short-Term Plasticity (STD)

So, now, the interactions between long-term plasticity and short-term plasticity are particularly interesting because so far, let's say in the last hour, focusing on long-term plasticity, basically I told you: "Dunno, this changed, change this... $\bar{G}$ (G-bar) changed". So, whatever the train of presynaptic action potentials and the temporal dynamics, anyway changing this (halving it, doubling it), I change the amplitude of all the peaks of a spike train, of a PSP train. I don't change the temporal structure, I don't change how if it gets tired, if it facilitates: I am scaling. This is literally a scale factor.

So if this tells me: "Ok look you inserted another 100 post-synaptic receptors in that synapse that had to be potentiated", so I changed, or vice versa the conductance of the single receptor increased because there was phosphorylation, I change here and I change a scale factor.

It is different if I change the maximum concentration of neurotransmitters, actually if I change **$U$** (the release probability). Because you remember that $U$ determined how much I subtracted during a spike from this container $R$ of vesicles ready to be released, of neurotransmitter in a kind of state ready for action. And if $U$ changes for me, and I have a train of 10 spikes... while before it was small (yes, between the first... depending on the frequency obviously, the depression would have been a little bit gentler, almost interceptible), if instead $U$ becomes ten times as much for me, at the first use I almost have no more neurotransmitter available. So it changes the temporal profile completely for me, altering what is, again, in *machine learning*, not a change of scale, it is a change even of *processing* over time.

### Dean Buonomano's Experiments: Hippocampus vs Neocortex

These are experiments done by an American researcher, who is a great one, he also wrote a very interesting popular science book on the representation of time in the brain (I don't remember what it's called), and he is not Italian, he is of Portuguese origins: **Dean Buonomano**. I met him several times and a couple of times I said: "But you have...". No, he is not Italian.

He did that type of *paired recordings* experiments, and he did it both on hippocampus slices, and on cortex slices (so in the end it is always on rat, but on different parts of the cortex), and he applied a particular protocol, the same plasticity induction protocol. And what he saw is that, comparing what was obtained before induction and after induction (before is this dashed trace and after the trace [solid]), it differed notably if it was in the hippocampus or if it was in the neocortex.

1.  **In the Hippocampus:** If you manage to see (unfortunately it should have been in color), you see that basically from the dashed part to the darker part there is a scale factor. I doubled everything. So the amplitude of this event here, which was smaller than the second, anyway is doubled; and the amplitude of the second, which was a bit more [large], is doubled. So in the end all these IPSPs (which are a consequence of the response of a train of 10 potentials... 3, 6, 9, whatever they are) changed uniformly over time. For example, if I were able to potentiate sufficiently, enough to, at a certain point, make the neuron fire, I would have that before this train, this echo of IPSP is not able to make the neuron fire (you see it doesn't go above threshold). Afterward however it may be that it goes above threshold. So here there is a small **temporal summation**: to make a long story short I increase, induce potentiation, not only does the synapse potentiate, it potentiates in a temporally indiscriminate way, but it makes me fire in correspondence with the presynaptic spikes towards the end of the spike train.
2.  **In the Cortex:** If I do the same thing in cortex, where there is this much more pronounced short-term depression, apparently I don't change $\bar{G}$, in cortex I change **$U$**. And changing $U$ you see that instead of scaling everyone in the same way, I scaled the first one: the first one became a nice quantity, but the second in proportion (given that there were no more resources or there were much fewer available) did not increase by the same quantity, in fact it is indistinguishable from what it was before, and so on. So what before was a train where there were no depression effects, now there are: given that the first one was a nice blow, a notable subtraction of quantity of neurotransmitter, the amplitude of the subsequent IPSCs is actually smaller. And if by chance I were in a regime such that these amplitudes were sufficient to make the post-synaptic neuron fire, I would see that the first, maybe the second presynaptic spike, would give rise to an excitation to make the post-synaptic neuron fire.

This and this are very different. In terms of signal coding, of some information coded in time, here perhaps I am saying: "I want to increase the gain of all temporal information". Here I mean, perhaps: "I want to turn the system into a system that signals only the change, only in the transition". The transition is when I turn on and then I want the system to forget, to get tired, to empty itself.

## Redistribution of Synaptic Efficacy

These are other examples (pardon, the one before was a cartoon), these are examples where this (and we are finished, and I leave you) where in the hippocampus there is this scaling that is revealed before and after normalizing. So what you see here, *scaled* and *subtracted*, are versions where I basically demonstrate that if I multiply or divide by a certain quantity two trains, the two traces become equal. This doesn't happen, doesn't work when I do the same scaling, the same normalization [in the cortex], why? Because the synaptic efficacy of the single ones changed.

This phenomenon here was (and I say goodbye) was for the first time described by **Markram**, who did not call it synaptic potentiation due to the interaction with depression: he called it **redistribution of efficacy**. Because in his mind he saw that before the efficacy was on the first one and now, changing, it is as if this efficacy distributed over this train of events redistributed itself, shifted. He had imagined a kind of conservation of amplitudes and simply a redistribution: now you don't have it here anymore but you have it in the middle (or at the beginning). And it has a lot of notable implications.

See you next week! Thank you.