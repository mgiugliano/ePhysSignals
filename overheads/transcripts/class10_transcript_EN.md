## Introduction: Analysis of the Cable Equation

The underlying idea is to approach the problem of understanding the origin of extracellular signals. And the only way to do this, as I tried to tell you intuitively last time, is to have the distribution in space, as well as in time, of the transmembrane potential.  In fact, one also gets the transmembrane current "for free," which, however, changes from point to point depending on the morphology of the neuron, depending on the distribution of membrane channels in the neuron's morphology, and depending on the state of the neuron.

To characterize this, which is a fundamental ingredient that tells us how signal characterization works from an extracellular point of view, we must face this apparent "monster": the **Cable Equation**. This cable equation is a partial differential equation, differential with partial derivatives, and it has two fundamental characteristics, two parameters that depend on the geometry and the biophysical parameters: one is the **space constant** ($\lambda$), and one is the **time constant** ($\tau$). 
In this case, which I have rewritten on the blackboard, I am also considering at a generic point — so effectively here I should write $x$ and $t$ — the additional contribution of a synaptic current and an external current, for example, injected by an experimenter, as we did last time and as we are doing again in this case today. It goes without saying that if there is no synapse at that point, this value here is zero, and therefore this term is not present. If there is no one with a pipette, or with an electrode, or with some opsin activated by light, or with some other "contraption" injecting an external current at that same point, this term is not present at that point.

This is a partial differential equation and it is a "tough nut to crack" from an analytical point of view, and therefore the first way we started to approach it to try to extract some useful information is to study its **steady state**. When one studies the steady state, it means that one looks for solutions in which the dependence on time is no longer there; therefore, instead of the solution being a function of space, of the point in the cable, and of time, in fact, this becomes solely a function of the point because in time it is constant. If this is so, the derivative with respect to time is zero and that transforms into an ordinary differential equation. Okay, it is of the second order, however, it is always linear; it should be something that you have seen and should "swallow easily" based on your mathematical reminiscences.

What we did last time was to consider the solution in the infinite case. One of you basically told me at the end of the last lecture: given that the degree of derivation in space is two and here in time it is one, well, two initial conditions are needed in space — which are called boundary conditions — and one initial condition. I made you a list of all the possible boundary conditions that can occur; it is not that there are always two. In this case, in the case we saw last time, the cable was infinite, so in that case, it was that problem, that geometry, that generated a particular boundary condition for me.

In the case we are doing today, in which the cable is not semi-infinite but is finite on both sides, I think there is a condition, I assume that given the geometry there is some boundary condition here at the extremity where, for example, the current is zero here, because there is an interruption of the cable. So the whole phenomenology of boundary conditions is a catalog from which to choose from time to time, just as the initial condition is what it is; it is not that *one* initial condition exists. The initial condition is somehow, even if we do not consider the time-variant solution for the moment, it could be that at a certain instant $t_0$ for all values of $x$, this is some function of the point. For example, it could be $-65 \text{ mV}$ for every $x$, so it could be that the cable is at steady state and at rest along its entire geometry, and this would not be an unrealistic condition; it would be a frequent case.

What I wanted to tell you is that there are a couple of ways to try to examine these equations simply. The first is to remove the time derivative, the derivative with respect to time, thus studying the **steady state**. Another one is — and we will see it later — to approach the so-called **sinusoidal steady-state regime** (permanent sinusoidal regime), in which by means of transform operations in the frequency domain (whichever you like best, Laplace or Fourier; we will do it with Fourier in about half an hour), in fact, again the derivative in time disappears. It disappears because in the transformed domain, temporal derivatives become algebraic operations, they become multiplications by — particularly in Fourier — $j\omega$, so even there in that regime, I lead myself back to the case where I know how to solve it.

I will show you the equation of the transient in which I do not make any, let's say, there is no simplification, there is no hypothesis and the solution changes both in space and in time, but I will give it to you "from above," we will not derive it.

The first thing we do compared to last time, of which I remind you the result was this, is that in the case of a semi-infinite cable — so here at one extremity there is an end and it has as an initial condition me with a pipette, with an ideal current generator, injecting a given current into it — as it was from a certain point of view trivial to expect, if I look at this semi-infinite cable, which therefore extends to infinity over here, at this point here I see a resistor. This should not surprise those of you who have a reminiscence about so-called transmission lines, or electromagnetism, or coaxial cables: in the end, when one measures the impedance at a point one measures $50 \Omega$. Here at $x=0$ I measure, if I inject $I$, I measure $V = R_{\infty} \cdot I$, a particular $R_{\infty}$ which is an expression that depends on the parameters. 
The obviously interesting thing is what happens when I consider not $V$ at this point, so at the access port, but along the entire geometry of the cable. We saw that this simple expression of a decaying exponential, where the scale of this *decaying*, of this exponential decay, is in space; it is not in time as we have been accustomed to looking at until now with the solution of the usual differential equation, etc. Here it is an exponential decay dictated by the space constant, $\lambda$, and in fact, it is saying that even normalizing for example to $100\%$ what $V$ is for $x=0$, let's say it decreases very rapidly. Again, it is on the scale of $\lambda$, so it depends on how much $\lambda$ is with respect to the length to say whether it is rapid in space or not, but in fact, it indicates that a morphologically extended structure like a dendrite attenuates. And it was taken for granted in the end that it was so, just like a distributed multi-compartmental dissipative system, made of dissipative elements. There are no transistors, there are no current generators, here it is stuff that is a combination of resistors and capacitors.

In the **steady state** case, the capacitors are as if they are no longer there, actually, they are no longer there, and the effect that remains is that of the behavior of many resistors in series and in parallel; what I mean is that they are transmembrane and axial.

What happens when the cable is not infinite, semi-infinite, but is short, has a length $L$, so at a certain point here it ends with its boundary condition? The graph changes considerably compared to this. This graph is "stretched" graphically because I took it from another book but I wanted to put it exactly in the... since the x-axis is represented with a variable normalized to $\lambda$, such that it can apply for any choice of $\lambda$, I wanted to "stretch" it so I could compare it. And what you see for the semi-infinite cable is this curve marked by $L = \infty$, and you see how different it is compared to $L$ being half a space constant, one space constant, one and a half space constants, etc. 
When the cable is already two space constants long, it means quite long. Long, short, and based on the... you notice it if compared with the infinite case. If it is infinite it means it is very long; very long means in the case of a finite cable that its length is large compared to the space constant. So if there is something either in the geometry, in the diameter — I don't remember wrongly, in here there is the diameter, it varies with the square root because that is $\lambda^2$, but $\lambda^2$ is proportional to the diameter, pardon, to the radius it is inversely proportional, directly proportional, it has axial and transmembrane resistivities — but for example to the radius, the geometry... so if a little piece of the dendrite of a neuron becomes particularly thin, its space constant changes and a piece could become short or long. Short or long, in the sense that if it is short basically it is as if there is no particular, dramatic attenuation effect; $0.5\lambda$ is there anyway, but it is very different than at the same distance in the infinite case, in the case of $2\lambda$. If conversely $\lambda$ is very small, that... that could become long... so the value of the space constant determines for me if a piece of dendrite is long or short.

Why does it have this shape? How does one find the solution? Is it particularly difficult? No. It is slightly annoying and boring because to avoid doing intricate calculations — intricate calculations mean boring from an algebraic point of view, not complicated substantially — there is a choice that now bothers you.

So, the choice is the following: this is the solution, I am therefore in the same case where I do not have this term, here I am using the change of variable name, so I am using small $v$ which is not... let's say it is offset, it is referred to $E$, the resting potential. This part is not there and it is the solution of a second-order differential equation with constant coefficients, whose solutions are, apart from the constant term to be identified, the two solutions of the characteristic polynomial of that equation, that is, which is obtained from that, from comparing that differential equation to an algebraic equation.

However, I start from here, only that instead of writing $K_1$ and $K_2$, I write them in a different way. I repeat: by doing so you have no way to understand the reason for this except at a certain point to say "oh well, okay, it is equivalent", it is only to simplify the calculations. I write it like this, so $K_1$ and $K_2$ are two terms, now, completely arbitrary, they are two degrees of freedom. I can always define two other values, two numbers, $\hat{A}$ and $\hat{B}$ — they are others — and I define $K_1$ as the semi-sum and $K_2$ as the semi-difference.

You might say: "Oh well, do it if you must, but I don't understand why. Couldn't you stay in this world?" Yes. I hope you agree that if I instead stay in this, let's say, I come into this completely equivalent and arbitrary description... because I can always find, if you tell me "look, however $K_1$ and $K_2$ have these values", fine, I can write $K_1 = K_1$, I can write a system of two algebraic equations in two unknowns, the linear system certainly has a solution, so I can find $\hat{A}$ and $\hat{B}$ that make these two things exactly identical.

I do it because in the finite case, what comes out in the solution are not only decaying exponentials but their sum and difference. Why didn't I, for example, see them so much in the courses I took of analysis 1, 2 etc... the hyperbolic sine and cosine functions come out, which seem to instill particular fear, but I will show you why they are called hyperbolic sine and cosine.

## Expression of the solution with hyperbolic trigonometric functions

So, if I write this thing in this way, as $\hat{A}$ plus... again in a completely arbitrary way, I can note that $\hat{A}$ appears here, appears here, $\hat{B}$ appears in both the first and the second term and so I can factorize them. Because I like to see that here there is $e^x + e^{-x}$ (beyond the $\lambda$) and here is $e^x - e^{-x}$, and then there is a divided by 2.

Have you ever seen these differences of these exponentials? They are customary because mathematicians define, by pure convention — but now you will understand why — the **hyperbolic sine** as the function given by:
$$\sinh(x) = \frac{e^x - e^{-x}}{2}$$
I always remember that the sine is a little more annoying than the cosine, it is "meaner" (*più cattivo*). When you derive it no... when you derive it the derivative of sine is cosine, but sine I don't know, I dislike it; it is an odd function, it is not even, so for some reason, I associate that there is the minus here. The exponential with the plus and the exponential with the minus are always there; the "divided by two" is there because otherwise it would be too simple to remember if it were normal, and so there is this annoying minus. So this is the definition of the hyperbolic sine. 
The **hyperbolic cosine** is a bit "gentler" (*più gentile*). Again, the cosine is even, it is an even function, and so the only difference is that here there is the plus instead of the minus:
$$\cosh(x) = \frac{e^x + e^{-x}}{2}$$
And they are called hyperbolic functions, hyperbolic trigonometric functions, because if you try to derive, just like trigonometric functions, if you take the derivative of the hyperbolic sine, you obtain something that is a relative of the hyperbolic cosine.

It doesn't take a scientist because if you derive, apart from one half which is a constant, this exponential remains itself and this other one gets the plus. The derivative of the cosine is not minus sine, as it would be in the case of trigonometric functions; the derivative of hyperbolic cosine is hyperbolic sine. But the fact that they are one... not the dual of the other, but they are one that is obtained by deriving the other somehow, also for reasons that I don't know, of analytical nature, of mathematical nature, they have been called hyperbolic sine and cosine.

They are clearly tabulated, so people in the decades or 50 years ago, 100 years ago found them tabulated, so they didn't necessarily need to have a numerical resolution that insisted on exponentials. Today it is not particularly problematic, even if you know that a computer does not actually calculate the exponential; it does it through a series expansion, it has a certain precision. And so on. Somehow, having these functions that evidently with this type of solutions of partial differential equations that are omnipresent in physics, biophysics, and mathematics — again: the heat transmission equation, diffusion, wave propagation with some change — evidently people said: "Okay, you know what? I'm tired of writing $e^x$ and $e^{-x}$, I work directly with hyperbolic sine and cosine".

So at this point, I haven't done absolutely anything, I'm not even trying to identify the constants, but I realized that these solutions can also be written like this:
$$V(x) = \hat{A} \cosh(x/\lambda) + \hat{B} \sinh(x/\lambda)$$
And like this, I like it a little more. Or maybe not, I was totally fine here, I liked it just fine here with the exponentials: one pleased me, the other exploded, but I could live with it. But no. No, let's use this form, again because if you don't do it you obtain exactly the same result, but it is a little more laborious and the expressions grow, they become a bit more annoying, they become in the end always sums and subtractions of exponentials.

So having said this I say: okay, this is equivalent, again $K_1$ and $K_2$ arbitrary like $\hat{A}$ and $\hat{B}$ arbitrary.

Given that, as in the semi-infinite case, the boundary condition on the left is that it is me with the pipette injecting a current into it and it had this type of form, that is the derivative of $V$ calculated at $x=0$, the derivative — so it should be the gradient of $V$ — the derivative in space of $V$ at $x=0$ is given, it is assigned. For example, a function that changes arbitrarily in time or a constant function in time, since I am studying constant functions. So I need at a certain point to write, to know how to write the derivative. However, beyond the fact that one sees that hyperbolic sine and cosine are simple to derive, here it is a composite function: inside there is $x/\lambda$ at the exponent. So it is obvious that when I do the derivative, by the chain rule (theorem of derivation of composite functions), I must remember to put $1/\lambda$ both here at hyperbolic sine of $x/\lambda$ — I pre-multiply $1/\lambda$ — and also in the other term, $\hat{B}/\lambda$ for the hyperbolic cosine of $x/\lambda$.

This is the expression of the derivative of $V$ with respect to $x$. I can if I want calculate it at $x=0$. For example, the hyperbolic sine becomes 0, while the hyperbolic cosine becomes 1. Ah, they seem like the normal trigonometric functions too: sine at 0 was 0 while cosine at 0 was 1, so a further reason for similarity.

Starting from this, I try to see if I can identify these $\hat{A}$ and $\hat{B}$ with this boundary condition. The second boundary equation is, as mentioned, that here the *boundary* type is **sealed** (sigillato), the outgoing current is 0; which means that the derivative of $V$ with respect to $x$ for $x=L$ here is 0. Since I have the expression of the derivative, I seem to have two conditions, two constants to identify, I should be "on horseback" (in a good position), I shouldn't have problems.

The first expression links me — since one of the two disappears for $x=0$, this derivative at $x=0$, this disappears — it allows me to identify $\hat{B}$. Now I write it bigger for you, it is not that important, I would, however, like to encourage you to do it once, to do all these calculations once in your life. You would understand that, okay, it is boring, you must in this case remember cosine and hyperbolic sine, which could be useful in the future, but in general it is just the whole point... and this equation is how one solves in the case where one studies the **steady state**.

In the other case, this boundary equation, I have to put $x=L$, and it is what it is, in the sense that it is quite ugly. You can continue to verify that from a dimensional point of view things add up, because here I have $L/\lambda$. $\lambda$ dimensionally has the dimensions of a length, or $L$ has the dimensions of a length, so if they are micrometers, they are micrometers divided by micrometers or whatever, so the argument is dimensionless as it should be, but I don't feel like simplifying it much. Oh well, at most I can write here that $\hat{A}$ is minus this term here that I bring to the right:
$$\hat{A} = -\hat{B} \frac{\sinh(L/\lambda)}{\cosh(L/\lambda)}$$
or similar terms. It is a number; beyond the fact that it is ugly to look at, it is a number because I know $L$, I know $\lambda$, my calculator does hyperbolic sine and cosine, or Python, or Julia, Matlab, whoever for him.

Here I have the expression of $\hat{B}$, which is given by the current I am injecting. I am leaving it indicated because I would like to arrive at an expression like the previous one, previous in which I had that $V(x)$ at steady state was an $R_{\infty} \cdot I_0 \cdot e^{-x/\lambda}$. Here obviously it will be more complicated, it will be in terms of this sine and cosine. In fact, a "mess" like this comes out and even when I change $\hat{B}$ with this expression it doesn't improve particularly. So it doesn't improve particularly, but nevertheless, this quantity here, $R_{\infty}$ divided by... [algebraic expression] and at the denominator it continues to multiply hyperbolic sine of $L$, it is a number. It is a number, so it makes me think that also in this case this quantity that is at the denominator for $I$ has the dimensions of a resistance and so, again from my point of view that I am injecting a current here, I see a resistor. I see a large resistor if I inject a current and I want to measure the voltage at that point, which is the thing that most people do.

Obviously, there is a term that again, I repeat, is a number, but it is a number that depends on $L$, so the longer $L$ is, this denominator changes. And the function, let's say, the functional dependence is given by this hyperbolic sine, which in the end is a relative of those two exponentials. If you plot it, you find this expression, this is the right graph, "stretched", but no longer compressed as it was before.

So in fact if you want, if you think that this whole discourse is particularly annoying or difficult, take Python and try to plot, to use it to plot a function, plotting a function with those values and you would see that by varying the values — now I'll show you a notebook that doesn't do this, it does it for the semi-infinite case — you would see how the solution changes. Again, dendrites are clearly not isopotential structures, that is, that have the same membrane potential along all values of $x$, and the potential, in this case, the potential distribution is induced by a current injection.

Remember that $V$ in this solution, as also in the other one, this is small $v$ and it is obviously to be considered... if you want big $V$, you simply have to use the change of variables: small $v$ equals big $V$ minus $E$, so big $V$ equals small $v$ plus $E$. But it doesn't take a scientist because you can imagine that, since $v$ is referred to 0, which was the resting point that you conventionally called 0, if you referred it to -65 you must add $E$ here, -65. So the potential attenuates. And if by chance this was not my current injection, but there was another neuron establishing a synapse, so a synaptic bouton, in the periphery of a dendrite and the soma of this neuron was for example here... at the periphery, obviously the effect of that synapse would be extremely attenuated.

***

## Steady-state solution of an infinite cable

Let's quickly see another case. I changed the slides yesterday because I think this other way to explain it to you is more approachable, is easier. The solution in the end is this, and it is the only thing you have to notice — and that I forgot, $I_0$ here, darn me, but let it be there compared to the semi-infinite case — this is an **infinite cable**, which means it is semi-infinite in all directions: from here going to the right and from here going to the left. 
So if I, again, am obsessed with this thing whereby, for example, I have a pipette, I inject a current and there I want to see what the membrane potential is (because experimentally people do this), I can think that compared to the previous case which was semi-infinite on the right, here if I inject a current, the current has a way to distribute itself towards the piece on the left and towards the piece on the right. So it is plausible to me that the depolarization at $x=0$, exactly where I am injecting a current, is **half** compared to before. This $R_{\infty}$ is exactly the same $R_{\infty}$ as the semi-infinite case.

The other different thing — apart from me forgetting to write $I_0$ here, otherwise it wouldn't make sense — you see that here there is the absolute value, which means that there is a symmetry, this is an even function. The modulus of $x$ means that when $x$ is positive here it is $e^{-x}$, when $x$ is negative it is $e^{-(-x)}$. It basically means that you have, as intuition could tell you, if in the semi-infinite case on the right you had a decrease proceeding from the current injection point towards the infinite periphery, also on the other side it must be symmetric and mathematically I can express it with this modulus:
$$V(x) = \frac{R_{\infty} I_0}{2} e^{-|x|/\lambda}$$

I'll tell you generally how I do to get, to reason in this case.
The **first boundary condition** or hypothesis I must make is that, as I did in the semi-infinite case, is that $V$ remains finite anyway. Here I have the problem that it must do so on both sides (at positive and negative infinity).
The **second** is that the solution, if I study it separately from here to the right and from here to the left, must be continuous; there is no reason why there should be a discontinuity of the first kind at the point where I inject a current. I will have a value anyway, maybe the derivative can be different, maybe yes, maybe no, but it doesn't matter: there I inject a current, I will have a potential drop, but physically I have no reasons to think there is a discontinuity.

The **third hypothesis**, which is slightly more annoying and maybe in this way is simpler, is a slightly different boundary condition, because it is me here injecting a current into it. If I am studying the **steady state** anyway and I have a point pipette, when I say I am injecting a current at a point, mathematically I could translate it — even if you surely don't like it — with not small $i$ but with big $I$. And now let's see what small $i$ is and what big $I$ is: a current that potentially could change in time but will not change in time, which is given by $I_0$, a constant. Then to say that it is exactly at that point and not elsewhere, I put a **Dirac Delta** ($\delta(x)$). Which means basically that at that point where the Dirac delta is, which means $x=0$, at that value... it's true, infinite, you are right, but things work because it represents the fact that the current is concentrated only at one point.

And the only thing I have to reason about is if dimensionally, if this is, suppose, picoamperes, I have to be careful if here, in effect, when I say "1 times" (which is the area of the Dirac delta), is not by chance the Dirac delta a quantity that has as area something that has... or pardon, the amplitude, doesn't have the dimensions of an inverse of a length.

I remind you quickly that one of the possible definitions of the Dirac delta is with a limit process of a rectangle function. So I call this $x$, I call this big $P$, delta. This is a function of $x$ that is 0 everywhere, except from $-\delta/2$ to $+\delta/2$. I am inventing it, actually I am remembering it, but I remember it because it is easy. And here, this is a definition, that is, I am inventing this thing here. Here I say that the amplitude is $1/\delta$. If I try to calculate the area, it is the area of a rectangle: the base is $\delta$, the height is $1/\delta$, the area is unitary, dimensionless. Note however: the amplitude is not dimensionless, it is $1/\delta$ which is an inverse of a length.
This thing here stays the same (today I have colors, I just trip out with the little colors, but I won't avoid it): if you make the base, change the delta value, make delta smaller, $1/\delta$ inversely proportional tends to grow. At the limit of delta tending to zero, it is demonstrated that this rectangle function $P$ tends to become the Dirac Delta; it has the same properties, but anyway it has amplitude $1/\delta$.
When I use $\delta(x)$ here, it is as if I were using this function here: the amplitude, I must remember, which is $1/\delta$, dimensionally is not dimensionless. So you will see it in a moment, I will have to write $\Delta x$ here, or I have to change my interpretation of $I_0$ as somehow a linear density. Anyway.

Let's see the solution. This is the solution in the generic case with constants $K_1$ and $K_2$ to be identified. This is basically having implemented, even if in a fairly intuitive way, the first hypothesis; that is to say, imagining studying this solution, the first and second hypothesis: this solution in two parts, $x > 0$ and $x < 0$. In each of the two scenarios you have me starting to get alarmed because there is one of the two exponential terms that explodes (either explodes to the right or explodes to the left), so I have to get it out of the way. And the other, continuity, I obtain by saying that for $x=0$ the solution for $x > 0$ and that for $x < 0$ must be equal. You can verify that when you put the modulus you have "for free" this situation in which in both cases only one of the exponentials survived: the good one, the dissipative one, the gentle one, not the one that explodes. And so you only need now to identify a single constant.

And obviously, you need this **third boundary condition**. The third boundary condition, in the next slide I tell you how it comes out, and has exactly the same form — again I ask you please, here there is $I_0$ involved which is missing — it has the same form as the semi-infinite case, however there is **1 divided by 2**. "1 divided by 2" I didn't put it there arbitrarily, it is not put in the current I am injecting. I will show you shortly a simulation that, under equal conditions, a semi-infinite case or approximately semi-infinite (it is infinite like this) in which there is this horizontal symmetry, the depolarization in this case is half of what you would have had before. Why? Because, as said, the current escapes both to one side and to the other.

How did I do it? I took what was still written on the blackboard at the top, the cable equation more complete than this I cannot, in which obviously I neglected this term because there are no synapses; in the middle there is only me with my current density. Here I have to put a current density to make things add up, a small $i$, and this small $i$ we defined last week as the quantity I inject divided by $2\pi a$ (radius) times $\Delta x$. But here, if I use the Dirac Delta, it is not a solution, it is not just any function; it is a function that in itself carries an amplitude that is not dimensionless and so I must necessarily take it into account and normalize, if you want, from this point of view.

After that I do the following thing. Okay, I remember because I always forget it... the only thing I remember is the space constant and that there is, surely there is a half, there is obviously the root because up there it appears as $\lambda^2$, being the degree of derivation with respect to $x$ two times; I only remember that it is proportional to the radius through the root, but I can't remember $r_m$ and $r_i$ if they go above or one below. So imagine if I can ask you in the exam what the expression is without speaking; maybe I help you derive it because Archangel Gabriel didn't come to bring it to Earth: we identified it starting from a lumped parameter circuit model representing the distributed parameter one, and slowly writing Kirchhoff we pulled out the partial differential equation and this term came out. End of aside.

This is the equation I have to study, or rather that in fact I have already studied, but I don't want to study it. I want to see if from here I can capture the condition I need to identify that $K$. In the previous slide I gave you the value of $K$ directly. I want to show you this way, which in my opinion is the simplest of all, of how it comes out.
Here I no longer have the dependence on time, I have all the dependence on $x$. And also this Dirac Delta is a function of $x$, it is not a function of time. What I can do... you see that since there was this $\Delta x$, here there is no longer the $\Delta x$ that would have remained.

What I can do is apply the integral to both members. If I remember correctly, maybe two weeks ago, when we were in the other classroom at the Policlinico, I had done exactly the same operation talking about my bank account, in which I wanted to tell you how treating synaptic release like a Dirac Delta it became easy then to understand what happened to a variable that was for example... represented for me the fraction of post-synaptic receptor channels in an open state immediately before the arrival of a pre-synaptic spike and immediately after. I had a stupid little rule that was "after equals what I had before plus a jump"; it came out from a similar equation (it wasn't in time and it wasn't in $x$ and the derivative appeared with the first order and not the second order, ok), but I had made the same speech in which I said: if I apply the integral, here I have an exact differential. Okay, the differential... let's say this kills only one derivative; the exact differential, the primitive, is the first derivative of $V$ with respect to $x$. Then I had said — and this is basically exactly the same term that was there — this term is a term that I am integrating a continuous function on a zero-measure term, on a zero-measure integration interval, so it is zero. And I had also told you that this was partly linked to a heuristic where when equations had an "ugly" input (ugly from the point of view of discontinuities like this), the output by definition of differential equation — which means that the solution is somehow obtained by integrating, and the integral is a thing that "doesn't caress", that smooths, not like the derivative that favors differences, that amplifies, that is a high-pass filtering, that makes a mess — so the output is certainly always more continuous than the input. And also in this case I can use the same reasoning to say: this is a continuous and differentiable function integrated on an interval of zero measure. 
Here, apart from these which are known constants, given — also $I_0$, it is me who says it is 20 picoamperes or 50 picoamperes — remains the integral of the Dirac Delta. Note: I am doing this integral between $0^-$ and $0^+$, like last time. Last time it was around, maybe we called it $T_0$, $T_0^-$ and $T_0^+$, around the moment when a pre-synaptic release occurred; in this case, it is in space. I am for simplicity saying that the pipette is placed at $x=0$, so the Dirac Delta is not translated, it is positioned at 0; that is when $x=0$ this is... takes infinite values.
The integral between $0^-$ and $0^+$ of the Dirac Delta is 1. It is the area. This is... I killed the Dirac Delta with its definition: it is a function such that, a distribution such that when one does the integral in a range where it is defined (in the limit case even $0^-$ and $0^+$, because it stays in there), the area is unitary.
$$\int_{0^-}^{0^+} \delta(x) \, dx = 1$$
And so I find this which in fact follows the boundary condition that I would have had in part... that I would have had let's say remembering that I was injecting current, so there was an additional current term in the Kirchhoff current balance at that node: one for the solution going to the right and one going for the solution going to the left.

It is easy, once one writes this quantity, to calculate it in $0^+$ and then minus calculate it in $0^-$. One discovers that this thing does not vanish and $K$ takes the value you obtain precisely from... putting this here becomes minus $2K/\lambda$, so the 2 comes out from here; what then becomes at the denominator one half (I told you the fact that the current escapes both to the right and to the left). And the other terms ($r_m$ over $2\pi a$) are at the end and $\lambda$ which is due to the derivative, are found here.
So this slide here tells you how I managed to pull out this solution in the previous slide. Clearly here there is "times $I_0$", I have to modify it and I will do it at the end of the lesson, excuse me. I thought and continue to think that it is this way... despite it being "oh my God, I am integrating the Dirac Delta", which is exactly what we did together two weeks ago, if you didn't die (some of you didn't die at the time). Also in this case you could say: "Okay, here it has a different meaning, totally different; it is me with the pipette and the fact that there is the Dirac Delta is because I am exactly in a microscopic point [without] any spatial extension"; there it was in time. *That's it*, ok.

So here with $V$, instead of small $v$, big $V$, is more something... is more one half, and here too I forgot $I_0$. $I_0$ I don't like, probably because it was... because partly the area of the Dirac Delta. In the infinite case, infinite in two directions, again: in each branch you have, as was intuitive to expect, an exponential decay.

It is as if it were exactly the same distribution you would have in terms of temperature if you had a metal bar and you had a heater, a fire at a point. It is clear that to the right and to the left you have gradually a reduction in temperature which is maximum at the peak. *Idem*, the same thing.

***


## Asymmetry of Attenuation in Space

Here I show you, just to liven things up, that when the cable happens to have a bifurcation point — of which I will simply mention quickly but we won't spend many words on — you basically have this symmetric trend. Here there is a further change at the bifurcation point and I put this figure because intuitively, if you understand here that the current entering this point goes to the left and to the right, so the shape changes around this point... when there is a bifurcation point you could think that the current here again bifurcates to one side and the other in a way dependent on what the possible diameter is. Because in the mix, inside that expression of $R_{\infty}$, there is $\lambda$ and therefore there is $a$, or the square root of $a$. 

So the contribution when you have an infinite cable or when the injection point of your current is not... or the arrival of a synaptic input — you see that in the end the current, apart from the minus sign, or the synaptic input are equivalent, roughly speaking; they are always pieces put as input into that differential equation. In this case, precisely, one has a spatial distribution that represents the point where the current injection occurs as the peak.

This is interesting because this very steep attenuation — again, very steep depends on the choice of $\lambda$, or rather on the choice... on what $\lambda$ is for those parameters — has a very interesting characteristic when speaking of **arborized structures** that are not by definition symmetric. In our case infinite in both directions, yes, it was symmetric. But if you have the current injection, as in this case, at the extremity of this dendrite, which is one of a branch, then there is a further branching, a further branching, of a very complicated structure... here there is no symmetry with respect to other points of this cable. If you want, it is a kind of generalized cable having more branches.

This branch marked as $S$ is a branch that, with its own, its own rectilinear, linear coordinate, has nothing to do with, does not share the potential distribution of the other cable except for a continuity connection at the junction point. But anyway, beyond this possible complexity which was understood only by starting to handle even just the **steady state** — as we are doing — the cable equation... and ah, ok, partly brilliant minds see it directly in the analytical solution, others have to do the numerical simulation. Numerical simulation which is required anyway when dendrites happen to have active conductances. Remember that the cable equation, as it is written, holds only in the case of membrane *leakage* properties; there are no — you can put them, but forget about doing the analytical solution — sodium, potassium currents in that equation. You can certainly put them, they would go to the second member, but you wouldn't write "pen and paper" as we are doing.

What is seen is that if you consider zero in this coordinate here as the point of the Soma, which on these — I repeat — this sort of rectilinear coordinate composed of many cables... you consider it in response to a distal current injection — distal means far, distant with respect to the soma — you would see that from here a current input having a certain amplitude would have enormous attenuation at the soma. This dashed line is the distribution of the membrane potential in space, at **steady state**, when the exact same current was injected at the Soma. It is a little bit more, but it is not so dramatically larger. 

So, nobody obviously knows, nobody can state it with certainty anyway: perhaps the dendritic tree of such an arborized neuron, exploiting "for free" the consequences of cable theory, has evolved in such a way as not only to increase the spatial extension like trees with leaves to catch as much light as possible — in this case to take as much input as possible — but the mathematics causes an injection of current in a particular cable, which you see is also thin (so it has a particular smaller $\lambda$, has a smaller radius compared to the diameter of this parent trunk), here the depolarization is very high. And at the Soma, roughly speaking, it has the same effect one would have if the current injection were not over there but at the Soma. Clearly, the attenuation is remarkable.

The interesting thing is that you see that while along this linear coordinate — curvilinear I should say — from the injection point up to this bifurcation point the attenuation is very very high (it goes down practically by 50%... here it is normalized, but it is normalized by the... I don't know what it is normalized by, it should be normalized but I don't understand why this is 3 point something; anyway from 3 point something to 1.8, whatever it is, is a remarkable attenuation), look however at what happens to the distribution of the membrane potential due to this distal current injection in this *branch*, in this arborization $S$: practically almost insignificant. And so it happens also in the other *branches*. So the attenuation is **asymmetric** and it is not a nonlinear equation, it is a very linear equation; what becomes nonlinear, or rather what breaks the symmetry in this case, is the geometry.

And the second thing we can note is that, since each of these domains can have a different radius and length — ergo a different space constant — and can have boundary conditions where they connect here and then here, it can give rise to what is called **synaptic territories**. That is to say: the fact that a synapse is here could make it function largely independently from a synapse that is instead on the other *branch*, due to a kind of electrical isolation due to attenuation. The attenuation is asymmetric, so the electrical behavior in different *branches* is too.

Note that this graph here, this analysis here and these considerations are from '73, so **Rall** [Wilfrid Rall], the father of this cable theory, contributed dramatically in the '60s, '70s and '80s to the understanding even before particularly performant computing resources.

**Idan Segev** was a student of Wilfrid Rall and is one of the world experts in this type of multi-compartmental description — one says of the excitable properties of neurons — and in particular he has applied this concept to the specific morphology and geometry of some pyramidal cells, human pyramidal cells. And where you see here the part of this dendritic morphology etc. in red — here, here, or here, or here, or here — and the mathematical description represented in colored form of what I showed you before, that is that the depolarization can remain... this quantified in terms of attenuation, relatively localized thanks to morphology. But this depends on where the inputs are applied. 

If they are applied at the Soma — and I will show you a demonstration shortly — this is somehow... the attenuation is relatively... it makes itself felt in the dendrites, but it is relatively low. So an input applied at the Soma has an electrical potential effect on the basal dendrites and also on a part of the apical dendrite. Conversely, if the input is distal, at the soma it is practically attenuated almost 100%, while it remains confined in this portion of the dendritic tree. *Idem* in this case; in this case instead it is an input in the basal dendrites which again behaves as a kind of compartment in its own right.

I will show you now one of the simulations. This is a notebook that is at your disposal on Google Colab. If you have problems speak up.
So, this other notebook compared to the previous ones continues to be written in Python, but since the problem is to demonstrate easily — I am giving you two, one I added this morning — to study the cable equation, there are two roads: either I use a simulator, therefore a program, a library if you want, whose job is to simulate cables, or I have to implement the numerical method myself or invite you to do the same. As long as it is Euler, even if Euler is the crudest method of numerical integration, things go well. I told you that for that type of partial differential equation things are more complicated.

So what I do in this notebook is use a program called **NEURON**, with much imagination; it has existed since roughly the '80s (from the 80's) and initially had — and still has, however almost no one uses it — an interface based on an obsolete language called HOC (it was a version of the C language with the initial of the creator, **Michael Hines**). He is an authority in the field because after... maybe it could be the mid-80s, anyway after 20-30 years this continues to be still the reference for the numerical simulation of cables, of networks, of neurons etc.
Currently for more than 15 years, it has more than 10 years, it has a Python interface and is relatively easy to install: `pip install neuron`. So just as you install a Python library — matplotlib, numpy, scipy or whatever, pandas or whatever you use — you can install this and you have it in the code. You can do it on this Google Colab that you see; I do `!pip install neuron` and I do it with the exclamation mark because I am doing a so-called *escape*: it is a *shell* command, this is not a NEURON command, but it is an operating system command, however Google Colab allows me to do these things. I am in fact downloading from a *repository*, with this command, I am downloading the Neuron *repository* and installing it in the cloud. A few years ago it was unthinkable to do a demonstration or put you in the condition to — without going too crazy — be able to access a simulation with Neuron without having to install 25 things on your laptop, then it doesn't work... so you are lucky to live in a time of ease from the numerical point of view.

What I do, as usual — if you are brave you can see the code, but it is not the object of this course, if not to stimulate some of you to play with it — in which I fix the space constant because I fix the radius parameter (the radius in this case is 0.5 micrometers and with the other parameters has as space constant value 500 micrometers) and what I can do is change the length of this cable.

If the cable is long, you see that the solution with the red dots, which is the numerical solution, seems to describe well — or rather conversely, seems to align well — with what is the black trace. The black trace is the analytical expression $R_{\infty} \cdot I_0 \cdot e^{-x/\lambda}$, therefore the analytical expression in the infinite case. While the red case is the finite case.
So much so that with the *slider* I am changing... how to make this go away but I think not, not being able to do it... I am changing the length. Now the cable is 4 mm, so 4 times lambda... sorry, 8 times lambda. If I make it closer to... now it is 2007... if I make it 900, this is two times lambda, you see that the theoretical expression in the infinite case deviates from the finite case.

You could help me and maybe help me make another box in which not only do we put the expression of the infinite cable, but we put the expression of the finite cable, the one with the hyperbolic sine that I believe I interpreted that you liked particularly. The expression is ugly but it is not complicated and it is extremely interesting because in the end neurons are structures that can have both long or compact compartments.

Here you have the same thing if you want to use it, in which this is the situation where I have a semi-infinite cable. In the numerical simulation I cannot make a semi-infinite cable, I have to make it long, but the current injection is in the middle and again you see that if the cable is particularly long, the agreement between the red and the black is beautiful: $e^{-|x|/\lambda}$ with that quantity, remember, $1 / (2 R_{\infty})$. If the cable is short, the solution is not... it is also profoundly different; this is about one lambda, the solution deviates significantly.

There is a further simulation here, which basically you have and in theory you can use. This is in time. This graph here represents, varying time, how in different points of a cable the membrane potential changes in time. Let's say, they look like transients that have a different attenuation. Here they are for different values of... I remember what, they are different positions, different positions for different lengths. When the cable is short... in the end it is not seen particularly in this one, so I should basically say that when the cable is very long compared to lambda, the attenuation is very evident and the thing you notice is that the shape seems to be different. Again, to do a simulation like this I cannot numerically in a simple way do the numerical method, I used Neuron again.

## Sinusoidal Permanent Regime and "Effective" Space Constant

Let's see then the case of the transient. However, before doing the case of the transient, I consider the transient in the **sinusoidal permanent regime**, in which by means of transform operations in the frequency domain... whichever you like best, Laplace or Fourier; we will do it with Fourier in about half an hour... in fact again the derivative in time disappears. It disappears because in the transformed domain temporal derivatives become algebraic operations, they become multiplications by — particularly in Fourier — $j\omega$. So even there in that regime I lead myself back to the case where I know how to solve it.

I will show you the equation of the transient in which I do not make any... let's say there is no simplification, there is no hypothesis and the solution changes both in space and in time, but I will give it to you "from above", we will not derive it.

This last graph of the Google Colab comes later. Here is the method I was suggesting to you in which to kill the temporal derivative term, instead of setting it to zero — so saying "ok, let's try to understand something", and something we have learned: symmetry, synaptic territories, attenuation, the fact indeed that there exists a difference between a finite, semi-infinite or finite geometry — the other case is that of studying a solution in the **sinusoidal permanent regime** (AC regime).

I do this because the equation is linear and in principle, if I have a signal, a current, a synaptic input that I can decompose into the fundamental Fourier components, I could suppose to study the individual solutions frequency by frequency and then recombine them; which is the principle of linear systems theory, recombining them knowing the solution, the response of the system to individual frequencies. But at least one generic frequency I have to find.

It is done with the method of transforms where this quantity here becomes, in the domain of transforms, an operation that is no longer a derivative but becomes a product, an algebraic operation. I want to use Fourier because I find it nicer and because in the end I am considering a sinusoidal regime, so Laplace is no longer indicated when I am dealing with transients. And I remind you that the Fourier transform of this function $v(x,t)$ — I am considering it a function of time, it is also a function of $x$ but this is the Fourier transform in time; I could do it also in space but here I do it only in time — I indicate it as $W$. It continues to be a function of $x$, because $x$ is not the independent variable I am considering, but it changes with $\omega$, the so-called angular frequency (that is to say $\omega = 2\pi f$, the frequency, in an equivalent way).

I like $W$ because I easily remember that the transform, if I want to transform small $v$, I have to put it under an integral, minus infinity plus infinity because it must be stuff on the whole domain of definition of the function, there is $1/2\pi$ and then inside there is $e^{-j\omega t}$ in $dt$. You see that $t$ is saturated being the *dummy* variable of the integral, so this quantity here is a number, it is no longer a function of time; time is no longer there, the integral is the subtended area. Obviously, time is weighted by $\omega$, so changing $\omega$ changes this integral.
$j$ is the square root of minus one and I use $j$ because engineers usually use $j$ and do not use $i$, where $i$ would be the current. In fact $J$ is also often used as current density, anyway $j$ is the imaginary number, $\sqrt{-1}$.

If I do this it can be demonstrated — and you can do it calmly if you haven't done it for years, you could try it again — what is the Fourier transform of the derivative of $v$ with respect to $t$. And you would find that the transform sign and derivative sign can be exchanged because they are linear operators and you would find that what survives is $j\omega$, which is the derivative of this term... is $e^{-j\omega t}$.

I have already done the little calculation because I am sure it is not so crucial, so here this derivative, so where I have $v$ and there is no time I replace it with $W$, because for this linearity the derivative twice in space of small $v$ is equivalent to the derivative with respect to space twice of $W$. Here is the different thing and here it becomes $\tau_m j\omega$ times $W$ plus $W$. I have already factorized and written it in this way, because this is exactly the same form that the cable equation had in the **steady state** regime, when however there wasn't this "pizza" (mess) here, so for $\omega = 0$.

So on one hand I am pleased to find the DC case when $\omega$ is zero; when $\omega$ is zero this is exactly $\lambda^2$ times the derivative now total — it is no longer $v$, but it is $W$, because obviously $\omega$ counts — this was the steady state case. When it is not the steady state but it is a sinusoidal regime, what engineers call AC regime, obviously it changes. And I write it in this way to be able to suggest to you that it is as if it were exactly the same solution we saw before, the same equation, and it will have the same solution we saw before.

But here I would like to write not $\lambda^2$ divided by blah blah... I would like to write **effective** $\lambda^2$, that is a kind of space constant that evidently depends on frequency, which I can put here and I can say: "Ah, ok, the solution is like in the steady state case, but the space constant changes". That is intuitively I think that at different frequencies the space constant is changing. So for slow signals the space constant could be large, while for fast signals the space constant could be small. It is as if a dendrite, or a portion of it, had an **accordion** behavior depending on how the signals occupying it vary. Which is very interesting, but this way I cannot do it because here I have this damned $j$. So this is a complex quantity. I would like it to be a real quantity.

A slightly boring calculation follows. Again you will see it is not particularly complicated. I apologize, this should have appeared earlier... ok:
$$\lambda^2 \frac{d^2W(x,\omega)}{dx^2} = (\tau_m j\omega + 1)W$$
Factoring $W$ I can see that there is this term $\tau_m$ and blah blah going to the denominator. If I took this quantity here and called it **$\lambda^*$ squared** ($\lambda_{star}$), this effective lambda, I could write this expression straight away. And actually, I write it because it is exactly the expression, the solution of this AC equation, but I am not yet satisfied because this quantity here is a complex quantity. As indeed is the Fourier transform of a function $W$: it is a function of variables, a complex function of $j\omega$, it will have a real part and an imaginary part, it will have a modulus and a phase.

So if I really wanted to, beyond the fact of the phase which might perhaps not... I would like to at least understand if it is the modulus. I would like to rewrite something like this as modulus times $e^{j \cdot \text{phase}}$, because if I can do it, then at least the modulus I can say: "Ah, ok, this accordion thing depending on $\omega$ I can do it". For the moment I still cannot do it, it is the same comment as a moment ago.

So $\lambda^*$ I define as what I would like to put here to make this denominator disappear. I would call it $\lambda^*$. And since this was $\lambda^2$, here I put $\lambda$ and here below I have to do the square root however. And again here it bothers me further because yes, I understood that $\lambda^*$ is a complex quantity, but if you put the square root at the denominator too it bothers me further. I would have liked to write $\lambda^*$ as $A + jB$ (real part and imaginary part) and then see if maybe... or rather, I would like more to do modulus and phase, because the modulus gives me an indication of the amplitude. And if I am talking about attenuation, about an amplitude, maybe that is enough for me. Yes, maybe there will be a phase shift that depends on every frequency, the concept of attenuation that could selectively change some frequencies and not others, I could see it with the *magnitude*, the amplitude, the modulus.

So the goal is to write something like this, to transform this exponential with a piece that is a complex number into $M \cdot e^{j\omega}$, where this $M$ evidently is also a function of space, a function of $x$, and also the phase will be a function of $x$. But I want to write it like this because I want to look at this $M$, because "you appeared to me, this $M$". I want to understand $M$, even the phase if it were the case. And this $M$, this modulus, is obviously a function of the point and the frequency.

How do I do it? First of all I note that this is $e^{-x/\lambda^*}$, so it appears at the denominator. To avoid suffering too much I already start studying $1/\lambda^*$, because you see that here at the exponent it is "1 divided". This brings to the numerator $\sqrt{1 + \tau_m j\omega}$. If there wasn't the root, could you tell me what is the part... imagining that there is no root... what is the real part of this complex number $1 + \tau_m j\omega$? And the imaginary part, whatever it is. It would be easy. The fact that there is the square root is a little annoying, however I could imagine writing that if I square this, I could in other terms say: if I have $C$, which is some complex number, let's forget the denominator... $1 + \tau_m j\omega$, and that's it. I could think not of writing $C$, but of writing $C^2 = 1 + \tau_m j\omega$. Of this I know how to write the real part and the imaginary part, as you did before.

The only rip-off is that then I have to go back. But I can remember that complex numbers also have another form, but not only this one where you have real part plus $j$ imaginary part, but you have... so $C^2$, would be $A^2 e^{j\Phi^2}$. With this trick, with which I am not going to stress you more than that, you manage to write both the modulus — and again I am probably more interested in writing modulus, at least the modulus — both the modulus... and this is no longer a complex number, thank you, it is the modulus. And the phase, if I needed to find the phase, the phase is half an arc having as tangent $\tau_m/\omega$ no, $\omega \tau_m$ (imaginary part over real part). The fact that there is the fourth root there and the fact that here there is a half, comes from the fact that here we did this trick: we squared and then we went back, but not with the same form real part plus $j$ imaginary part, but using the vector notation, so modulus and phase.

Here I invoke, I believe it is the only time... no, not the only time, but anyway it is relatively rare in neuroscience to invoke Euler's formula... ah no, I did it also... no, it is not rare. I invoke **Euler's formula**, which if you see YouTube is full, full of videos where the beauty of this equation is explored, etc.:
$$e^{j\phi} = \cos\phi + j\sin\phi$$
Again I am doing "back and forth" (*avanti e indré*) between the two ways of writing a complex number, because I wouldn't want $e^{j\phi}$... I wouldn't want to miss a part of what... repeat, my goal is to write modulus and phase of this equation here. Of this equation here. So I have to suffer, I have to stress this $1/\lambda^*$ a bit to be able to arrive at least to concentrate here everything that is not... that has no complex terms and here the imaginary part, the phase part.

As said I have to do it in stages. The first is to understand this $1/\lambda^*$ how I write it in terms of modulus and phase. Ok, but the phase I can write further like this, so I can go back and write that $e^{-x/\lambda^*}$ I can write as an exponential $e^{-x A \cos\phi}$ times $e^{-j x A \sin\phi}$. So this piece here is the phase. I, repeat, was interested in understanding this damned modulus part because I would like again to compare it with the analytical solution in the case where there is not... where $\omega$ is zero, where I have no complex numbers and where the solution was simple, it was a decaying exponential, with a $\lambda$.

Here what $\lambda$ must I put? I imagine it is some... you see, beyond the fact that $A$ is this clamorous pizza (mess) here, if there wasn't this clamorous pizza it would be $1/\lambda$. So here it would be $1/\lambda$, $e^{-x/\lambda}$, it is the same thing. But in the AC case, sinusoidal, you have this additional term at the numerator, so inside here, and you also have $\cos\phi$, and here you have the cosine of a half arctangent blah blah. I do it because $\phi$ also depends on $\omega$, so not only $A$ depends on $\omega$, but also $\phi$ depends on $\omega$. This part here maybe I don't care, repeat: I am attracted to understand how to do in an equivalent way this blessed sinusoidal permanent case.

There are relations that I wrote simply to explain to the most adventurous... this let's say I will never ask you in the exam to... not that it is complicated, it is not complicated but it is boring and I don't think you can easily remember the expression of the cosine of the arctangent. I didn't even know this formula existed. This one I remembered... cosine, so a bisection formula, this one I think I remembered, and it is important because at a certain point this cosine of the arctangent is a cosine of half arctangent.
Anyway, if I were so brave as to do this cosine of $\phi$ and write it properly, in the end, I would obtain a quantity that depends on $\tau_m$ and $\omega$, besides $A$ which is also a quantity that depends on $\tau_m$ and $\omega$.

To make it short, this pizza here I can actually write as this $A \cos\phi$. I can write it with an expression... here I am representing it as "1 over", because here I represent it as an inverse of a... this is an equivalent, effective space constant, AC. It is a horrible expression, but I can plot it and I see that when the frequency is very low... this in semi-logarithmic coordinates... when the frequency is low... low compared to what? Low compared to $\tau_m$.

$\tau_m$, membrane constant, tells me how fast is "fast". If the membrane time constant is of the order of 20-50 milliseconds, if I have something that varies very slowly of the order of 1-2 Hz, comparing 1-2 Hz with the inverse of $\tau$, which should be 20 Hz, makes this frequency negligible. If instead I have an oscillation that is higher than a frequency component that is higher than 20, 30, 50, 100 cycles per second, then wherever I have $\tau_m$ times the frequency ($\tau_m \cdot \omega$), here I wrote it as $f$ to be able to say cycles per second and not radians per second, then there it is a fast frequency.

When the frequency is slow, practically $\lambda$ is one of the terms... it is not this term here under the root, practically it is uninfluential. So if I go slow, for slow signals... if I have an IPSP, just to be clear, and this IPSP arrives... this is a neuron and this is its dendrite, if I have a synapse here and this IPSP let's forget that it jumps very rapidly, but when it decays slowly this *decay* behavior is very very slow and there this graph is telling me: "Look, the attenuation solution of that phase is indistinguishable from the case where it was a cable at steady state".

The problem is here, this instantaneous rise. In reality, it is not instantaneous, but if you remember the value of $\alpha$ for the concentration of a neurotransmitter was a fraction of a millisecond, particularly I am thinking of AMPAs, they go up rapidly: a millisecond could mean a kilohertz. I am literally doing one divided by the time constant; the inverse of a millisecond is a kilohertz. Suppose there are various... ok, 500 Hz, 100 Hz, whatever it is. We are anyway here, where this effect of this term here makes itself felt: the faster you go, the more this term that "kills", lowers the space constant kills it more and more.

So I expect that an IPSP or EPSP or even the inhibitory equivalent has certainly a deformation, cannot pass, despite the attenuations, cannot let the very rapid transients get away with it. Because in itself it is telling me that the cable becomes with a small space constant. If the cable becomes with a small space constant, even if it is a little dendrite, a small dendrite... if $\lambda$ is very small, that cable there is as if it became infinite. I am taking it to the extreme to make you understand, so at high frequencies $\lambda$ becomes small, it is as if the cables became long, so good luck attenuating! Nothing arrives at the soma anymore.

And if you think about it, it is not so impossible to think, this is the same thing I said before, it is not so unheard of to anticipate intuitively because you know that if you have a structure with capacitors, resistors, you have a **low-pass filter**, so it tries to disfavor very rapid changes in time. So if I have really long stuff — I think of Lord Kelvin, telegraph cables — it is stuff... a rapid transient, no surprise if it didn't arrive on the other side of the ocean. Not only did the telegraph note not arrive at a certain amplitude — thanks, you killed it, you attenuated it a lot — but also the spectral content is very different, the rapid transient is completely altered. A filter, if you think you understand what a low-pass filter with an RC in parallel is... if you put many of them, it is like putting many filtering blocks one after the other. If the first one deformed and changed the frequency content of the output, if you put a further filter and then a further filter, so imagining decomposing a cable with a series of filters, at the destination you have something that is strongly *low passed*, strongly passed-low, strongly deformed. Rapid signals are no longer there.

This is a further graph that shows you at different frequencies, this is the solution, the solution in the AC case. You see that it resembles very much the case of a semi-infinite cable, as also this is a semi-infinite case, in the DC case, in the **steady state** case. The steady state case is represented by this blue curve, it means frequency zero. If the frequency changes — increases: 100 cycles, 500, 1000, 1500 cycles per second — it is as if you were changing $\lambda$, you are making it much smaller. 

This graph is further normalized to $\lambda$, again so that it can be used always. So what counts is the comparison: the attenuation is very very abrupt if you go fast and is practically a little less abrupt if you go very slow. Paradoxically if you don't move at all, in the DC case, the blue case.

So this quantity here and how we arrived at deriving it may require some annoyance from the point of view of manipulation of complex numbers, which I do not ask you in the exam, but I would like at least, more or less — particularly from here, from this point here — simply writing Fourier, an intuition to say: "Ah shit, here is $\lambda$ divided by $\omega$". Yes, it is true, there is $j$, so this discourse is very intuitive and you cannot do it rigorously. But when $\omega$ changes, the effective $\lambda$ here becomes smaller and smaller. This is an intuition I ask you to develop.
And for those who are not particularly annoyed by complex numbers... I am not talking about doing the integral in the complex field of complex variable functions, only massaging expressions to make them return with modulus and phase, to be able to write modulus and to the $j$ phase. The phase would also be interesting and important to examine, but I am not interested; for the moment I am only interested in the modulus. This modulus has an expression that depends somehow inversely on frequency: the more the frequency, the more this term at the denominator kills the $\lambda$.

## Interpretation of Experimental Data: Somato-Dendritic Recordings

So, now I tell you what we do with this knowledge. The goal is that of extracellular signals, but while we are at it I tell you what this entails from the point of view of the effect of synapses, of dendrites, from the point of view of the soma.

And I simply show you two examples where, not so in the past — maybe you were born in '94, it is not in the nineteenth century — this is a thing that has always struck me and I met the guys who did this experiment and this impresses me: the fact that it is a recent evidence, that it is not an evidence that has always been known. So it is a relatively young field, that of neuroscience, that of neuroengineering.

Here you see an experiment where a pipette was placed to record or stimulate both in the Soma and in the apical dendrite several tens or hundreds of micrometers away. I told you last time that this is absolutely not a simple thing, because while the Soma on a computer screen, despite the infrared microscopy technique, you see it well, the dendrite you see as a thread and it is difficult with a pipette to know if you have only an image, if you are in front, if you are behind or if you are pinching it. If instead I did the same thing with a soccer ball it would be a little easier. The Soma is a soccer ball, the dendrite is really very very thin stuff: the diameter here could be one micrometer, two micrometers; the Soma instead is about ten, 15-20 micrometers, so it is nice and massive.

What was done is injecting a current and making the neuron fire. What you see here, this trace here, is a somatic action potential and this other electrode, which at a distance of several hundreds of micrometers, caught the echo of it. This is a strange thing because it is called **backpropagating** action potential — anterograde perhaps — and it is an unexpected thing. Neurophysiology from the textbook predicted that when a spike, an action potential was generated at the Soma, it would propagate along the axon. What do dendrites have to do with it? 

The dendritic structure as I sold it to you, despite there being evidently a remarkable attenuation and a remarkable filtering, is such that with a certain delay — the delay you see apart in the peak but you see the rise of the potential trace after... note that it might not be, no ok, I think here the calibration bar is the same for both... it is a much more attenuated activity, about half, and it is slower: it goes up more slowly and also goes down more slowly, so it is evidently a filtered signal.
It is interesting because it seems to be the means by which a synapse that sits here... if you remember two weeks ago, we talked about synaptic plasticity dependent on the arrival time of a pre-synaptic action potential and the arrival of the post-synaptic action potential. I had told you it was pre-post, pre-post reinforced the synapse; if the post occurred first and then the pre, the synapse would depress, unless the time window was too large. This is a cellular mechanism based simply on the cable equation, on the fact that the dendrite is a passive cable — it is an active cable even better, we won't talk about it though — in which at the synapse, here after a few milliseconds of delay, the information arrives: "This neuron is firing and I see the echo of it". So this is the attenuation of the action potential that backpropagates.

Instead, the orthodromic propagation, so directed in one direction from the apical dendrite to the soma, was done... here this again, the calibration bar is for both traces, one millivolt, so it is extremely smaller. What was done was here injecting a current that simulated, that emulated an excitatory potential, an excitatory post-synaptic current. In other experiments, if you don't like this, the experimenters didn't put the pipette inside the dendrite, they put it around there and then they did, inside the pipette there was a little glutamate and they did a so-called *puff*, they did a release of glutamate. And this glutamate bound to the synaptic receptors that were here in the surroundings, giving rise to an almost identical waveform.

What you see is again what we discussed before: a distal input to the Soma propagates with a remarkable attenuation, as well as with a filtering, a deformation of the... you see that this very steep rising stage does not pass, it is filtered.

***

## Transient Solution and Conduction Velocity

Let's see the case of the **transient**. The solution of the transient I won't tell you how it is derived; if you are interested you can look at or search for the same derivation for the diffusion equation or the heat transmission equation. It is exactly the same mathematics, the same form, only that there it is not $V$ but it is a concentration or a temperature, and they are not transmembrane resistances or capacitances, they are thermal capacities or thermal resistances of thermal coupling, for example of conduction, of heat exchange by conduction or convection; not phenomena linked to Kirchhoff and in the case of diffusion they are conservation of mass, not of charge and current.

To make it short, the solution in time and space, when you have an infinite cable in both directions and you are injecting a current at a specific point — so a kind of Dirac delta with $x$, but this is also in time, in the sense that it is kept constant — has this form and apparently it tells you nothing. I would like to rewrite it in this way.
If I rewrite it in this way I hope that this exponential of $-x^2$ resembles, reminds you of, what is the expression of a Gaussian, of a bell curve. Actually, it is exactly a Gaussian because Gaussians have $e^{-x^2 / 2\sigma^2}$ (something you call $\sigma^2$, the variance) and if it is a Gaussian it means that first the term multiplying the exponential must be $1 / \sqrt{2\pi \cdot \text{variance}}$.

So, then this is an interesting observation for those who know or have by chance studied the diffusion equation because there the Gaussian pops up. As in that case also here however... so it is a Gaussian with zero mean, it is centered exactly for $x=0$, it is this one here. But you see that the variance, this $\sigma$ that I identified — that is I am calling it variance, but it is not a variance, it is a mathematical term that here only analogously by analogy has the meaning of variance — you see that it depends on the space constant and on time. 

[Image of Gaussian distribution spreading over time]


That is, you have to imagine that if I have this cable and I have an electrode that I start stimulating at a point, initially the Gaussian is very very narrow and as time passes it starts to "belly out" (flatten/spread). Not only does it belly out, but its amplitude, this $A(t)$ — you saw it from here, from this exponential term, I rewrote it here simply for convenience — its amplitude, fixing the point, is an exponential in time, with the time constant you are used to seeing.

So in space there is something with this $e^{-x^2}$, but it is a simultaneous solution, both in space and in time.
In space, like the bell curve, in space it would tell you, taking a flash at a particular instant... so this instant is practically immediately, 0.1 times $\tau_m$ (the $\tau_m$ is the membrane time constant, suppose it is 20 milliseconds, this means after 2 milliseconds it is like this), after 20 milliseconds it is like this, after 40 milliseconds it is like this: it bellies out and flattens.

This is quite ok to understand because what I am doing is: I gave a "flick" (*schicchera*) here — I have to give a flick with a laser, but I can't — after which, let's say, if it were ink in a tube full of water, this would diffuse laterally and the concentration would be initially denser, the ink would be denser here and then it would tend to belly out. The same thing you have with the electric potential and you can also study it not in space but in time at different points.

It was what I showed you in the Google Colab code at the beginning, but it doesn't make much... it makes sense only if some of you were particularly curious and wanted to get your hands dirty and see how this thing changes here. Here you have in time; in time means that you must have more pipettes, or from the point of view of a simulation have "record here" (so record from $x=0$), then "record from $x=0.5$", "record from $x=1$", $x=2$, maybe 1, 2, 3 as a function of $\lambda$ (so of the space constants), and every time you record. Sorry, not every time: you start the simulation and you simply record from distinct points.

You see that for $x=0$, at time 0 basically the amplitude is infinite and then it starts to decay exponentially. I am looking at this curve here. Something that surely you would have expected thinking of the usual RC, of the usual single-compartment cell. You give it a flick, the potential shoots up and then decreases, decays in time, like with the time constant $\tau$. Again here the $t$ axis, of time, is normalized to $\tau$, so that it is universal. So you see that after one $\tau$ the amplitude has reduced by 66%... I will never remember this engineers' thing, whereby after one $\tau$ the exponential is at minus 30%, I don't know.

If you move to 0.5, to 1, to 2 lambda, you no longer see the peak; you see it very attenuated and shifted, displaced. In the end also the potential that backpropagated in the simulation I showed you before seemed not only to shorten but to shift. At one lambda or two lambda it seems to be profoundly shifted to the right, as well as attenuated.

What one can do, and Rall did it for the first time in the '70s-'80s, is to try for example to understand what is the position in time... at what time the peak occurs.
Note: I am not dealing with the wave equation. The wave equation, which in fact was the one I simulated when I showed you the axon (the all-excitable axon), had by definition a waveform that was always the same and propagated in space. This is the wave equation, the one called D'Alembert's equation. It would have, if I remember correctly, a second derivative term of time, but I could be wrong.

Here there is not a true propagation, because the waveform there is not a primitive that propagates, as in electromagnetic fields, in Maxwell's equation, it propagates always identical. Here it is something that looks like a perturbation moving both in time (fixing the point) but also in space (fixing the time). So what one can do is plot the position of the peak and say: where is this peak as time varies?

You write this relation and find that in fact it is an almost linear relation, except the beginning. At the beginning precisely there is, for very small times, there is a small exception: this curve seems not to start straight but start with a greater derivative. And you can write that the slope of this curve — in the end this is a space-time curve — so the *slope*, the steepness of this curve is a velocity. Repeat, it is not a wave propagation phenomenon, it is a propagation phenomenon of a perturbation that changes shape. But if I take the peak as good, I can think that this peak could be representative of the peak of an IPSP, of an excitatory potential, a distal inhibitory post-synaptic potential. If I know what the distance is and I know what the velocity of — let's call it — propagation, of conduction (let's call it better conduction velocity) is, I could predict how much time a remote synaptic potential takes to arrive at the Soma. Not only the attenuation which, ok, is there, etc., but also the time.

It turns out that this conduction velocity is given, perhaps trivially, perhaps not... not trivially because there is this 2... space constant divided by time constant:
$$v_{conduction} \approx \frac{2\lambda}{\tau_m}$$
Ok, twice space constant divided by time constant. In the end it was absolutely not intuitive to think that that term put to multiply the spatial derivative and the other to multiply the temporal derivative had to do in such a simple way — and this obviously is an approximation — to describe what seems to be a kind of conduction velocity.

And this thing of conduction velocity, in the '80s or '70s, was compared by Rall with experimental traces. What you see here are experimental recordings made by someone who in the motor neurons of the cat spinal cord recorded the soma and saw different waveforms. Rall normalized them, that is he put their amplitude... he made sure that dividing by the amplitude of each peak, that it was always the same, and so he realized two things.
The first is that the shape changes and quite a lot, remarkably; it seems not to be simply a scale factor, it seems to be a filtering. So dendrites filter (we saw it before, we know even that they are low-pass filterings, ok, of a slightly strange type because we also have a spatial component and we saw it with the AC analysis). 

And he was able to compare the so-called *half width*, the width, and the time to peak and plot — these are the little triangles — the measurement in an experiment and compare them with the dashed line which is the consideration we made before. He had the equation, the solution in the transient, and he could say: "Well, do I have these experimental results? Do they add up?". And the answer is yes: these points align perfectly on this theoretical curve.

And the *take-home message* is this here: that distal synapses not only attenuate but you also have, let's say, they belly out, you have a filtering. And this is particularly important for attenuation and bellying out, for what is the effect on the Soma. If I am bellied out and I am for example a current traveling in the dendrite and I enter inside the Soma, maybe I will be lower, but the subtended area (current times time) is the charge, so I could have a different effect depending on my shape.

Obviously, the problem of when, of *timing*, remains. If I have a synaptic input very close to the Soma, the possible *recruitment*, the possible evocation of the action potential can occur within a short time. If I have it distal the peak occurs anyway with a delay. Again, also from the point of view of time-dependent synaptic plasticity, these considerations are in the same order of magnitude as for the potential backpropagating in dendrites; also these which are synaptic inputs propagating directly from dendrites to soma have a delay that is perhaps comparable with those time scales of plasticity.

## Dendritic Democracy in CA1 and Spatial and Temporal Summation

There is an exception to this rule, at least from the point of view of attenuation (bellying out, filtering is inevitable), and it was discovered a few years ago in a population of hippocampal pyramidal neurons. This exception is called **dendritic democracy**.

So the normal case is that if you have a far input and a closer input, distal or proximal, at the soma the distal input results very attenuated as well as bellied out, while the proximal input results a little attenuated and a little bellied out, but less. So somehow the more distant you are, the less attention I pay to what you say, because at the Soma, a site close, near to where the action potential is generated, I am little depolarized by a far input.
However, in this class of cells, it was found that in the periphery the distal input was proportionally much higher, so that despite the attenuation is called electrotonic — which is the one we described with the cable equation, has to do with the electric tone of what was thought to be a mechanical phenomenon, but in reality is a purely electrical phenomenon and has to do with the cable — at equal electrotonic attenuation, distal or proximal inputs in fact have as peak, as amplitude, practically the same quantity. You see however that here the red is bellied out anyway, but it started wider, so in the end the amplitude is equal.

This is an important graph that summarized this phenomenon in which if you look in the dendrite at different points... so each of these, then this is space in which the distance from the soma, 0, 100 micrometers, 200 micrometers, 300, 500 micrometers, where you have a synapse. If you have a synapse here and you are lucky enough to put an electrode here, you would see that its activation causes a much larger depolarization than it causes here when you have a synapse here. Instead, the electrode placed at the Soma, which you never change, should show an attenuation, you should see an exponential curve $e^{-x/\lambda}$ because this is space, if it is distant I attenuate more; actually you see flat, so it is like what has been called democracy because everyone has the same voice, an equivalent voice to make the Soma fire presumably. 

It is not clear if this thing here is a specific thing of some... well it seems it is like this in CA1 cells, pyramidal cells of the CA1 zone of the hippocampus and it is not like this in the cortex. Is there a reason why this is so? Is it a kind of evolutionary effect to compensate for what is a bug, an electrotonic attenuation? "I would have wanted the neuron to be a point where all synaptic inputs had equal weight, instead I had to for genetic, molecular reasons, express these ramifications which however do not represent me, I don't feel like an arborized dendrite". Ergo I ensure that when a synapse is established, the number of post-synaptic receptors inserted distally or proximally is different. I put many more far away and few close by, so that if there is for example the same amount of neurotransmitter, these make much more distal depolarization and compensate for the attenuation.

Another concept I want to tell you before concluding this part is to conclude the part of extracellular potentials, which will be again relatively qualitative, will be based on the presentation of computer simulation, of simulation studies, is the concept of **temporal and spatial summation**.

**Temporal summation** you should have overheard with the story of the university paying my salary, of the famous "tails"; some of you in the interruptions continued to use the term I used of the fact that when an event — in this case an IPSP, an excitatory synaptic potential, could be inhibitory — when it arrives before the tail is not completely exhausted, it sums, there is a summation in time.
It is not a surprise, this has to do with the fact that the charge balance equation, $C dV/dt = \dots$, in the end is an accumulator, it is the equation of a condenser, capacitor, that accumulates. If I integrate both members $C dV/dt$ or that $V$ equals the integral of the current... integral means sum. So if I have a neuron receiving four afferents, four axons with each a synaptic terminal, independent... this is an ugly one, maybe with your help I can make a nicer graph, because they are not IPSP, I could have made it EPSP... instead of being a single pre-synaptic activation event, they are four events very close together, given that each event sums to the previous tail that was not yet exhausted, there is a progressive depolarization.

The same thing happens when you have synaptic inputs occurring at close points of the same dendrite, in a first approximation, so it is not a summation in time, but it is a **spatial summation**. And here too it is not particularly complicated to develop an intuition, instead of having a pipette at a point, or more pipettes if you want, or more synapses, and by pure linearity of that equation superposition of effects holds. That is the response to the sum of inputs to these is the superposition of the individual responses obtained when the individual inputs are administered independently. So there too there is a decomposition due to, if you want, a spatial summation, but there spatial summation continues to hold because the equation is linear.

This is the graph that probably could enlighten you the most. These graphs, these purple, cyan and green curves, are the responses in an infinite cable to a current injection at different points. You see: here the injection is at zero, here the injection was at minus three... I am not a genius, I see where the peak is... in this dendrite condition is only passive, that is the only explanation, I had an electrode or an input there. A current that in space was a Dirac Delta for example centered there, this was centered at 3 and had a smaller amplitude, so these single curves are those I obtain by injecting into the cable one of these currents at a time, independently, I put the others to zero.
When I inject them all together, the response — either numerically, or analytically, or simply with this heuristic which is correct, because the cable equation is a linear equation — you obtain it as a superposition, so a sum point by point of the three curves. So this is the graph one obtains from the response to the three simultaneous currents.

I wanted to show you the last of these simulations in which I have... this is only in time, but there is the spatial component. These synapses are in a remote position, they are many synapses in a remote position and I demonstrate with this the fact of attenuation and the fact of spatial summation. So at the distal dendrite, 414 micrometers, I am activating these synapses and I see these EPSPs which are very large; over there they are large and have these tails. While in black I see the depolarization of the Soma.
If I increase the frequency of these events — instead of 30 Hz, 50 Hz, 70 Hz, 90 Hz — I see that progressively the Soma is also rising, is maintaining a depolarization. Why? Because each new event arrives when the tail of the previous one is not yet exhausted.

There is a small *catch* compared to the single compartment thing of my salary, of the university paying my salary, and which you see that if this holds very well in dendrites, in the Soma, that there is a cable in between, things are a little different: there is an attenuation, it could also be that it fails to make the neuron fire. Ok, here in this case I make it fire. So here is an example of spatial summation put also with attenuation.

Again, I hope some of you can be curious and say: "But if I have a lower frequency, but I start to increase in a very remarkable way the amplitude of the synaptic input remotely, so I make these pulses become much larger... how come the neuron doesn't fire?". Sorry, I made a mistake. It is this slider. Here they fire. Ok, here it always fires.
So, at a certain frequency, a low amplitude, ok... for example like this, I don't know if it can or cannot add up for you, that despite me increasing the amplitude of synapses... ok, here in the end it is working, so ok, I cannot do it. What I wanted to verify, but I am not managing to find the combination of parameters, is that a phenomenon of **synaptic saturation** exists, whereby even if you increase the synaptic amplitude a lot, not necessarily on the Soma do you have a proportionally high effect. But this is another thing, it is not crucial.

***


## Shunting Inhibition and Dendritic Spines

This is the last aspect, which concludes this chapter, and it is a slightly counterintuitive aspect. If spatial summation holds, perhaps there are still a couple of slides later... if spatial summation holds, then if you imagine having excitatory and inhibitory synaptic inputs on the same arborization, on the same branch or on different branches, you could imagine that at the Soma a kind of algebraic sum of the effects could occur. So it is no longer in time and space; the neuron has evolved with these arborizations to maximize the surface area, but in fact, it is as if it behaved like a sphere.

The interesting thing is that in some cases there is not necessarily a linear spatial summation, but a **sublinear** (*sublinear*) summation. This happens due to the fact that synapses have a particular characteristic of being — perhaps I have already repeated it *ad nauseam* here — **conductance inputs**. So here it was written:
$$I_{syn} = g_{syn}(t) (V - E_{syn})$$
and then there was also $1/R_m \dots$ But it is this thing here that I dwell on. It is not that when a synapse activates there is an injection of current; the conductance also changes.

You can imagine it — and this is quite easy to imagine — that the opening of channels leads, in a cable-like structure, to leakage, thus changing the total transmembrane resistance. So this transmembrane resistance somehow here is multiplied by the synaptic conductance. If the synaptic conductance increases, somehow it changes or disrupts the transmembrane electrical properties: that is, it makes the membrane more *leaky*, less resistive.

This means that if you apply only, turn on only these three excitatory stimulus synapses (this is a *cartoon*, it is not an accurate simulation), or activate them when there is both excitation and inhibition... in this case you have killed the effect, you have remarkably reduced the effect of excitation. Which seems instead not to happen here, where you have two distinct *branches*. Here, if it were simply an algebraic sum of currents, here too you should have the same effect. When you turn on excitation and inhibition, the inhibition should subtract from the excitation, but the excitation is not subtractive. Excitation and inhibition are multiplicative (or rather divisive) inputs; here they multiply by the state variable, it is no longer current. If it is me injecting a current, then yes, it is a current that is algebraic, which sums if the current is positive or negative; but synapses are different, they have an effect of changing the conductance. 

This for example is seen here, it comes from a very important article from a few years ago, in which in particular inputs coming from the auditory cortex, or rather from the periphery of the auditory system, the researchers speculated that it was dependent on their simultaneous activation. If you have only inputs from the left ear and you have the activation of these two inputs, since this is a cable, you paradoxically have an opposite effect, there is no summation. If you want, the activation of this other synapse... imagine that this synapse really makes a current flow. The current going here, if this synapse activates, there are ion channels, post-synaptic receptors that are ion channels (for example ionotropic receptors) that open, so the current here finds a passage and exits as well as continuing, so it is much more attenuated.
This is a strange, counterintuitive effect. If instead I activate a synapse only from the left side and only from the right side, so like here, which are on different *branches*, I obtain for example an integration, a summation. I do not obtain this effect which is also called **shunting** (*shunting effect*). I believe that in electronics *shunting* or *shunt* means putting to ground, it means a connection... I believe cardiac shunt, help me out, means doing a kind of bypass. So *shunting* means that in the end it is as if I am putting to ground, I liquidate myself, I dissipate those electrical signals that otherwise would have invaded this part of this branch in a normal way, as expected.

There are two other things I want to tell you. The first is that normally, especially — or rather almost the totality — of excitatory synapses do not arrive in the dendrites on the cylindrical lateral part of the dendrite, but are established on a dedicated structure called **dendritic spine**. This is still a post-synaptic part and the synapse embraces this sort of knob, so it is more a grasping by the synaptic bouton of the dendritic spine rather than the synaptic bouton touching the naked body of the dendrite. There are also that type of synapses (inhibitory synapses are like that, some excitatory synapses are also like that, in the Soma there are synapses like that, there are also synapses that are on the axon, insist on the axon), but in the dendrites of pyramidal cells, of excitatory cells mainly, there is this structure.



[Image of dendritic spine structure diagram]


And this structure, even if we do not go into detail, has a characteristic of being a kind of small cable and having a bottleneck here, even if I don't remember anything about the history of the cable equation... I can think that here from the electrical point of view having a bottleneck, a very very small diameter, you can think of the space constant becoming very small, you can simply think that from the point of view of a remarkable narrowing I have a very high axial resistance here, because I have difficulty if I am an ion entering inside.

This object therefore functions as a kind of amplifier. If I inject a current here, since here the resistance is very high ($V = R \cdot I$, $R$ is high), $V$ will be very large compared to injecting a current on the naked part of the dendrite.

It has another effect: the fact that if there are calcium currents here, having made a compartment that is almost isolated (because this is a bottleneck where few ions can cross), when you go to measure this with optical methods — so the electrical potential comes... so this is the electrical potential, you see that it is around 1 mV, an EPSP of 1 mV — when you go to measure in time the calcium concentration, you see that inside the dendritic spine this goes up remarkably. Here it is a differential measurement, so it is not absolute, I don't know how many micromolar the calcium went up, but you see that in the dendrite, not far away, the calcium has absolutely not altered.
Again, if I have synaptic plasticity mechanisms, a structure like this allows me to localize only at this synapse some phenomenon that could be calcium-dependent. STDP plasticity, *spike-timing dependent*, has a remarkable dependence on calcium that is brought by the backpropagating potential (maybe), is brought by the activation of calcium, of calcium channels here (maybe), however in both cases the increase due to the activation of this dendritic spine is local for ionic concentrations. And what happens is that the depolarization is also very amplified here compared to what it would be if the synapse were on the same *shaft*.

## Active Dendrites, Calcium Spikes, BAC Firing and Rall's Model

Anyway, this is not only a *glimpse*, a look at this type of description, which again somehow uses tools that are linked to electrotonic conduction and cable theory. There is one last phenomenon that I tell you quickly before taking a break, it is linked to the presence of voltage-dependent conductances in dendrites, at a particular point of the apical dendrites of a particular type of cortical pyramidal neurons. So in this case the dendrite is not passive; if I wanted to simulate it I would have to modify the cable equation and do it numerically, and I can do it, you can do it in 5 minutes with NEURON and Python.

What has been seen experimentally is that, since at this point, far from the Soma, there exist voltage-dependent currents — in particular they are voltage-dependent calcium currents, and if you remember calcium currents are also like sodium: they activate... so first of all extracellular calcium is more abundant than intracellular calcium, so if there are channels that activate, they depolarize the membrane locally, like sodium (you lick the salty sweat), also here the calcium outside is very concentrated.
When an action potential backpropagates, somehow it interacts with this part and can generate what is called a **Calcium Spike**, a spike to calcium, because here there are conductances that create an additional depolarization. So it is as if in these cells there exist two sites of integration or spike emission: one is the somatic one — actually in the initial part of the axon — where they are sodium spikes, and another dendritic one where they are calcium spikes.

And you see in the blue trace, this blue one, the spikes are almost like cardiac action potentials: there is no inactivation, they are a thing, a "blob". This blob however is sufficient when it is evoked — so it is evoked by an action potential that backpropagates — and once there is this excess of this non-linear depolarization, if you want (because there is an opening of channels and an influx, an electrogenesis, that is a generation of an ionic current), this current obviously propagates both up and down and invades the Soma. And in some conditions this "ping pong" between Soma (when it fires it invades the dendrites) and dendrites (they activate and further by reflection generate a further cascade of depolarization at the Soma), well basically it could decide to fire further. 

What is called *Backpropagating Action Potential activated Calcium spike firing*, **BAC Firing**. Now I gloss over the fact of the interpretation as a coincidence detection mechanism between somatic and dendritic inputs. It suffices for me just to leave in your head — and we take a break in a few seconds — that if I have active conductances in dendrites, things can be much more interesting and complicated. In this direction there is a lot to do, to create a ping pong in which if by chance this manages to fire, so imagine that a certain threshold must be reached in the dendrites, and this is sufficiently... somatic firing is sufficiently capable, as repeated frequency (the famous tails), of making the potentials backpropagating in the dendrites sufficiently big — and one that sums in the tails of the previous one — to create this depolarization, here the soma activity by reflection persists longer.

So what normally would have been a *burst*, an emission of a single action potential, becomes a *burst* of 3, 4, 5 spikes. It is represented here, and here in fact there are no additional spikes, there is this little hump which is called *after depolarization*, a depolarization after. If this little hump were even greater there would be an extra spike. I have "for free", repeat, on the dendrite a mechanism that allows me to amplify — and on a single compartment I wouldn't do it — the firing frequency of a neuron. If I fire a sequence of 4-5 action potentials I will have 2 or 3 more, and these 2 or 3 more are due to the fact that I have this dendritic mechanism. I stop for 10 minutes. Thank you.

(Break... Brief summary of technical interactions... Resumption of the lecture).

This thing of electrogenesis due to calcium currents in the apical dendrites of layer 5 pyramidal cells causes conduction here, or integration, not to be linear. While before with the cable equation (which I continue to indicate there on the blackboard) in fact it is a little different, but it continues to be a capacitor, it is a capacitor both in time and in space. Where I see a differential equation with a derivative and at the second member... in the end it is the charge balance equation, but distributed in space: I think of an accumulator.

An accumulator, if I have an input — exactly the story of the tails of the university paying my salary — if the salary payment frequency were increased, maybe I would expect the response to be linear at a certain point. I don't remember if two weeks ago I showed you — I think so — also that in the dynamic **steady state** case, the amplitude of that rise and fall (if you take the peak, the bottom part, the middle part), anyway tends to grow proportionally with frequency, as one would expect. So the more frequent the activity is, the more I have these tails that... so a kind of *build up* on the previous tail is a behavior that in amplitude is proportional to frequency.

However, the fact that here, when you have a particular depolarization, here you have in fact... it is as if you had a generation of a spike, so from the point of view of amplitude, the fact of having this distal electrogenesis — so in points far from the Soma — causes the behavior that would be linear to become **supralinear**, explosive and even steeper. Because at a certain point, at a certain frequency, the action potentials backpropagating from the Soma to the dendrite count and count additionally, because I have here, only here, voltage-dependent calcium currents.

I show you this curve which was obtained also experimentally by **Matthew Larkum**, who is now a professor in Berlin and is — maybe I mentioned him before — is one of the first who was able in the '90s and 2000s to put multiple pipettes in the same dendrite and is a musician, he is a violinist, so the manual dexterity is obviously reasonable and it is understandable that it is from a musician. He showed that in fact in remote positions, this is the calcium concentration, integration seems to be not only supralinear but seems to be **sigmoidal**.

This thing of sigmoids... so it means that there is some, even if calcium... forget that it is calcium concentration, there is a sigmoid here and there is surely a sigmoid where there is the point where sodium-dependent action potentials are generated. I call it sigmoid here because I told you about the frequency-current curve. The larger the current, the more frequent it is. So there is a minimum value of the input current below which the output is flat, after which it starts to rise and then somehow saturates (see the absolute refractory period); it doesn't continue to go to the stars, at a certain point it saturates.

In the context of *machine learning*, the more biological primitives you have similar to sigmoids, the more you have what *machine learners* call representability power (*representability*) for a series expansion of kernels, of elementary functions that are non-linear, sigmoids. So this thing here has aroused much interest because the fact that there is a supralinear summation could indicate that this object behaves as two units, not one. It is not a perceptron where things sum here and emission occurs here. Dendrites sum, here there is an integration, so a threshold, and the result of this threshold operation — so in the end the perceptron is only in the active dendrites — this is fed to the soma and for example to the integration it does on proximal dendrites and somatic inputs.

So all this could make one think: "Wait, if you thought the brain worked like... not a *Transformer*... but like a collection of sigmoidal units like *deep learning*, *deep architecture* due to the hierarchical characteristic of the various *layers*, the single units are neurons... it could also be that a neuron has as equivalent model, as equivalent concept, two units, not one".
Also I mention that there is a very interesting study from several years ago, picked up several times and expanded over the years, by a very smart researcher who is in Crete (it is not the University of Crete, it is the FORTH institute), her name is **Panayiota Poirazi**, who demonstrated that if you take a neuron with a cable, so spatially extended, this does the same thing as a network of perceptrons with a hidden *layer*. So one neuron alone, with many inputs, is a neural network, a neuron.
In recent times, **Idan Segev** — the same one I mentioned to you — has published a very interesting article in which, again, he says a neuron is equivalent to a *deep* network. Then what this means for cognition, memory, for emotions, for consciousness, this is another discourse; at least it seems to suggest that from a biophysical point of view there is a substrate that "resonates" computationally with something we are starting to — if nothing else — exploit (understand is a big word in the case of machine learning, we don't always understand why it works). Anyway, we do not treat this topic.

I just want to make you think that when there is an arborized structure with *branching* points, ramifications (I don't know how to say it in Italian, "i branching", ramifications), the possible solution of the cable equation is absolutely possible provided to connect... in this case you see that here the radius of these two children is different from the radius of the father dendrite, you must connect the boundary conditions saying that here there must be a conservation of current, so the outgoing or incoming current here must be the sum of the other two. This thing works very well and is very simple to implement in a numerical simulator like NEURON's.
It is demonstrated that there exists some type of relationship, some type of coefficient that depends on the relationships of the radii, the radii of the child *branches* compared to the radius of the father *branch*. And if they are in a particular relationship, which I am not going to comment on... if the father is equal to the sum of the children regarding the radius... actually it is the cube root of the radius must be equal to the sum of the cube roots of the radii of the children, because it is so:
$$d_{father}^{3/2} = \sum d_{children}^{3/2}$$
**Rall** demonstrated that a model in which you have a single Soma and a single Dendrite is equivalent from the point of view of sitting at the Soma — attention: only from the point of view of sitting at the Soma — to a model instead arborized as much as you want, provided the ratios of father and son radii (even multiples with many ramifications, it doesn't matter) provided this law works, with a complicated arborization. So if I have a synaptic input on this *branch*, in this model that looks complicated from the geometric point of view... if this holds (and more or less in the biological case this holds), I can calculate at the soma the same effect that at equal linear distance the same synaptic input would have in a model where I have a *ball and stick*, I have a single dendrite.
Clearly, the neuron is real, there are a lot of dendrites, there are a lot of arborizations, but I do this almost with my eyes closed. While the arborization I have to think about the connection etc. If geometry supports it, then there is an equivalence. This is what is called **Rall's Model**. So when we talk about Rall's model we also talk about the *ball and stick* model. Ball and stick.

Ok, out of the way this part of the spatial description of the properties of the single neuron.

## Volume Conduction and Current Sources

Note: this was essential because I need to know in every point of the neuron's morphology what the currents and potential are, or at least the currents, because intuitively I told you that I can understand what happens to the extracellular space treating it as a **volume conductor** only if I manage to formalize and understand current sinks and sources. So I need, given a neuron messy at will, to have its description. It is true, intracellular, but after I have done this intracellular description, what remains is essential to me with these currents to say to an extracellular point, in this point here, what the extracellular potential is.

And obviously, it is evident that it is affected from all parts of a neuron's morphology. If I change that point, move it, move it from inside the cortex, put it on the skin or even maybe a few centimeters from the head, ok, something must change: the distance between sources and sinks and the point where I measure the extracellular potential must change.

So now I return and conclude, I hope, the story of extracellular signals. This part, as said, is essential; this other one allows me to characterize in every point of space... in this case you see the case where I have a neuron, but you can think — and people have done it in recent years, this textbook is a nice example of the culmination, of the success of these studies — not a single neuron, but a network of neurons, even 100,000 neurons, and in points of space far away go to characterize what is... to say what the extracellular potential is.

Why does it make sense to do it? Because most of the time, if I have an electroencephalogram (EEG) electrode, I put it even on the skin, very far away. If I have an electrode measuring **Local Field Potentials** (LFP), I put it maybe a bit closer, but I certainly don't put it in the belly, inside, not in the intracellular electrode. If I have a metal electrode near the Soma and I measure spikes, I measure the extracellular correlate of action potentials, it might make sense to try to say: "But what am I measuring? Am I actually measuring spikes, am I measuring synaptic activity, what am I measuring?". Given that it might not be so trivial to infer the potential at a point given a distribution of currents.

Now I tell you why I am talking about current distribution and where this $4\pi$ factor comes from and where this $\sigma_T$ comes from which is the conductivity of the extracellular medium, the transmission current. So this modulus of $R - R_n$ is the distance in modulus referred to the same system, a Cartesian reference system of the single sink or current whose value is this $I_n$.

However at the beginning of the course, now several weeks (time flies when one is having fun), I told you that at a certain point $P$ the potential is given by a weighted sum of charges where the trend is 1 over the distance. And then there was in the middle a term that was $1 / 4\pi \varepsilon_r \varepsilon_0$, because it came out from the definition of potential, so this had to do with the definition of Coulomb force, of electric potential and of potential that was... whose gradient, minus whose gradient, would have given me, would have led me back to the expression of the electric field. 

Well, I lied to you. Or rather, I lied to you for spatial scales like these. At the beginning of the course I wasn't lying to you: we were considering microscopic electrical behaviors across the membrane and there yes this holds. But at a particular distance it is not that these cells are in a vacuum; they are in an aqueous solution and these objects which are water molecules have a charge distribution that is not with the same center of gravity. The fact that there are two hydrogen atoms and one oxygen atom and they are placed in this way (I think the angle here is 105 degrees, they are reminiscences, things I should remove from my head) causes there to exist a center of mass of the positive charge different from the negative charge.

So I can think — as it is, as it happens — that these **dipoles**, which are called dipoles, these water molecules, go to screen... suppose so $H$ is positive, these blue spheres are negative... go to create a hydration layer neutralizing the charge. So this formula, although correct, is not easy to use for macroscopic distances. Macroscopic means not nanometers, but micrometers, tens or hundreds of micrometers, a few millimeters if I go outside the cortex. Not that it is wrong per se.
I have another slide where there are three sketches that should further clarify this point. It is not that I lied to you: in that context, it was appropriate. So in the case where we move to macroscopic distances I should consider the effect of these... shielding, of **shielding**, water dipoles. And this modifies this expression in a known way with exponentials for each term, so it is not only the dependence 1 divided by the distance of the charge. Here is the point where I want to calculate the potential, this is the distance, 1 divided by the distance is not enough: at a certain distance a term $e^{-r/\lambda_D}$ also holds, a certain, if you want, space constant that is called **Debye radius** (Debye length), a *shielding* distance. And it is of the order of a nanometer.

You can imagine therefore that if I am under the nanometer, ok the exponential stays a little, but if I am at some ten, hundred micrometers, this is already gone, it is at zero, so there is nothing left, I don't see anything anymore. I am so distant that the water dipoles have neutralized any effect. So: one, if I am at distances greater than the Debye radius, I forget about using this treatment.
Furthermore, I must forget a treatment that has to do with charges if I am considering times, temporal scales, that are of a few milliseconds. Because I remind you that when we considered, defined the mobility of an ion in an aqueous medium, we wrote the law of dynamics in which friction, let's say, combined with the mass of an ion, generated a transient solution that we said: "Look, basically this is always at **steady state**". But what was this particular time? It was of the order of a nanosecond.
So a nanometer (this Debye *shielding* distance) and a nanosecond. Basically this description, beyond this description, is no longer adequate and I must convince myself, I must accept the fact that in these conditions, at this time scale, at this scale of times, the extracellular medium is isopotential but is neutral, **electroneutral**, and therefore isopotential.

This is not that I lied to you: at the nanoscale of the membrane and channels this held, because I was potentially interested in channel currents that... so the crossing time of an ion inside a membrane channel is not so much larger than a nanosecond, it could be lower, and from the spatial point of view I am not above nanometers, if you remember that the membrane is a few nanometers thick. But at a microscopic and mesoscopic scale (here he calls it macro, but in my opinion, it would be better to call it meso, but this is a fact of philosophy) this description no longer holds.
So much so that at the microscopic scale we started talking about concentration, density fluxes or concentration, we started using electrostatic formulations, Nernst, etc., ionic fluxes, ohmic and non-ohmic, equation of... I don't remember anymore what the guy is called who replaces the ohmic formulation of a current inside a membrane channel... Goldman, Goldman equation.
If I go even further up on an extended scale — so where I am seeing not the detail of the membrane channel, not the detail of a space very close to the membrane, but I am seeing several neurons, I am at a few micrometers — what counts are the current sources, sources and sinks of current.

In the previous lecture I introduced you intuitively, saying: if there is an excitatory synapse here that opens, sodium or calcium ions enter inside, empty the extracellular medium of positive ions. So this suction current is probably remarkable, important for me to understand, simply because I think of Ohm's law. I have this idea that $V = R \cdot I$, so if I have some current somewhere and I have an electrode and I see a potential, maybe I am seeing the echo of that current, where $R$ is the resistance of... which will vary with distance, linked to the ohmic properties of conductivity of the extracellular medium.

So it comes in the end one of the results of relatively recent activity, of studies relatively recent over the last 15 years, 20 years, is that extracellular potentials can be understood, in very simple terms, in the distribution of currents, because in fact I am assuming that there is charge conservation and therefore Kirchhoff, current conservation.
This is what we have already highlighted. In the case of a morphology, at this point, it must be extended, because you remember with a *Point Neuron*, a point neuron, I had no way to see this closure of currents that, if somehow occur extracellularly or intracellularly, must close the circuit. So it was fundamental to be able to discuss and have now a tool, even if numerical (because maybe given the complex morphology and given the distribution of even active conductances in the dendritic tree, leaving aside even just the axon, which is a cable with active electrical properties), I need to have this description telling me how currents evolve and how they exchange. If these currents so far away, when for example there is an action potential here, there is an echo... but this echo for reasons we have now dissected and are all in there in that cable equation... but intuitively it continues to be a thing whereby here the membrane continues to be *leaky* and the current disperses, so the membrane potential changes and so the current changes.
If I could know how I have, how I know (because once I have solved the cable equation I have the currents: if in particular they are *leak* currents it suffices to know what $g_{leak}$ is and the *reversal* potential, if they are active currents I will need also the state variables but I have them inside, at least inside a computer simulation), the theory of volume conduction allows describing even on the scalp the distribution of the extracellular electric potential.

And it has assumptions. The assumptions are:
1.  That all currents and potentials are **known**, or are measured (they are known in the case of a synthetic, modeling approach like the one I sold you up to now, or at least they can be measured). Clearly, it is unthinkable to think of being able to measure every single neuron with many pipettes (not even if Matthew Larkum were here could he do it with lots of pipettes in all neurons of a piece of cortex where there are millions of nerve cells). But this is the assumption: I know them and I know them through cable theory, cellular excitability, synapse localization, everything we have done so far in this course.
2.  This is another very important hypothesis: the fact that there is a certain extracellular potential at a certain point **does not influence**, does not change the intracellular potential of a neuron that is nearby. What in literature is called interactions, would otherwise be called **ephaptic interactions**; in other terms, if I am a pyramidal neuron, I am "lanky", I have my dendrites etc., and I have one next to me, the fact that I am perturbing — because for example I have a synapse that activated here, so here there was some current — I perturbed the extracellular space, here there is a particular value of extracellular potential. The other is a neuron standing next to it, in theory, it should a little, should feel it. I can only invoke the fact that the neuron behaves a bit like a Faraday cage and so probably the inside is shielded, but if I were to talk to one of you on a particularly "no" day, maybe you would tell me: "But what the heck are you saying? When you have channel currents, you have $\Delta V$: one is the intracellular part, minus the external reference, which I cannot think is isopotential, because you yourself tell me that in different points it has a different extracellular potential". The only thing I could say in my justification is that probably this effect is negligible. It is not during — it is thought — during pathological synchronized activity like some **epileptic seizures**, in which you have a block of cerebral cortex that starts to be synchronized and so there is from the extracellular point of view — simply by a sum, a resolution of effects — a huge quantity of a *build up* of the external electric field and it is thought that that too contributes to trigger further crises or to the persistence of this crisis. But for this theory, there are no ephaptic interactions. This in the end is an interesting thing because it says: "Did you do the cable simulation? Sure? Just give me the data you recorded. I don't want to do calculations again, you give me data, then I deal with putting these currents, doing the *playback* of these currents in different points of the morphology, but it is not that then I come inside and say: wait look you have to change the potential, there is an extra spike". No, what you did, what is called a **forward** description, because it is invoking the linearity of Maxwell's laws, of electromagnetism: if I have sources I can predict $V$, but I don't go back, it is only *forward*, it is only one direction, I don't go back. So, since it is $0V$, does it influence the cable equation dynamics? No, it is only in one direction and works well enough, the reason why I tell you about it in class.
3.  Another hypothesis is this: the brain tissue is a **continuous medium**, ok, it is what is in the title being a volume conductor, conductor of volume. And in particular, the conductivity of the tissue is **uniform**, that is it does not change in space, it is constant in time and is **isotropic**, that is in all directions if I speak of currents it is the same. And you could be dissatisfied and say: "But how? You broke our boxes (bothered us) that pyramidal neurons are all aligned, you sold it to us during the first two or three lessons, of the fact that when yes, in particular as also in this context (there it was qualitative), potentials are measured from the scalp, EEG or Local Field Potential, it is affected by this geometric organization". So here, just like in magnetic resonance (which perhaps some of you will have overheard), the fact of having... here they are not fibers, they are not axons, they are not nerves, they are not projections from one hemisphere to the other... however in this direction there are pieces of conductor that are placed aligned, so this thing of isotropy might not be so verified. And also the fact that it is a continuous medium, I don't know exactly what it means. That is I know what it means here in the extracellular space, because I imagine it as a kind of bathtub where there is me who am a neuron but next to me there is only salty water. Here there is a mess of other types of cells, fibers passing, so currents — as indeed Rall had also intuited — currents pass also inside the tissue. So what probably happens (and people are starting to characterize and study it) is that this medium is continuous but the conductivity value is not the value you would have taking a little extracellular cerebral fluid and measuring it: it is an **effective** value. With the fact that there is a particular tortuosity, with the fact that for example an ion does not have an easy life passing, it is not a *bulk*, it is not a bath in which I move in all directions easily. But this goes beyond; these are considerations to try to keep a critical spirit in you, not to make you drink everything, or at least be very *aware*, very conscious of the hypotheses behind it.

And for all these considerations at the macroscopic scale something similar to **Ohm's Law** holds. The medium is ohmic, here too, but signals are perhaps... and if they change in time, dielectric properties — that is Maxwell works beyond the fact of electromagnetic propagation, so radiofrequency — whereby maybe signals are not so fast as to generate electronic fields. It is difficult that we can read each other's mind because there is a propagation of radio waves between me and you, because signals vary slowly. But anyway they vary slowly and have to do with dielectric properties, of capacitance. These for the moment I do not take into account and I will not take into account in the continuation.

I want to explain to you why this expression comes out, where it comes from, why I started reasoning about something that is inversely proportional to the distance at a certain point. Clearly I can do it for different points; it will be interesting to do it for points near the Soma during spiking activity. Do I see a spike? And if I move away do I see the spike attenuated? Repeat, this is in the extracellular ambit, so I expect that if one of you speaks and I move away a lot, if the medium is ohmic, the resistance is greater, quantities will be attenuated. But does the shape change? So this I will move, the point where I measure the extracellular potential I will move abundantly.
But why does this depend on $1/r$ in the end, 1 divided by the distance from these sources or sinks? And why is there this $4\pi$ at the denominator? And why is there the $\sigma_T$ which is the conductivity of the medium for transmission currents?

To do this I remind you, because probably you have seen something like this in an electromagnetism course (if you took it), what Ohm's Law is in a conductor, not in a wire, not in a one-dimensional structure. So not in a lumped structure and I consider this thing of point sources. Here I am taking a small cube and anyway I treat for simplicity the one-dimensional case. I point out to you that normally the constitutive equation for a resistor is written in this way: the potential drop between this point and this point is $V_a - V_b$. So when you take it like this, then the current $I$ is equal to this $V_a - V_b$ divided by $R$.

In this case here — and it will be clear why, because I am obsessed with the incremental ratio, because nature intrinsically comes to be described by differential relations (the reason for this, as said other times, I don't know, it is deeper) — if I take a small cube in space and think of taking this as point $x$ and this as point $x + \Delta x$, if the cube has a certain side of measure $\Delta x$, I can instead... instead means I simply have to pay attention to a minus sign, but for the rest nothing, only I warn you that you might be confused now for a millisecond... I write $\Delta V$ as $V$ here minus $V$ before. Repeat, I want to do it because this sounds to me like it already prepares me for some differential expression. Here, shortly, I am sure, I am confident, I will start writing $dV/dx$ or $dV/dr$, the gradient in space. And parenthetically the gradient, so the derivative in space, came handy because I knew how the electric field was written. So if Ohm's law is in a volume conductor, maybe I will have something that is not exactly $V = R \cdot I$; maybe instead of $V$ there will be the electric field. Now we see it.

So written like this, taking this $\Delta V$ like this, the current must be taken with the opposite sign, because the current is positive when it flows in this direction, taking $V_a - V_b$ (before versus after). It would have been before versus after, but I took exactly the opposite, I must be consistent; I can do what I want, but I must adapt the constitutive equation if I changed the convention. So instead of writing $I = \Delta V / R$, I describe it in terms of conductance which I like more, but with the minus sign.
So this is the first thing: ok, $I = -\Delta V$ is proportional to the current through conductance. And $I$ in the end (I am thinking, dealing with a volume, it is not a lumped conductance, but it is the result of a property of space, which I call conductivity, $\sigma_T$ of transmission) multiplied by the passage surface divided by $\Delta x$.
I remember it because it is the inverse of the story of resistivity: resistance is typically resistivity times length divided by the passage section. This I don't know why I remember it better than that... because I understand that the longer a wire is the more resistive it is, and the wider it is the less... sorry, the more resistance is high at equal resistivity, and the larger the passage section, since it is at the denominator, means resistance is low. Here it is exactly the inverse of that, but I don't know why, I don't remember, I should but I don't remember.

With this geometry this is the value of conductance and you see: finally I am happy because I have this $\Delta x$, $\Delta V$ and $\Delta x$. I make $\Delta x$ tend to zero. Before doing it, if you want, I can write the current $I$, which here would have been a total current, as a flux, so a current density if you want. That is this is the total current and if I know the current density I have to multiply it by the passage section. I do it because I put myself in conditions where this $S$ goes away. In the end I assumed, because I remembered the capacitor formula, that a total current existed, but it is easier that I can describe in the medium, as also in the case of a neuron, current densities, currents per unit surface.
So writing big $I$ as $J \cdot S$ (here $I_t \cdot S$), I cancel $S$ from both members, I can do it as much as it is not null, and what remains is that the current is equal to minus sigma t, the derivative or the gradient of $V$ with respect to $x$, with the minus:
$$J = -\sigma_T \nabla V$$
So this thing of the minus comes necessarily from this choice linked to the definition of the incremental ratio. But since there is a minus for the gradient, this thing of minus gradient is the **electric field**, it is exactly the definition for a conservative force field of potential. Potential is a function such that if I take the gradient with the minus I get the field. So, ok, written in an elegant way in a three-dimensional case where writing the gradient, but what I wanted to say is that in the end this minus gradient is the electric field. This is Ohm's law in the case of a three-dimensional volume.

Simply to show this and continue reasoning in the end with this, in the end with this expression, this is enough for me. So, what I do now is take a point current and calculate a point current density and say that the total current, taking a sphere of given radius, so at a certain distance, is the effect one has multiplying the transmission current density, the passage current, by the surface, the surface of a sphere $4\pi r^2$, the external surface of a sphere.
So the current $I_0$ due to a generic current density $J$... again I here unfortunately reason with a current density, but if I have in the case of a cable, of a neuron, or the (I pull it down), these sinks or these sources might need total currents, because there they are total currents. So I basically substitute $J$ ($I_t$), in this equation here, $I_0 / (4\pi r^2)$.
Another way to see it is: if I have at this point a total current $I_0$, for example due to this sodium channel opening or closing or doing what it wants, to put it in relation with density, with a density, I must divide by what is the passage surface, since this current presumably expands in every direction (since the medium is isotropic) at a certain distance $r$. Here is where this sphere comes from, here is where this $4\pi$ comes from.

Written like this, I am writing $J = -\sigma_T \nabla V$ (derivative of $V$ with respect to $r$). I integrate both members. This is easy because the primitive of $1/r^2$ is $-1/r$; if I take the derivative of $1/r$ it is $-1/r^2$, so this part here is very easy and immediately the term $1/r$ comes out, it is no longer $r^2$.
How did it work in the end for charges? Potential seems not to vary as $1/r^2$, with the inverse square, but with the inverse of distance. And from this part here instead it is easy because it is an exact differential, so it is all how I take the integration extremes. I choose integration extremes between infinity, a point at infinity and a point at $r$. Why do I take at infinity? Because at infinity I can say in the *bulk* that I have the reference electrode where the potential is by convention zero. Making this choice comes very handy, but any other choice would have been possible, because the primitive here, $1/r$ or $-1/r$, when I calculate it in $r$ is ok, whatever it is. When I calculate it at infinity, it means actually a limit process, I have 1 divided by something going to infinity and 0, so this term also simplifies. And I have the expression of the potential, apart from dividing by $\sigma_T$ which goes to the denominator, and it becomes plus and plus on both members.
$$V(r) = \frac{I_0}{4\pi \sigma_T r}$$
So this thing of the minus in the end was necessary to rediscover that expression where somehow it told me that at this point $V$ testifies to me how intense the current is there. If a current is presumably a source, so it is spitting something spitting positively in my direction, I become more positive. So if I hadn't done this, if I had made a mess there, I would have probably had a minus and this thing would have been strange. But how? That current is spitting positive ions at me, it is a positive current, in the electrotechnical convention of positive currents potential must increase and varies with the inverse of distance.

By linearity of the medium, if I don't have a single source, but I have a set of sources, and in this case I am thinking finally of discretizing a piece of neuron morphology complicated as you want, I discretize it as a sequence of very small point current sources. You could object to me that in fact... and here it is all a cable, why did they become point-like? Is it not by chance there is a slightly more accurate description? In the end they are small cables, they are small cylinders: is it not that there is a description where instead of a description on point currents it holds with line currents?
You would be right with this intuition, but we see it probably next week. In this case, every single element has the effect at a certain distance, an effect that superimposes, so the overall effect (I wanted to say by superposition of effects) the overall effect is the superposition of effects that single elements taken individually would have at that point. So given that each element is the current divided by $4\pi \sigma_T$ divided by distance, for distance, you have that the potential due to a series of... for example to a cable, a series of point sources or sinks, is the weighted average of currents weighted by distances $1/r$.

So, we see it now and it is the last thing I tell you, before the end. A better approximation is that of **Line Sources**. To understand this thing here you should or will have to spend 15 minutes looking at how I chose these axes. They are slightly different from how the choice is in the textbook I recommended to you and in fact represent the contribution of a small cylinder (in the end a cylinder that will become infinitesimal) and on a point $P$ putting however this cylinder in such a way that its end coincides with the zero point of this system of Cartesian axes. Don't ask me why in the book they didn't do like this, for me this way is easy, more or less easy to understand.



[Image of Line Source Approximation Diagram]


What I do is: I have a cable of length $\Delta L$ (for the moment $\Delta L$ is finite, not infinitesimal) and I call this coordinate $x$, I call this point $L$ and I ask what is the effect of this... this is a point current because it is a little ring on what is a whole cylinder full of currents. With all these little arrows I am assuming that each of these rings, in a process then of limit integration, of integral, each will contribute in its own way at this point. This one here contributes on this point given its distance.
For how I chose the axes this is easy. This distance is $x - L$, because this is $x$ and this component is $L$, squared is Pythagoras, this, plus $y$ squared under root. This is this hypotenuse, this distance here. And once you have done this, in the end it is a walk in the park, so to speak, because the contribution of this current ring on this point, this current ring I call $\Delta I$ and it is basically proportionally the part that depends on $\Delta L$ on $dL$ on $\Delta L$. If this is a tenth of the length, the current term will scale as a tenth of the total, there isn't much else to understand.

And at this point, again I apply the definition: I have $I_0$ times $dL$ divided by $\Delta L$, which is the part of current, divided by $4\pi \sigma_T$ times the distance. It is ugly as an expression, but I am pleased that there is $dL$ at the numerator, because it tempts me to say: "Ok, this is the contribution of this $dL$. If I want to have the sum of you, so the collective effect, the superposition, and therefore have the total extracellular potential, I have to do a summation, or in the continuous case an integral".
That is I have to take this quantity here, integrate it from $-\Delta L$ to $0$, so making these little rings move in such a way as to cover all this length. And in fact it is... so $I_0 / \Delta L$ I can bring outside, this $4\pi \sigma$ here I bring outside; what remains is 1 divided by this square root of $(x - L)^2 + y^2$.

The integration variable is only $L$, so in theory if I remembered integrals I could say: "Ah, this is a known integral, 1 divided by the square root of variable $t$, $t^2 + a^2$". I honestly didn't remember this, I went to look at it and after that I verified that if one takes the logarithm of the absolute value and takes the derivative one obtains this quantity.
Remember: logarithm is "1 over" and then this is a term, a composite function, so you have to do the derivative of the sum and the derivative of the root with the rule for polynomials. But beyond this thing here — which honestly, given this thing here, particularly with Wikipedia, ChatGPT, trust or don't trust, try to do it yourselves pen and paper to see if this is truly the solution of the integral, just do a derivative — given an expression even ugly like this, somehow you could at least say: "Ok, somewhere there is some mathematician who 500 years ago already did this integral".
You can write in this way a new expression, which I am not going to comment on much, which is ugly compared to the other. The other was simple, it was for a point current term, it was $I_0 / 4\pi \dots$ it was only this, where here there was the distance. When you consider a current line, a line source of current, you have dependence on space that is linked to the logarithm of this ratio with these roots. It is a mess (*schifezza*).

I close by making the comparison. If you have the description of the field of a source — so in the case of a real neuron, so in the end it is an integration on many points of the membrane — a point current or this line current, you see that the difference is noted particularly at short distances. Here I am writing with this *shading* of color the description of the extracellular potential at a distance of the order of 10-50 micrometers. I just show you the slide after: here you see that it is a bit different, here it is more circular, here it is more elongated. So in theory one says: "Then there is a benefit because it is different on larger distances".
So distances of the order of 500 micrometers... I challenge anyone to compare these isoclines, these extracellular isopotential curves: practically there is no difference in having used one way, the simple one of point currents, or this other way of line currents.
And so the group of **Gaut** and **Einevoll** [Gaute Einevoll] made this comparison and said: "I show you what the difference is, the error you commit if you use point sources instead of line sources". The error you make is of the order of a tenth, it is already small, when you move — so maybe it is not negligible a tenth — when you move is of the order of 20-25 micrometers. But as soon as you are around 50 micrometers, the error is of the order of a hundredth. So in theory one, depending on cases, could say: "But if I am measuring a signal that is 100 microvolts, ok, I make an error that is a hundredth. And amen".

I stop here. See you next week which should be the last lecture. Thank you all.