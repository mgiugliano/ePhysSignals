## Markovian Stochastic Formulation of Excitability

### Introduction to Biophysical Non-Determinism

The idea I want to revisit is to conclude the part on **neuronal excitability**, pushing beyond the minimal and deterministic description introduced so far, to embrace a non-deterministic, or **stochastic**, description. I have emphasized multiple times that excitability is not a continuous, distributed property of voltage-dependent ionic permeability of a membrane, but is intrinsically linked to the discrete and microscopic nature of **ion channels**. When something is small, especially in a biological environment at physiological temperature, phenomena like **thermal agitation** dominate, and the behavior is no longer purely deterministic.

### The Markovian Kinetic Scheme for Ion Channels

Let's return to the fundamental concept that an **ion channel** is nothing more than a system that jumps between distinct states, for example, a **Closed** state and an **Open** state. These transitions are governed by probabilities, giving rise to a **Markovian kinetic scheme**.

Imagine a single channel, where the state variable $s_i$ is binary, $s_i \in \{0, 1\}$, where 1 indicates the Open state ($A$) and 0 the Closed state ($B$).

* $s_i = 1$, if the state is $A$.
* $s_i = 0$, if the state is $B$.

The probability $P_A(t)$ of finding the channel in the open (conductive) state at time $t$ is correlated with the transition dynamics.

The transition between these states is governed by the rates $\alpha$ (from closed to open) and $\beta$ (from open to closed). In a small time interval $\Delta t$, the transition probability is:
$$Pr\{A \to B \text{ in } (t; t+\Delta t] / \text{state } A \text{ at } t\} = k_1\Delta t + O(\Delta t)$$
$$Pr\{B \to A \text{ in } (t; t+\Delta t] / \text{state } B \text{ at } t\} = k_2\Delta t + O(\Delta t)$$
where $k_1$ and $k_2$ are the transition rates, which in the voltage-dependent case are $\alpha(V)$ and $\beta(V)$.

### Stochastic Simulation: The Random Number Method

How can we simulate this microscopic dynamic? We need to generate a random event. We assume we can generate a **pseudo-random** number $r$, uniformly distributed in the interval $[0, 1]$.

To simulate the transition $A \to B$ in $\Delta t$, the condition is that the event occurs if the random number $r$ is less than or equal to the transition probability in that time window:

* If the channel is **Open** ($s_i=1$): the transition to Closed occurs if $r \le \beta \cdot \Delta t$.
* If the channel is **Closed** ($s_i=0$): the transition to Open occurs if $r \le \alpha \cdot \Delta t$.

This mechanism produces the typical **"flickering"** signal of the single channel, which is the direct manifestation of stochasticity.

## From Single-Channel Probability to Population Equation

### The Markovian Derivation (Master Equation)

Let's focus on the occupation probability $P_A(t)$ for a single channel. The event of being in $A$ at time $t + \Delta t$ can only happen if:
1. One was in $A$ at time $t$ **AND** remained in $A$.
2. One was in $B$ at time $t$ **AND** transitioned to $A$.

Using the definition of conditional probability and considering that $P_B(t) = 1 - P_A(t)$:
$$P_A(t+\Delta t) = Pr\{A \text{ at } t+\Delta t / A \text{ at } t\} P_A(t) + Pr\{A \text{ at } t+\Delta t / B \text{ at } t\} P_B(t)$$
Substituting the transition probabilities:
$$P_A(t+\Delta t) = (1 - k_1\Delta t) P_A(t) + k_2\Delta t (1 - P_A(t))$$
Rearranging and dividing by $\Delta t$:
$$\frac{P_A(t+\Delta t) - P_A(t)}{\Delta t} = -k_1 P_A(t) + k_2 (1 - P_A(t))$$
And taking the limit as $\Delta t \to 0$, we obtain the differential equation that governs the temporal evolution of the occupation probability, known as the **Master Equation** (or in electrophysiology, the form of the Hodgkin-Huxley transition rates):
$$\frac{dP_A(t)}{dt} = -(k_1 + k_2) P_A(t) + k_2$$

### The Expected Value (Ensemble Average)

This result is remarkable because it demonstrates that the equation describing the **probability** of finding a single channel in a given state is **identical** to the deterministic differential equation describing the **fraction** of open channels $n(t)$ in a large population!

Indeed, let's define the **expected value** $\overline{x}(t) = E\{x(t)\}$ for the binary variable $x$:
$$\overline{x}(t) = 1 \cdot P(x=1) + 0 \cdot P(x=0) = P_A(t)$$
Thus, the expected value of the microscopic state variable is exactly the occupation probability, and since the equation for $P_A(t)$ is the same as the deterministic equation for $n(t)$:
$$\frac{d\overline{x}}{dt} = -(k_1 + k_2)\overline{x} + k_2$$
The **Ensemble-average** is equal to the deterministic description.

## Variance: The Essence of Channel Noise

### Quantification of Fluctuations

If the deterministic description captures the population average, it is **blind** to the fluctuations and **flickering**. The noise resides in the **variance** of the state variable.

The variance of a random variable $x$ is defined as:
$$Var\{x(t)\} = E\{(x(t) - \overline{x}(t))^2\}$$
Since the variable $x$ is binary ($x \in \{0, 1\}$), $x^2 = x$. The variance simplifies to:
$$Var\{x(t)\} = E\{x(t)^2\} - \overline{x}(t)^2 = E\{x(t)\} - \overline{x}(t)^2$$
Substituting $E\{x(t)\} = P_A(t) = \overline{x}(t)$, we obtain the formula for the variance of a Bernoulli variable:
$$Var\{x(t)\} = P_A(t) - P_A(t)^2 = P_A(t) (1 - P_A(t))$$
If we use the notation $\overline{x}$ for the open probability, the variance for the single channel is:
$$Var\{x(t)\} = \overline{x} (1 - \overline{x})$$

### The Variance of the Open Channel Fraction ($n$)

Recall that the macroscopic current $I$ depends on the fraction of open channels $n(t)$:
$$I = \overline{g} n(t) (V-E)$$
where $n(t)$ is the average of the individual state variables, $x_i$:
$$n = \frac{1}{N_{\text{tot}}} \sum_{i=1}^{N_{\text{tot}}} x_i$$
Due to the property of statistical independence among channels, the variance of the average is the sum of the variances divided by $N_{\text{tot}}^2$. Thus, the variance of the fraction $n$ is obtained by dividing the sum of the individual variances by the square of the total number of channels:
$$Var\{n\} = Var\left\{\frac{1}{N_{\text{tot}}} \sum x_i \right\} = \frac{1}{N_{\text{tot}}^2} \sum Var\{x_i\}$$
Since all channels are **identical** and **independent**, $Var\{x_i\}$ is the same for all, and the sum is $N_{\text{tot}} \cdot Var\{x\}$:
$$Var\{n\} = \frac{N_{\text{tot}}}{N_{\text{tot}}^2} Var\{x\} = \frac{\overline{x} (1 - \overline{x})}{N_{\text{tot}}}$$

This is the crucial expression that tells us that **fluctuations are inversely dependent on the total number of channels, $N_{\text{tot}}$**.

## Biophysical Consequences of Channel Variance

### Inverse Dependence on Channel Number

The expression $Var\{n\} = \frac{\overline{x} (1 - \overline{x})}{N_{\text{tot}}}$ explains the intuition that "the larger the choir, the less you hear the off-key voices."

* **$N_{\text{tot}} \to \infty$:** If the number of channels is very large, $Var\{n\} \to 0$. The fraction of open channels $n$ approaches its expected value $E\{n\} = \overline{x}$ (the average activation) with very high probability (this is the **Central Limit Theorem**). In this regime, the deterministic description is perfectly adequate.
* **$N_{\text{tot}}$ small:** If the number is small (e.g., in dendritic **hotspots** or the **axon initial segment**), the variance is large and fluctuations are significant. These fluctuations can induce **spontaneous spiking** or cause **jitter** in *spike timing*, as observed by **Mainen and Sejnowski**.

### Functional Dependence on Potential ($\overline{x}$)

The variance also depends on the average fraction of open channels, $\overline{x} = P_A$. This function $\overline{x}(1 - \overline{x})$ is a downward-concave parabola, which peaks at $\overline{x} = 0.5$ and is zero at $\overline{x}=0$ and $\overline{x}=1$.

Since $\overline{x}$ (the asymptotic value of $n$) depends on the membrane potential $V_m$ via the activation curves ($\alpha$ and $\beta$ are voltage-dependent), it follows that **the intrinsic channel noise depends on the membrane potential state**.

* **Extreme Hyperpolarization ($V_m \to -100 \text{ mV}$):** All channels are closed, $\overline{x} \to 0$. The variance is near zero: $Var\{n\} \approx 0$. The trace is clean, not noisy.
* **Moderate Depolarization (near threshold):** Channels are in an intermediate state, $\overline{x} \approx 0.5$. The variance is maximal. Fluctuations are high, which can explain why **jitter** and **noise** increase when the neuron is depolarized under constant input.
* **Extreme Depolarization (spike peak):** All channels are open, $\overline{x} \to 1$. The variance is near zero: $Var\{n\} \approx 0$. This justifies why the peak of the action potential itself is not particularly noisy.

This dependence on the state $V_m$ is the reason why creativity or the imprecision of *spike timing* is an active phenomenon and not just background noise.

## The Langevin Method as a Mesoscopic Approximation

### The Computational Alternative to the Markovian Model

We have established that the single-channel **Markovian** model is biophysically accurate, but the simulation is **extremely slow** and **computationally expensive** because it requires tracking the state of hundreds of thousands of individual channels.

To overcome this hurdle, an approximation is sought that is **fast** while simultaneously retaining the **stochastic** component (the variance). This approach is known as the **Langevin Equation** (or effective mesoscopic formulation).

The Langevin method consists of taking the **deterministic** differential equation that describes the average fraction of open channels $n(t)$ and **adding** a stochastic noise term $\xi(t)$, whose amplitude and color (temporal correlation) are rigorously derived from the single-channel Markovian theory.

For the dynamics of the state variable $n$ (which is $\overline{x}$):
$$\frac{dn}{dt} = -(k_1(V) + k_2(V)) n + k_2(V) + \xi(t)$$
The noise term $\xi(t)$ is chosen so that its variance is consistent with the population variance derived Markovianly. In this way, the Langevin equation incorporates the variance:

$$Var\{\xi(t)\} \propto \frac{\overline{x}(1 - \overline{x})}{N_{\text{tot}}} \text{ (dependent on } V \text{ and on } N_{\text{tot}}\text{)}$$

### Advantages of the Langevin Approach

1. **Computational Efficiency:** Only four differential equations (for $V, m, h, n$) need to be solved, just like in the deterministic **Hodgkin-Huxley** model, but now with a noise term that captures fluctuations. There is no need to track individual channels.
2. **Accuracy:** Despite being an approximation, the Langevin formulation is capable of faithfully replicating not only the average but also the **variance** and the **autocorrelation function** (the noise "color") of brute-force Markovian models.
3. **Predictive Power:** Being an analytically tractable model, it allows for the inference of biophysical properties (such as the number $N_{\text{tot}}$ of channels) from the experimental measurement of membrane noise, as demonstrated by **Conti and Vanche**.

This dual perspective (microscopic/Markovian for the foundation, mesoscopic/Langevin for efficient simulation) provides us with the essential tools for analyzing the active and noisy behavior of a real neuron.

---

## Stochastic Hodgkin-Huxley Models and Intrinsic Noise

I wanted to show you what happens when one takes the Hodgkin-Huxley model and transforms it, in the way I described to you, into a stochastic model. In this model, the two currents, the voltage-dependent sodium current—which is also called **fast inactivating** because it inactivates rapidly—and the **delayed rectifier** potassium current, are transformed and described stochastically.

The first thing I do is, as I did last time, apply a constant current stimulus. So, I turn it on at time zero and leave it on indefinitely. If the current, as in this case, is not particularly intense, it doesn't do much. The more astute among you might notice that there seems to be a little bit of fluctuation here. The trace does not seem to be exactly what would be expected; there would be some fluctuations. These fluctuations, if I take a current and make it even more negative (this is $-0.5$, which must be $\text{nA}/\text{cm}^2$; the unit counts, but not the dimension because I am simulating per unit area, so I am not saying that the cell is of that size), might remind some of you of a phenomenon we discussed last time: the reproducibility of the firing **pattern**.

It was this figure here, and it was a seminal experiment by **Mainen and Sejnowski**, where, by stimulating a neuron with a constant current, a constant stimulus active for one second (on the order of $800$ milliseconds or whatever), taking a pause and then restimulating it, then pausing and restimulating it, one could see that only the first **spike** of the series of $25$ repetitions seemed to be reproducible in terms of **timing**. The others seemed as if there was a kind of noise source, of **sloppiness**, of imprecision that accumulated, such that perhaps the total number of **spikes** was maintained, but there was what is technically called **jitter** (I don't know the Italian word for it: a flickering, a flutter, I don't know if that's the right word). And you see that this flutter tends to become larger and larger the more time passes.

While, for the same neuron, a moment later, when the type of stimulus was not so boring, fixed, **DC**, constant, that is, but fluctuated (it was always the same current, always the same trajectory, it was generated like a set of random numbers but was always the same, somehow it was **rewound**, rewound, and the same form of stimulation was applied again), you see that it continued, instead, to maintain, except for very few cases out of $25$ repetitions, a very rigorous reproducibility. It seems, that is, that there is a kind, as I said last time, of noise generator that, however, changes with the electrical activity of the neuron.

In particular, something I didn't emphasize last time, it would seem from here (I drew this red bar specifically): if the membrane potential remains rather depolarized, it seems that this noise generator is present, plays a large role; it's as if the variance of this noise generator, the intrinsic fluctuation of the membrane potential, were greater. If, on the other hand, the fluctuations bring the membrane potential, the input brings the membrane potential, periodically back to this slightly less depolarized bar, perhaps the noise generator tends to reset itself, tends to decrease.

I mentioned that this story was even more interesting than just the fact that perhaps creativity, perhaps the possibility for an animal to have responses that are not identical (we are not robots; if you do an experiment with a rodent or a primate there is no deterministic consistency—besides the fact that even a robot would not have deterministic consistency because it would be a physical system, and in a physical system there are a lot of inaccuracies, non-idealities, you see this when driving a car, which make it impossible to execute exactly the same trajectory twice in a row). This is even more interesting because it seems to be state-dependent, activity-dependent.

And so the most acute, astute, no, acute among you, will see or would see that by lowering this current, the value of the current, the trace becomes cleaner. While, by depolarizing the neuron, the fluctuations tend to become consistent, even such as to emit spontaneous action potentials: creativity. It occurred to me... ah, and where was it? Meh. It's a fluctuation due to the intrinsic noise of the membrane channels that caused that neuron to fire, which, on its own, codes for the red color of a visual stimulus, but the red stimulus was not there in the visual stimulus, because I was looking at something else, yet the same neuron fired. And all of biology is like this, all of biology is noisy.

The interesting thing is that, having implicitly included channel noise, I did not write something **plus noise**. So, in the deterministic part, which I wrote here, where this current $I_K$ was $G \cdot n(t)$, the fraction of channels in the open state, times the Nernst reversal potential minus the membrane potential, this is called the **driving force**. It's not that I took, as engineers do, who say: "You know what? I'll write this as $n(t)$ plus a white Gaussian noise, $\delta$-correlated, etc., etc." No, I didn't do that: I went and took the biophysical meaning and, **for free**, I get a phenomenon that seems to fit, seems to be exactly what happens, and it is exactly what happens.

One thing that can be done is, here, for example, changing the number of channels. And if you remember the conclusion from a little while ago, it was this thing where the fluctuations, the variance, or the standard deviation (but it's the same thing anyway), are **inversely dependent on the number of channels**. If the number of channels becomes very high, the members of a choir become very numerous, there are no more fluctuations. So, if I greatly increase sodium and potassium, the number of channels, the fluctuations tend to decrease. In theory, if I were to put them... indeed, for example, the behavior of spontaneous **spiking**, of spontaneous action potential emission, is gone. If, instead, this number of channels becomes very small, the fluctuations tend to be very important, even resulting in many action potentials being fired.

Here you might object: "Wait, you are putting a small number of channels, so this term here is small, because this is the maximum conductance and the maximum conductance was the total number of channels times the conductance of the single channel." I am deceiving you because, to explain this concept to you, I am rescaling the quantities here; so, when I change the number of channels, I keep the maximum conductance value fixed. I do this because biologically, in the case of some conditions, for example morphological ones, like the **axon initial segment** of the neuron or a part of the proximal dendrites (so close to the soma), it could be that there is a very small number (now I write $n < 1$ but simply because for me it means a very small number, not that it's a fraction, but a low number: not a million channels, but $10, 15, 100, 500$). It could be that this **hotspot** with few channels can influence through electrical proximity.

You can imagine that the part... and we will look at this in depth... the dendrites to the **Soma** are as if they were connected by a resistor inside the cytoplasm. It behaves like a conductor, perhaps not ideal (previously, at the beginning of the history of excitability and equivalent circuit models, I sold it to you as an ideal conductor where there is no resistance, because I told you: "This resistance is negligible compared to the membrane resistance"). Spatially extended structures can start to matter, and this distal or at least proximal noise generator (but still not close to where action potentials are generated) can have this effect.

---

## Response Variability and Stochastic Mechanisms

Another interesting thing is, for example, the same type of protocol where I keep the current constant, normally at zero. The deterministic case would be this: I keep the current at zero, there is an instant when I give a small current pulse, a small step, which here is given around **$50$ milliseconds** and probably lasts **$5$ milliseconds** or **$10$ milliseconds**.

If I did the exact same experiment (here, anyway, this number of channels are such, there are many, but they are still not enough to eliminate all fluctuations, okay? Maybe at this level it is almost deterministic), the response to this step is more or less... in this case maybe it's not excitable. I increase the amplitude a little more. My apologies. The neuron is excitable, the response did not generate a **spike**, an action potential. I increase the current a little more, the amplitude of this step... nothing. I increase it a little more... **spike**. But, you see that (and this is what I wanted to show you) even with a very high number of channels, a protocol like this reveals... I am repeating it five times (as always, you have the code; you can take a look at the Python code and see how I did something like this internally).

The interesting thing is that by giving the same stimulus five times, **three out of five** I had a so-called **failure**, a failure of conduction, a failure of initiation, of the **triggering** of an action potential. Twice, however, out of five, I had a **spike**. And another thing you notice is that the instant at which the **spike** is generated fluctuates. So, again, this voltage-dependent characteristic of the membrane potential, because it depends on the number of open channels (I remind you, we are talking about these fluctuations of the variable $n$), and this variable $n$ has a variance that depends on the number of open channels, on the fraction of open channels. Therefore, when the potential is very hyperpolarized, the channels are closed, this variance is low. When, instead, the number of channels is such that $x(1-x)$, which functionally is a parabola, is a parabola with concavity downwards (try drawing $y = x - x^2$; the minus... then there is no constant term, so it is centered, it is not centered, as you might say), it means there is a peak of the variance, a peak of the fluctuations. And it makes sense that, biophysically, when the potential is all hyperpolarized, no channels are open, there are very few, and that is why you saw, practically, when the current was negative, you saw the trace practically without noise.

Let's ignore **Kolmogorov**. Here, on these two **slides**, you have what I showed you live with that simulation. Well, this type of mathematical description, although very biophysically accurate, is, as said, quite complicated, it is quite slow. You have to, for every single voltage-dependent ion channel (be it the type of sodium channel, or the potassium type; if there are other types of channels, you have to do it for every type of channel), keep a state variable in memory, okay, binary, **0, 1** (this perhaps costs nothing).

Yes, go ahead.

## Relationship between Variance, Channel Number, and Membrane Potential

Yes. So, if I look at the expression for the variance, I realize that... therefore, if I try to **plot** that quantity there... so, that the variance gives me the magnitude... the variance, in other words, is the mean squared deviation, okay? And it should light up in your mind the fact that when you have a random variable that I call $z$, and this variable is Gaussian, the probability distribution density of this variable $z$, for example, is made... I mean, without example, it is made with a Gaussian, bell-shaped trend, it is centered at something you call the mean, $\mu$, for example, and here (now it is not important exactly where it falls, but roughly) the magnitude, the width of this Gaussian is given by $\sigma$, the square root of the variance. The variance is often called $\sigma^2$, so $\sigma$ is the standard deviation. I make this distinction because $\mu$ and $\sigma$ must have the same unit of measure. The variance is squared, so if it is the average height, the variance will be $\text{meters}^2$. So that's why people also use the standard deviation.

So, mean and variance: you should visualize the fact that you have some **offset** and the more variance there is, the larger this magnitude is. In my mind, I imagine that this tells me that it is probable that when I extract or see people of a certain height, or see what is the instantaneous value of the current generated by a population of membrane channels, etc., they are around the mean but can, very rarely, be where the tails are, at values where the tails are, because the probability there is low, but where it is not low, it is easy for me to have events at these values. So, the larger the variance is, the larger the fluctuations are.

Another way in my head that I have to imagine these things is if you generate on the computer a set of Gaussian, pseudo-random numbers (not uniform, but Gaussian, so with this distribution), you **plot** them over time (so this is the beginning of my generation and this is the value of these numbers), they more or less do something like this. And if I squint, I see that this signal that goes up over time (I inverted the axes because I liked relating this bulge or the belly of the Gaussian to roughly the band in which it was contained, at least **$68\%$** I should say, one or at least two standard deviations, you tell me if it's **$99\%$** or three standard deviations is **$99\%$**), so for me it is how large the excursions of this stochastic process are.

There I have that the variance is dependent on the mean number of open channels, which is $\overline{x}(1-\overline{x})$. When $\overline{x}=0$ it is $0$ and when $\overline{x}=1$ it is also $0$. If you want we can do the function study, but the maximum is at $0.5$ and it does something like this. The variance is not negative because it is a positive quantity and $\overline{x}$ only changes between $0$ and $1$. So this is the graph.

So, if the membrane potential, thus $\overline{x}$, changes by changing $V_m$ (in addition to changing on its own in a transient), it changes due to the membrane potential through those $\alpha$ and $\beta$, or $k_1$ and $k_2$ that we called them before, because these here are voltage-dependent. And they are voltage-dependent with that trend, actually it would be $\frac{\alpha}{\alpha+\beta}$, which would be $n_{\infty}$, $x_{\infty}$ here. Somehow I know that when the membrane potential tends to become more depolarized, both sodium and potassium channels begin to activate. So from here I am moving in this direction. Therefore the variance tends to increase, and I see it because when I tended to increase the current I pulled up the membrane potential. A lot of channels opened.

I believe you would not have flinched in the deterministic case: you would have said "I see that at a certain point the membrane potential deviates from the case of a simple passive **RC** because the sodium and potassium channels start to activate." Here it is the same thing, but also in the dimension of the fluctuations. True, the mean value would tend to integrate the current, would tend to change, the story of the concavity, I told you a colleague of yours from the second year last year was complaining to me, saying "Yes, but the concavity is different from what a passive **RC** is." Yes. Here, in addition to that, you also have the fluctuations, and these fluctuations begin to become larger and larger the more channels are in the open state.

The correct objection would be: "Okay, but after a while shouldn't it decrease?" Yes, indeed, during a **spike** (which is the only way the membrane potential is allowed to explore very depolarized points, unless you go in with a very notable current injection, you truly give a very strong current pulse and the membrane potential stays up there because you are injecting a lot of current)... I think technically the pipette, touching the membrane, would heat up a bit and you would lose the connection. But if that didn't happen, you would see that the noise would tend to decrease. And in fact, during **spikes**, during this behavior, the **spikes** are not particularly ugly, they are ugly and noisy here, but they are not here... in fact the membrane channel there is not particularly high.

What I can do on the fly... I'm really causing trouble... is to quickly change the simulation time. This is not enough. I would also like to change here, so not only the time but also the **range** of this figure. So I know how to use **Python** and **NumPy**, and so theoretically if I wanted to have a **zoomed spike**, it's not noisy here. Before, yes. So, when the membrane potential is depolarized, this is not particularly different from a potential generated by a deterministic **Hodgkin-Huxley** model. Can it be an answer? Did I answer? Well, what I'm thinking about is this.

## State-Dependent Stochasticity and Experimental Implications

I am thinking, that is, I don't know if there was, maybe two times ago, I don't know if you told me you missed the lecture. Well, here I am thinking about these activation curves, and if you want, here I can reinterpret them (because the math is the same) as not the asymptotic value that the fraction variable would have, if there was time, but here it would be, in our context, the probability variable. But in the end the concept doesn't change: it is that in the case of sodium and also potassium, when I tend to depolarize, the number of channels in the open state tends to go from $0$ to $1$, to **$100\%$**. It is true, here I should actually multiply this $m_{\infty}$ by $h_{\infty}$ and it would have a kind of up-and-down trend, but let's pretend there is no inactivation for the moment.

The more I am depolarized around $-70, -80, -100$, this is flat, it is zero. If it is flat, then the... then intuitively the channels are all in the closed state, and so good luck **flickering**. Yes, they can **flicker** but the transition probability is practically zero, so even spontaneously they do it once in a blue moon. When instead (because this curve here, or I can also interpret it as an occupation probability, but actually I prefer to think of transition probabilities), when the transition probability tends to increase (so $\alpha$ and $\beta$, which you also had and which are here)... now I should remember who is who, I think. Written like this, it means that $\alpha$ is the transition probability between closed and open. It is. Damn it, I don't remember things. So if it is open, closed and I say this is $\beta$, this is $\alpha$, what do I write? $\frac{dO}{dt} = -\beta O + \alpha (1-O)$. Yes, written like this, $\alpha$ is when it is closed, from closed to open. So, I start when I depolarize to increase this transition probability. So it means that if I have a sodium channel there in front of me (and potassium more or less does the same thing, in fact it's even a little more... no, the scale is not the same, it is slower, in fact it is delayed), I would see that at very hyperpolarized potentials it is practically always closed and rarely makes a transition, makes a fluctuation, **flickers**. If, instead, I start to increase $V$, this, which is the transition probability per unit time, I would see it much more often, okay, pushing the bottle means I am thinking that it opens and then closes. So I should reason about both $\alpha$ and $\beta$. And it is easy because you see that $\beta$ practically, when the potential is very depolarized, practically becomes zero, that is, it is very easy for me to make the transition between closed and open and to remain open. The concepts conceptually hold even in the stochastic context.

Then, if and for those who like the quantitative and mathematical aspect, it can be useful not only as an exercise, not for intellectual exercise, but because (and this is what I am not telling you, I will only hint at it now) with the expression of the variance, I could in principle take a real neuron, stick a pipette into its stomach, and experimentally (and it is always like this) see that the potential is not fixed perfectly straight at $-70$, it would have a little bit of fluctuation. So, the experimentalists among you would say: "Yes, you said last time that the synapses (maybe it wasn't you, no, it was your colleagues on Friday), the synapses that we will talk about later, in the second half of this lecture probably, are incontinent, because they too are stochastic systems. Even if there is no action potential, in the end they are always ion channels, so they are also vesicles that fuse, so they are incontinent, they leak and every now and then they have spontaneous activations, what are called **miniature synaptic potentials**."

Experimentally I also see a fluctuation. If I block the synapses with a drug, so I make the receptors deaf, so I put something that blocks the synaptic receptors, the neuron no longer feels, no matter how much the other neurons release neurotransmitter. I have a selective antagonist that plugs the channels from the outside and so the channels no longer feel, no longer bind to the neurotransmitter. If I start to depolarize, I see more and more fluctuations, and if I have this formula, I can in one go measure the fluctuations and ask and answer the question: "But how many channels are there?"

There are $225$. This was a revolution of sorts in the early eighties. There is a nice **review** article, by two Italians, **Franco Conti and Enzo Vanche**, both now retired, who at the Institute of Biophysics and Cybernetics of the CNR in Genoa performed this experiment, saw the fluctuation, and had derived this type of description. It is exactly this, if you take their **paper** you see that it is this thing here that you study now, and from this expression they can infer what the total number of channels is. So how do I measure, if I don't see them, these ion channels? One way is with antibodies that bind, I see them fluoresce with some microscope, but if I don't have a microscope, with membrane noise.

This reminds me of a very brief anecdote, not from my childhood, but from when I was in Belgium. They had given me cells that were stem cells differentiated into neurons, and they told me: "Look, this cell here is a human induced pluripotent cell, I'm almost sure it was not an embryonic stem cell, so it was not taken from human embryos, it was instead a cell, for example from the skin, reprogrammed to become pluripotent and redifferentiated to be not a skin cell but a neuron cell." Molecular biologists normally say: "To be a neuron you must express these genes, these proteins. I have a specific stain for all these markers, for these proteins or genes or whatever, I tell you that this is a mature neuron." I am a pain in the neck and I say: "No, for me it is a mature neuron, it must fire **spikes**." And by putting the pipette inside the stomach of these neurons (so with the **patch clamp** technique, I mentioned it last time because **Neher and Sakmann** used it for this aspect), we saw as a function of time... so the experiment was practically the same thing we are doing together. A current **step** was given, a current step, and this current step... suppose this could be **$500$ milliseconds** and the amplitude was increased. So a stimulation, **$500$ milliseconds**, **$500$ milliseconds** the neuron rested, and then I resumed with a greater amplitude. Suppose these **steps** could be **$25$** or **$50$ picoamperes**, to give you some numbers, so that if I ask you during the exam: "Roughly, how many are they, pears, apples, are they megaamperes, what are they?" You can say that they are tens of picoamperes, hundreds of picoamperes. The membrane potential, which I draw here, suppose **$70$ millivolts**. When the current was zero, it was a rather flat thing, there were no synapses, so all the fluctuations I saw were either membrane noise or amplifier noise. And since I paid **$10,000$ euros** for the amplifier, for it to be so bad as to make noise... I'm from Genoa, so I'm sensitive to these things. Then, by increasing the current, the fluctuations tended to increase more or less as I showed you and became high.

Now, these cells here never fired. So this made me think that I had proof that voltage-dependent channels existed without having done complicated experiments where I took the pipette, placed it on the membrane and then quickly retracted it so that a small piece of membrane remained. No, a silly thing, I put the electrode inside the neuron's stomach and looked at what it was doing and I saw that the fluctuations changed as the potential varied. So, from this formula, in theory, in fact, only because these fluctuations changed with depolarization, I could state: "Okay, yes, there are voltage-dependent channels, so it's not that these cells you give me are not excitable, but for me they are not neurons because I have to see **spikes**." Now, to be fair to that colleague, there were some ugly **spikes**, probably because there weren't enough sodium and potassium channels to create a real action potential. If you want and you have the **notebook** on **Google Colab**, theoretically you could see what happens if you take the **Hodgkin-Huxley** model and change the maximum conductance value for sodium and potassium, and so you are fundamentally playing (even only in the deterministic case) with the number of sodium and potassium channels. You will see that if they are not numerous enough you don't have **spikes** and if they are too numerous it could also be that you can't have repetitive **firing** because the potential remains blocked under a stimulus. It stays blocked, if there are many sodium channels, sodium wins, the sodium currents win over potassium.

## The Langevin Approximation: Computational Efficiency

Well, and yet, this Markovian, a complicated description, and I deceived you because the demonstration I showed you with **Google Colab** a moment ago has another approach. Another approach that we have shown to be very, very similar, is an approximation that has the name **Langevin**. Have you ever heard of this guy? It comes up in the so-called **Langevin equation** and we mentioned it when, in the case of the mobility of an ion, I told you that to do the derivation or to show you those stupid animations where I had balls moving, I had simulated the dynamics equations $\mathbf{F} = m \cdot \mathbf{a}$, okay? $\mathbf{F} = m \cdot \mathbf{a}$ in two dimensions, so the force had two components, okay? So the acceleration had two components, the velocity, which was the integral of the acceleration, the position, which was the integral of the velocity, etc., they had viscous friction, but they also had a stochastic term, because I wanted to show you the effect of diffusion.

The effect of microscopic diffusion is collisions, **Brownian motion**, the fact that water dipoles tend to agitate due to temperature and thus transfer kinetic energy to the particles, for example to the one whose mobility we wanted to calculate or infer. In that case, either you do the numerical simulation, as in this case the Markovian numerical simulation, or you try to write an equation where all the non-deterministic terms are enclosed in a single term which is noise. Exactly what engineers do, who say, in fact engineers do it for other reasons, they do it out of ignorance and they say: "On this communication channel I assume there is the signal I want to see plus a quantity that I don't know, which is noisy, which I hope has zero mean, I hope the variance is known and I see, I wonder if I can make some..."

Well, the same thing of writing a certain quantity plus noise is very easy, but what kind of noise do I put? I mean, I want to put a kind of stochastic process that fluctuates and it is an approximation because you tell me: "But no, you said just now, you have to follow every single ion channel to understand if it opens or closes," which is this novelty that you want to describe a kind of stochastic behavior of a population. It would be like saying: "In this room I have many gas molecules, but I give up because it is too complicated to track the kinetic energy of the single one (it would be exactly the same thing), and I want to have a formulation that, yes, respects thermodynamics, but also gives me..." (in the case of gas in a room, no, in the case of a flight doing turns tomorrow, so turbulence) "...I have the single particles, but the plane is not microscopic, it is quite large, so there are fluctuations, you feel the fluctuations when there are air pockets and turbulence. How the hell do I model them, how the hell can I capture them?"

The way to do it is to put... so for a number of years, people took the charge balance equation and then said: "You know what? I'll put the noise here." And a priori I'll tell you, in fact I'll tell you in a wrong way, in an arbitrary way. A priori I tell you: "Look, here I put a noise whose variance changes with the potential." So, from a certain point of view, I can say: "Okay, yes, you're on the right track," but on the other hand, the person who proposed this could say: "But what's the alternative? Doing a simulation with Markovian models, first of all it is extremely **time consuming**, it is extremely long and computationally ineffective." Then, perhaps if you have, maybe you put noise here, some consideration, not for the **Hodgkin-Huxley** model, but for other descriptions of excitability, you could infer something without having to do a simulation, which is ultimately the goal of engineering, of physics. It's not that I have to build $25$ bridges or $25$ artificial valves to understand the one that works in the heart, I hope to be able to have formulas that tell me that they are **guidelines**, a guide to design. Here it would be the same thing, only that putting the noise here would mean that I am imagining that that term with $n$, which is non-deterministic, does not have, first of all it does not have the product between $n$ and $V$, because this thing here, yes okay, changes with $V$, that also changes with $V$, because $n \cdot V$, so if $V$ changes $n \cdot V$ also changes, but it is inside $n$ that the variance changes, there it is an arbitrary dependence.

Another way is that in the deterministic equations you put noise here, and I'll make it brief because you can do this derivation rigorously using exactly the technique I showed you before. You have this $n$, now here I'm calling it $x$, you have this $n$, okay, I calculate mean, variance, actually here I have to calculate something else that perhaps you have never heard of, but in the end it is the color, which is called the **autocorrelation function**, but pretend you haven't. So it is possible to take a shortcut.

## Comparison between Models and Noise Properties

So here, for the case of **Hodgkin and Huxley**, which is the one I simulated for you, I still have four differential equations, I don't have to track the single channels that are perhaps a hundred thousand, a few million, for which the single channels are then $0$ or $1$, no, no, it is exactly the deterministic model complemented by noise and here I put the variance formula that I derived, so that I derived analytically that I showed you before was $\overline{x}(1-\overline{x})$ divided by the total number of channels. So I continue to have a kind of knob that allows me to say if the number of channels is large, this term tends to be negligible, and since that variance was calculated with the formulas, with the dynamics of the single channels, this thing here somehow inherits the same characteristics. There are limits and I won't talk about them, it doesn't always work and perfectly, but I won't talk about them.

And the thing I'm showing you is the comparison between the **Markovian** case and the **Langevin** case, which is this effective, mesoscopic, still noisy, but approximated case. And in the end this is for **$100$ milliseconds**. I am **plotting** the fraction of open potassium conductance channels and these of the sodium conductances for a certain potential value. I am keeping the membrane potential value fixed. And if I look at the black trace and the red trace, okay, I can say: "Okay, yes, it's a stochastic model, yes, it seems to fit both the mean and the magnitude of the fluctuations, both for potassium and for sodium." And I do not only look with my eyes to see a temporal realization, but I make the histogram, that is, I flatten all these points onto the same axis and count how many times, how many points there are here, there are very few, here a little less, here inside there are very many. I make, that is, the histogram and the figure is **tilted** precisely to give the impression that if you half-close your eyes, if you squint, you see that here there is a band and this band is exactly the band in which this Gaussian... you say: "Where does this Gaussian come from?" The Gaussian comes **for free** because you have a lot of channels and you have heard of the **central limit theorem**. Have you ever heard of it? It is a very important theorem in probability theory, which says that when you have a lot of stochastic variables, random variables and you sum them together, the sum is distributed according to a Gaussian distribution. It is incredible. How the hell is it that if one has binomial distributions $0-1$ with a certain probability, you sum them together and it becomes a Gaussian? If you have a lot of magnitudes (I won't give the example of height again: if you take height, weight, sex, whatever), you sum them together into some other variable (here it makes sense biophysically, here it is a current because **Kirchhoff** currents sum up, in the example I gave no), however that sum of height etc. etc. is Gaussian, which is amazing, because the Gaussian should be a kind of primitive of the world of probabilities. And you see that it fits, that is, the **Langevin** approach, which is the red one, perfectly captures not only the mean but the variance, I couldn't say which is which. And it also captures this which is called the **autocorrelation function**, which I won't tell you exactly how it is calculated what it is, it has to do with how quickly these fluctuations change, how quickly, what their color is, the energy in the transformed frequency domain.

So the thing I can do is: if I can do the brute force simulations at least once with the single Markovian channels, with the single Markovian conductances. Give me a minute and I'll finish. I can do exactly the experiment as before and compare the times when there is, for example, a **failure** or there is an action potential, or vice versa compare what the **jitter** of the generated action potentials is. And I can see the model, so the neuron made with all the trimmings, with a hundred thousand channels, each described by a Markovian scheme, how it compares to a **Langevin** model which is very easy and very fast to simulate? Here I won't tell you which is which, if I didn't tell you that red and black are **Langevin** and **Markov** respectively, you would say more or less the same thing. The blue one is a guy who proposed another method and that in this article years ago we said: "No, the other method is really different, it's really poor. Why didn't this guy do the same simulation we did of the Markovian model? Did he want to make an approximation? And why doesn't he compare it with the real model?" So, simply, one can make quantitative indicators and transform that comparison, that comparison I hinted at by showing a current pulse, how many times it fires compared to the total. So this is the number of times it fires, between $0$ and $1$ is a probability compared to the stimulus amplitude, etc., etc., and things work reasonably well.

## Synaptic Transmission: Electrical vs. Chemical

Okay, and the climax: if one does the experiment, if one reproduces that experiment from before, one sees that if the stimulus is constant you have this **jitter** of action potentials. It is not made to be exactly identical to the one before. Here I am only showing the first four **spikes**. When, instead, the stimulus is with fluctuations, rapid excursions, here the **jitter** is very small. Paradoxically, the first one has more **jitter** than the second or the third. So it means that the **Langevin** description is not so bad. So many of the things that ultimately descend from this theory of these two guys, **Franco Conti and Enzo Vanche**, are statistical population considerations, so, pardon, linked to probability theory, they fit both experimentally and from the point of view of the equivalent description, when the simulation becomes too computationally demanding. This closes the part, I won't tell you about this, it closes the part on excitability. Let's take a ten-minute break and we'll resume on synapses, which you know a little about. Thank you. Okay, it starts, right? No, I'll continue until tonight. Okay. Yes, yes, okay, we'll finish at 6 p.m., or shortly before.

So, okay. So this part is relatively light. It starts with telling you, perhaps again, based on what you saw with Professor **Zoli**, but with the language of bioengineering and biophysics, synaptic transmission, because in the end synapses work, at least in the postsynaptic part, with exactly the same logic as voltage-gated channels. They are not voltage-gated, they are **ligand-gated**. You will see that exactly, I will torment you with open and closed, $\alpha$ and $\beta$ will return in a moment.

So I will tell you that there are two types of synaptic transmission (perhaps **Zoli** told you about it, he told you about both). The first is called **electrical** synaptic transmission to distinguish it from **chemical** transmission. Not all neurons communicate or not necessarily communicate always and only with a release of neurotransmitter. Conventionally, "the brain is an electrical machine." In fact, this is also wrong: neurons convert, through synapses, electrical stimuli into chemical stimuli, then, **by the way**, in parentheses, they are further immediately converted into electrical stimuli.

There is an exception which is that of electrical synapses, and I will tell you in my way to try to give you, as usual, not a mnemonic nomenclature, but hoping to really be able to... to make you activate, to make some synapses appear, the nomenclature and the classes of these postsynaptic receptors in a way that I hope you won't forget, particularly also in the spirit of saying: "When is a synapse excitatory or inhibitory?" You would tell me: "Eh, it depends on the neurotransmitter." Maybe, yes, but it depends on the effect that neurotransmitter has when it binds to the postsynaptic receptor. Now it doesn't make much sense, it will in a moment.

The first type are also called **gap junctions**, where it means **gap junction**. **Gap** is a space, a small void, and they are synapses that you will understand in a moment why, or perhaps in a few minutes why, I define them as **bidirectional, slow, and signless** (where for me the sign, at this point, would mean that if a synapse is excitatory, somehow, with respect to the postsynaptic neuron it tends to depolarize it, it tends to facilitate the probability that that neuron can reach threshold and fire, while an inhibitory synapse tends instead to hyperpolarize, tends to discourage the... it is not universal but it is an excellent approximation... tends to discourage the postsynaptic neuron). So I get excited and by getting excited I release neurotransmitter which can have an excitatory or inhibitory effect depending on the identity of the neurotransmitter. In reality, the identity and the type of functioning of the postsynaptic receptor are what matter most. But here nothing is released, but I wanted to tell you what the sign means to me.

In this case there are membrane channels, which I just forgot the name of, **connexins**, that when they juxtapose, and they must be in the presynaptic part, so in the membrane of neuron A and the membrane of neuron B, and the membrane of neuron A and neuron B must be close, they must almost touch, or at least be close enough to have a **gap**, a small space, and these **connexins** aligning due to binding reasons, I think electrostatic (I don't know this, I'm ignorant), form a pore that puts the cytoplasm of one cell in direct communication with the cytoplasm of another.

In the debate that I haven't told you about, perhaps I only mentioned it, between **Camillo Golgi and Ramón y Cajal**, it was interesting because in the end both were right. **Camillo Golgi** was convinced, even though he had invented the silver impregnation technique that allowed **Cajal** to not only make beautiful drawings but to infer the fact that neurons had a kind of polarization, in the sense that one part takes the input and another part, the axon, is the output. **Cajal** said that there was no continuity between the inside of one cell and the inside of another, but **Camillo Golgi** said: "No, it's like the heart, it's a **syncytium**, it's a continuum of excitable cells that yes are discrete, but they have communication between the cytoplasm of one cell in direct contact with the cytoplasm of the other." In reality I think **Golgi** thought there were no individual cells, I think, but that it was a kind of network, a kind of graph. Instead **Cajal** said: "No, a junction exists, I see it, there is clearly space between one neuron and the other."

**Gap junctions** in some way prove **Golgi** right, and they are, because, I repeat, they put the inside of the two cells in ionic communication. And the interesting thing is that it is a type of synaptic communication from a **phylogenetic** point of view (which means the evolution of different species), relatively ancient and very represented especially in invertebrates, especially in insects, which are invertebrates. And also **ontogenetically** (which means from the point of view of development, not philosophical, constitutive of being, but the development of the individual, from embryo to adult), **gap junctions** appear predominantly during the early stages of development. However, in recent years, they have been found in the adult cortex between different types of inhibitory neurons.

It seems that, since there is a diversity of cell types, "if you are an inhibitory neuron of type A, you touch and have **gap junctions** with other neurons of your same type, but not with those of type B." They are all inhibitory and in some way, there are reasons that maybe we will discuss together in the future, these **gap junctions** are specific. It is not difficult to imagine the reason for this specificity, it is in fact a consequence of gene expression. If I am led to establish, to express these **connexins** because I am inhibitory neuron of type A, it may be that there is an evolutionary reason why I am always led to express them and to establish an electrical synapse with another neuron.

**Bidirectional** because if I ask you: "Who is speaking, who is listening?", simply from this figure, from this **cartoon**, you tell me: "It's symmetrical," that is, an ion could pass. I haven't told you whether these **connexins** are selective or not, let's assume they are not, they are not particularly selective to ionic types. Typically the calcium ion manages to pass from one cell to another, it could pass from here to here or from here to here, so it is bidirectional. There is no: "I speak, I spit out my neurotransmitter, then on the other side there is someone with a receptor, so with a hole that I spit into." Okay, the neurotransmitter binds and an electrical cascade continues. Not here, it is **slow** for a reason we will see (here there is no particular thing, there is only a hole, a pore), and from an electrical point of view, it is slow because this is analogous to two **RC** circuits, two **RC** circuits placed close together, they are two cells, they are two membranes. And it is **signless** for a reason we will see: it's not that if the one above is electrically active and starts firing action potentials, necessarily the neuron below tends to be either excited or inhibited. Maybe both, or, correctly, the answer would be: "It depends." Now I will tell you what it depends on.

---

## Gap Junction Properties and Signal

This is an example of two... so they are not insect cells, it is a relatively recent article, perhaps, okay, maybe **$10-15$ years ago**, in which two types of **interneurons** of the rodent cortex of the **low threshold spiking** type are inhibitory neurons that have this connection, these electrical synapses. How did the investigators, the experimentalists, realize it? From the fact that by injecting, suppose, into the first neuron, into this one (you must have pipettes in both neurons, which is very complicated experimentally, because when you have the pipette in one neuron, you must first be lucky enough to hit the other neuron that is connected—it's not like you can see it and say: "Ah, these are connected"—and then you must ensure that when you establish the **patch** with the other neuron you don't lose the first one, because it only takes a little, only a breath). There are people who have abandoned research because finding connected neurons was something that required years, and after one year this guy had a number of... he had managed to lower the electrodes, to hit two connected cells only $10$ times out of $12$ months that he tried. Considerable frustration.

The interesting thing is that these experimentalists injected not only a depolarizing current, you see this constant step (here the calibration bar does not say how many **picoamperes** they injected). When the stimulus current is positive, it is depolarizing, this neuron does what we expect, I even see a beautiful frequency-dependent adaptation (you see that the **interspike intervals** of the first two or three are smaller than the others, then at a certain point it reaches the **steady state**). And here I see in the other neuron, by definition of synapse, I see some... not action potentials, they are called **synaptic potentials**. Why? Because they are the echo of these action potentials. When there is this action potential I see this hump, then I see another one, then I see another one, and so on. And these humps also have an amplitude that maybe depends on what it is, but above all I see that the time, the instant when they occur, exactly reflects the same **timing**, the same temporalization of the presynaptic **spikes**.

The strange thing is that when I also give a hyperpolarizing stimulus, where the neuron does not fire, this neuron here behaves like an **RC**, it becomes something like a sink full of a liquid with skin, it becomes something extremely boring. So much so, even when the neuron is hyperpolarized, even if it does not emit **spikes**, you see an echo, an echo in the membrane potential of the other neuron. This membrane potential hyperpolarizes a little.

Here it is important to note the time scale. This calibration bar has a value of **$30$ millivolts** for the upper trace, and indeed they are **spikes**. If you take this piece and put it here, you probably see that it fits $2-3$ times, so around **$90$ millivolts**, **$100$ millivolts** from the base to the top of the **spikes**, which is the **mantra** I tell you: **$100$ millivolts**. So eat bananas, the voltage of a cell phone is not so different, $0-1$ volt, anyway. Instead the same length here (to avoid putting the two... from a graphic point of view it is a bit confusing, but it is a notable habit, a frequent habit), this calibration bar means that in the second graph it stands for **$4$ millivolts**, so this thing here is $10$ times smaller. And for the synaptic potentials you could tell me: "Well, okay, yes, I can imagine they are small, it's not that I fire and the postsynaptic neuron also fires." This does not happen or happens very rarely, particularly very rarely in vertebrates. But not only that, this hyperpolarization is also small, it is on the order of a few **millivolts**. The fact is that it is there, so it is as if there was a continuous coupling, not impulsive, that is, it's not only when I fire that there is communication. Whatever this membrane potential does, this other neuron follows.

Try to understand what the way to mathematically model or functionally, mechanically, mechanistically describe the way these two membrane potentials, these two cells, are coupled to each other could be. There is a wire in between, there is a resistor in between. So before I told you that **connexins** are pores, so it doesn't take a stroke of genius, and I believe it, I'll show you shortly.

The other way neurons talk to each other is a **chemical** connection, which you probably know well, and in this case it is **unidirectional**, because I am projecting to you, and this is my axon, and the axon, except in rare cases, is what spits out the neurotransmitter. And it typically spits it out not onto another axon (in that case it would be an axo-axonic synapse, which exists, but now I don't want to confuse you). I have a part of my morphology that is specialized to spit out the neurotransmitter and I can project to a neuron there, but that one in order to have the same impact on me must elongate its axon, maybe it doesn't elongate it, it elongates it elsewhere.

---

## Chemical Synapse Mechanisms and Receptor Classification

So this structure, which is the termination, the synaptic terminal, the termination of an axon, which can also be, which forks, there are several **branchings** of ramifications, is called the **synaptic bouton** and it is, compared to **connexins**, a world of complexity completely, orders of magnitude higher. I think it is the most complex cellular organ, from the point of view of anatomy and cellular physiology, that exists in the human body and in the nervous system and in biology. It is extremely complex, there are a lot of players involved.

The final result is that when this terminal is invaded by a depolarization that travels in the axon, more or less always the same (for the moment I haven't told you about these propagation phenomena, we will mention them, we will see them more from the point of view of dendrites, but that will be in the future), you can imagine that if the axon is excited and fires a **spike**, this **spike** can be like signals in a transoceanic cable, particularly digital signals in a transoceanic cable which are periodically, periodically in space, regenerated. This is why digital communications are used, because anciently **Lord Kelvin** in the nineteenth century had to, together with another guy, **Heaviside**, face and understand what the physics was of why if I had a cable of **$10,000$** or **$100,000$** kilometers (maybe **$100,000$** is excessive), a few tens of thousands of kilometers, I spoke into the microphone on one side with an analog signal and on the other side the signal was extremely attenuated. You say: "Okay, thanks, it's like a very long wire of a resistor." Yes, but it also has capacitive and inductive properties and this leads to a deformation, what are called **transmission lines**. So at the receiver, even if they were only telegraph signals. If, instead, you imagine having to transmit only a logical level **0** or a logical level **1**, if at a certain point I put a station that instead of sensing **$5$ volts**, **TTL** standard (which is something in electronics still very frequent), so instead of sensing **$5$ volts**, it sees **$4.5$**, it says: "Okay, this is very different from **0**. It meant **$5$**," I regenerate it to **$5$** and this method of communicating digitally is extremely advantageous.

Here you can imagine that the propagation of an action potential along an axon, since the axon continues to have a lot of voltage-dependent channels, continues to self-regenerate. And ultimately it is the miracle of why I manage to move my toe from my motor cortex, despite it being one and a half meters, one meter eighty, whatever it is.

When the membrane is depolarized here, there are voltage-dependent channels particularly selective for calcium. Calcium enters because they are channels that open when the potential is depolarized and as I told you there is practically zero calcium inside, there is zero free calcium. Calcium actually exists in the endoplasmic reticulum, in a kind of compartment in a warehouse, but free to be able to trigger biochemical cascades, there is none, there is practically zero. So when it enters, what it can do it starts to do, and particularly what it does, this is the story, is that it makes vesicles fuse, vesicles which are pieces of lipid bilayer membrane that contain neurotransmitter molecules inside. An incredibly complex and fascinating mechanism of how a neuron can have, how biology and evolution can have compartmentalized a **quantum** of molecules. Something that resulted in a **Nobel** prize because this release of neurotransmitter is not continuous, it is not with arbitrary values, it is in **quanta**, it is in discrete units. Why must they be discrete units? Perhaps the operating principles of this thing here have to do with a digital encoding. Perhaps it is a coincidence. Try to make a system that makes analog spits, which can modulate the quantity of neurotransmitter. It might be more complicated, no one I think knows yet.

And these vesicles fuse and in what is also called here, or perhaps only here, **Kiss and Ride** (no **Kiss and Go**, **Kiss and Ride** at the airport), where in the intracellular part of the presynaptic bouton these vesicles are anchored and when there is free calcium, there are obviously proteins, **synapsin** in particular, that keep the vesicle anchored to the intracellular part of the synaptic bouton membrane, a pore is created and by **exocytosis** the content of this vesicle diffuses and at a certain point this one moment later seals itself again, having become empty, and returns inside, no longer anchored, ready to be **replenished**, to be reloaded, resupplied again.

What happens in the postsynaptic bouton instead is relatively simpler. Note that this happens in a fraction of a millisecond.

Go ahead.

Calcium is outside, there is a lot of calcium outside and when channels that are voltage-dependent (so $V$ here of this membrane must increase, must depolarize), when it increases there is an electrochemical gradient here, this enters and this signal is sufficient to do, to trigger a cascade. Things are a bit more complicated than this, the arrangement of these calcium channels is very close to this spatial arrangement, there are **microdomains** where they are. So it's as if... it's not simply "calcium enters and I release." It may be that there is a kind of modulation: "Now I release a lot, now I release little," so much so that people (we will see it later) have also described this behavior in stochastic terms, saying: "What is the release probability?" So the release probability is not all or nothing, it could be $0.8$. Then there can also be a spontaneous release probability where there is no action potential but here fusion still occurs and this synapse releases neurotransmitter. So, for all intents and purposes, to the receiver it says: "Ah, but something happened." Maybe it released little. Okay? Did I answer?

From the postsynaptic point of view I say it is simple because it is fundamentally the same thing. I'll show you a couple of animations by that guy I saw on the internet who makes graphic animations. I repeat, they are not molecular dynamics simulations, they are just **cartoons**, but they are aesthetically pleasing to watch. If before we had in the postsynaptic neuron membrane... I repeat, even if it is funny, I have my axon elongation that spits out the neurotransmitter, the other neuron does not necessarily elongate its axon towards me, the other neuron has an ear, it has a kind of receptor. I call it an ear because, as I imagine it, I will show you, it is a membrane channel, so it is intercalated in the phospholipid bilayer and the only difference with the channels I told you about, which we have discussed so far, even from a stochastic point of view, is that it does not have the famous activated **gates**. So $\alpha$ and $\beta$ are not functions of the membrane potential. They could be, but in general they are not. They are a function of the presence or absence, in some domain in the extracellular part, of the **binding**, of the chemical bond between a neurotransmitter molecule. Suppose this is **Glutamate**. When the **Glutamate** molecule binds to this part, to this mouth, to this site, the receptor opens (it is not always like this and we will see it), and when it opens, here simply due to an electrochemical gradient, this channel lets a current pass which is the one made up of ions for which it was selective. It is not necessarily said that **Glutamate** means that you are selective for sodium, potassium. It depends on the receptor and evolution obviously ensured that there was some alignment, but there are cases, especially during development, where a particular neurotransmitter, which is not **Glutamate** but **GABA** (I am thinking of **GABA**, gamma-aminobutyric acid), does not have the same effect as in adulthood. During the embryonic state, and in humans I don't remember when, in rats it is up to the first two weeks of life, after which the rat opens its eyes and things change, when **GABA** binds, the current here is not inhibitory, so it is not hyperpolarizing, but it is depolarizing. But how? This is an inhibitory neurotransmitter. Yes, okay, but it depends on this one here. If this membrane channel acts on its own and has its idiosyncrasies, it is this that dictates whether you are excitatory or not.

So the postsynaptic part we have amply commented on, thinking of the voltage-dependent case. Here it is ligand-dependent. There will be in $\alpha$ and $\beta$, there will be some dependence on the number of these molecules around, if you want, or if you really want to know, **spoiler**, there will be a variable that is a function of time, the concentration of neurotransmitter in the so-called **cleft** (I don't know how to translate it into Italian, in the... I don't know... in the small space between one, in the interstice between, exactly, interstice between the synaptic bouton and the postsynaptic membrane). Here, suppose, for a few milliseconds there can also be a concentration of **Glutamate** or **GABA** as large as a **millimolar**, which is a lot. Why is it released here? The space, the volume is very small, so concentration or density are magnitudes that are the number of particles divided by the volume. If the volume is very small the concentration can be very large.

So this is **unidirectional, fast** (for the moment it makes no sense, I haven't explained why, fast or slow). In the end it depends on this. I told you that voltage-dependent sodium channels are among the fastest things, I compared them to **Ferraris**, taking the responsibility of saying something like that here, and they have the **sign**, in the sense that this neurotransmitter always creates more or less the same effect on postsynaptic neurons, unless there is some **shift-switch** during the embryonic phase.

This is an example of a heroic experiment performed with the **patch clamp** technique, but not with one pipette, with **four pipettes**. Keep in mind that for each of these pipettes there is about **$15,000$ euros** for the amplifier, another **$30-40,000$ euros** for the micromanipulator, and so if you have one pipette, or you have $4$, or you have $3$, etc., it means you have much more money for research. The world **record** was in a laboratory where I worked, they had $10$, but I think it was paid for half, but this is a legend, I don't know if it's the truth or not.

This is an image in which four neurons, actually there are a few more, six, seven, eight pyramidal neurons, are visualized with a microscopic technique that I already told you about, the one that looks like seeing the craters of the Moon, which is called differential interference contrast microscopy, **DIC** (**Differential Interference Contrast microscopy**), with infrared. The infrared penetrates a little more into a tissue, this is a piece of rat somatosensory cortex tissue, **$300$ micrometers**, and here I have four pipettes (now you don't see them here, there is one in neuron 1, one in neuron 4, one in 3 and one in 2). If I control by means of a current pulse, exactly like the simulation I showed you several times, I am the one injecting into neuron 1, I don't know if it is a presynaptic or postsynaptic neuron, because I told you I don't see them, that is, I don't know what the connectivity is between them. The axon of number 2 probably goes down and then perhaps comes back up and establishes a synapse with the dendrites of neuron 1. I don't know, I don't see it. So the only way is **blind**, so, okay, it's not completely **blind**, but I put these pipettes and start stimulating one neuron at a time and see if there is an echo, an echo of activation of these receptors which should show me that every time I make an action potential I see an echo or not. Okay, here are the pipettes. Here you can glimpse with this fluorescent stain, you can glimpse the apical dendrites and also the basal dendrites, but the axon is honestly so thin that you can't follow it anyway. It could even be outside the plane of the slice, because it is cut. It's not that the plane of a section contains, it's not that the neurons are all, at least in the cortex, not all aligned in the same plane, so sometimes you might have two neurons that were connected *in vivo*, but when I slice the brain I cut the connection, which is what happens most often.

What I see is that if this is neuron 1, I see in one of the others, I don't remember which one it was, I see the echo, an excitatory postsynaptic potential. Note, here the calibration bar is **$50$ millivolts** or **$60$ millivolts**. **$50$** while here this small calibration bar is **$0.2$ millivolts**, so the amplitude of these synaptic potentials is very small. Which should make you think: "It takes a lot of firing for a presynaptic neuron to eventually make a postsynaptic neuron fire." Perhaps a convergence of connections is necessary, where many neurons converge on one postsynaptic neuron. It is not one-to-one, it is not a chain: "I fire, then you fire, for you to fire," one **spike** is enough. No, this has a lot of very interesting considerations.

While the horizontal calibration bar is the same, and it is **$20$ milliseconds**, which more or less fits because the **spike** is small, it's on the order of a millisecond, going down and up. For me it's only this that matters: the fact of this hump indicates that there are a certain type of channels, but it doesn't matter. When there is the **spike**, after some delay, perhaps due to the time it takes for the neurotransmitter to diffuse, about a millisecond, or at least the order of magnitude is that, it must diffuse in a very small space, which is on the order of a fraction of a **nanometer**, and it binds to this receptor. And this receptor will have its kinetics before opening. So I don't immediately and instantaneously see the echo of a channel opening.

Furthermore, I see that here, this is the resting potential of the postsynaptic neuron, I see that this deflection goes up, that is, it goes towards the excitability threshold, if you grant me that it exists. Regardless of whether it exists or not, the more I go up, depolarize, the more I increase the probability of firing the postsynaptic neuron. Now, this neuron here was sitting at **$-70$ millivolts**, so it is impossible for it to fire, but perhaps dynamically it could have been, due to other neurons that had excited it, it could have been a little more depolarized on its own, then this arrives and makes it fire. In this case, by injecting an action potential into the other neuron as well, into the postsynaptic neuron, it also had an, it elongated an axon and established a chemical synapse towards the starting neuron. So in this way one could say: "Look, even if with different amplitudes, these neurons, 1 and 3, were surprisingly bidirectionally connected," which is very unusual.

Now the thing here was interesting because the probability of simply finding a pair, I'm not saying one pair and the other one as well, simply two neurons A and B, A connected to B, in the cortex is a very low probability. That's why that colleague abandoned research, because it was on the order of **$10\%$**. It means that out of $100$ times that you insert the electrodes, you spend several minutes, you probably spend **$5-10$ minutes** (unless you are very very **skilled**, very good), only $10$ times you hit a connection. These guys have $4$ electrodes, because perhaps you can realize that if I am poor, I must have at least two electrodes, otherwise I don't study synaptic transmission. So at least two must be there. Every time I insert them and ideally manage to hit two cells, I hold them stably, I manage to do, I have that with $n=2$ I have two possibilities. I could have A connected to B or B connected to A. I also have both, but conceptually there are two possibilities. Can you tell me what happens when $n=3$? When I have $3$ electrodes and therefore I can study $3$ cells at a time, which can be connected for their own reasons, so what is the number of possibilities? The probability, I already told you, is very low. In the cortex the connection probability is on the order of **$10\%$**, $0.1$, which means **$10\%$**. Probability is a number between $0$ and $1$ and so I like to express it as $0$ and $1$. If I have three, it means that every time I have to put an electrode on one cell, and I have two **targets**. Then I jump and I have two **targets** and I do it $n$ times, I do it three times. You see how many possibilities there are? And for $4$, for each one you put on and you have $n-1$ possibilities because you are already... you could study **autapses** which are synapses that the neuron makes with itself, but they are not necessarily... they exist and are important even in the cortex, but they would not be seen with this type of electrophysiological technique. $n \cdot (n-1)$. And if you see here $12$ compared to $2$ is an order of magnitude, it's a factor of $10$ approximately. So if you have $4$ electrodes don't leave science because more or less every time you insert the electrodes into $4$ cells you find something, unless they are very, very distant because obviously this probability could depend on the distance. If you have two you need to sweat a lot. That's why this **dataset**, this article and then the data it generated was very important. And with $10$ electrodes you can think that you have $10 \cdot 9$ (sorry, it's $12$, the world **record** is $12$, so it's $12 \cdot 11$ possibilities), so there it is practically certain that there are several connections. So in one go, $12$, you manage to study a lot of synapses simultaneously.

So this is just to show you, beyond the aspect of so-called **connectomics**, a relatively recent, young field of research, of the last **$20$ years**, in which people manage or would like to have the circuit diagram of the connections of a piece of brain. In this case the **connectome**, identified with this method, by brute force, was only between neuron 1 and neuron 3. Surprisingly it was a bidirectional connection. Could you tell me what the probability of this happening is, assuming there is no favoritism, it's wrong to say that, that there is no predominance of bidirectional synapses? The information you need is this. And it is the probability that there is one pair and that there is the other connection. Remember the story of the conjunction and disjunction of probabilities. Is it easy or difficult to find, if **$10\%$** is hitting one pair, to hit the reciprocal one? You could tell me that no, that's exactly the discovery, that there is an overexpression of bidirectional synapses, so that more or vice versa, an underexpression. Answer. But if you think it's simply that there is no dependence (I project towards you and you project towards whoever you want), you are projecting towards me, okay, there is no reason why if I establish a synapse with you because we are both pyramidal and I am spatially close, then you also contact me. If they were independent events, what would the probability be? Because it would be that you did $0.1 \cdot 0.1$, right? Yes. So it is a very small number and the whole story of this specific article is that it is more frequent than $1\%$ to find bidirectional synapses in the cortex and they also have other properties.

Now, let me focus on chemical synaptic transmission and then briefly return to electrical synaptic transmission. I have already introduced terminology, talking about **synaptic potential**, simply because it is not an action potential, there is nothing to understand. The **synaptic potential** is called the echo that is caused in the postsynaptic receiving neuron when there is a **spike** in the presynaptic neuron. And we talk about potential **P** or current **post-synaptic current, PSC** excitatory (so **Excitatory Post-Synaptic Current, EPSC** or **Excitatory Post-Synaptic Potential, EPSP**). It is obvious that I am considering and causing currents. The channels are in effect, through a change in conductance, in permeability, giving rise to an ionic current, so in effect they are currents. But if I have my electrode I measure the potential and it will be the cell with $C \frac{dV}{dt}$ that integrates that synaptic current. So I see a **blip**, I see a kind of change and I call that potential, but conceptually they are currents, indeed more correctly one should say a **change in conductance**. And this change in conductance can be excitatory-inhibitory.

The story of excitatory-inhibitory means only that these channels here have a **reversal potential** that is more positive or less positive compared to the resting potential. I remind you of the current formula, always in the ohmic world: $I = G_{\max} \cdot x \cdot (E - V)$. There is a maximum conductance, there is some fraction of channels, okay, these are ligand-gated channels, okay, always the usual thing, and then there is the **driving force**, $E - V$. This thing here depends on whether $E$ is greater than $-70$ or less than $-70$. If it is **$0$ millivolts** or **$20$ millivolts**... When this thing here is something that changes from $0$ and varies between $0$ and $1$, this activates, this over time probably does something like this, it activates and then deactivates. I interpret this rise and this fall. Then obviously the lion's share will be done by the conductance. If the conductance is very large it will have a very notable impact in terms of current, but the crucial term is this thing here. Where is $V$ located? In fact where is $V$ located at that moment? If $V$ was at rest, so if this was **$-70$ millivolts**, if this reversal potential $E$ is **$0$ millivolts**, $0 - (-70)$ is a positive number, the current is positive, it is a depolarizing effect, an excitatory effect. If instead this quantity was not $0$ but was **$-60$** or **$-80$**, suppose **$-80$**, so less than **$-70$ millivolts**, now $(-80) - (-70)$, so $-10$. You just have to pay attention to the sign. So you have a current that is negative, a hyperpolarizing current, a current that discourages crossing a threshold and therefore the emission of an action potential. So it is an inhibitory synapse. It is the postsynaptic receptor that commands, that says what it is.

Then in fact there is a custom that in neurosciences is called a law, but we don't actually know if it is a law, indeed there are exceptions, which is called **Dale's law**, which says that if I take a neuron, the neuron always releases the same neurotransmitter. Particularly for vertebrates it is always true, almost always true. That is, if I take this neuron, it either releases **Glutamate** or it releases **GABA**, it does not release both. In insects and other animals it can be different. So it seems that... Whether you are excitatory or inhibitory, and it is still a matter of the reversal potential, so it is a matter of **Nernst**, of that particular ionic species for which that ligand-gated channel is permeable, but it depends on the identity. So if you release **Glutamate**, you release **Glutamate** for all your projections. Or if you release **GABA**, you release **GABA** to everyone. So if I have several, my axon projects, it branches out, they are all spitting out the same neurotransmitter. It's not that I excite one and inhibit another. At least not in terms of neurotransmitter release.

---

## Types of Synaptic Receptors and Equivalent Circuit Model

There is also an important distinction between receptors. I think you should have seen this thing here. The first thing was this, that it depends on the Nernst potential. Yes, it is true that **Glutamate** is associated predominantly or almost exclusively with excitatory synapses. **GABA** is an exception because sometimes during development it has an excitatory, depolarizing effect.

And the other thing that perhaps now, in light of all the... of how much I have bombarded you about ion channels, how they are modeled, how they are described, could be a piece of cake. And it is the following difference: there are **ionotropic** receptors which are pores. They are exactly the ones I told you about: the ligand attaches to a certain particular point in the extracellular part and the conformation changes and a pore is created and the ions that were there on their own, to which this channel is sensitive, is selective, by virtue of the electrochemical gradient that was there and characterizes that type of ions, flow or cross the channel either from outside to inside or from inside to outside.

Note, the story of the ligand attaching to a particular site is exploited for psychoactive drugs. When you, I hope not, take **benzodiazepines** or other neuroactive drugs that have a fast action, you are attacking, you are mimicking, not necessarily at the same binding site, the effect of a neurotransmitter that is not there. I take **benzodiazepines**, **Xanax** and the effect is what I would have if a particular neurotransmitter had bound, which would be the **GABA** neurotransmitter.

In the case of synaptic receptors, they are named after the chemical substance that was first seen to open these channels. For example, for **Glutamatergic** receptors there are two important examples. One is called the **AMPA** receptor (**AMPA** is an acronym, I don't remember what it means, it has a very long name). Another is called **NMDA**. You can find a bottle of **AMPA** on the market, not of **AMPA receptor**, but of **AMPA**, which if you take it and pour it onto **AMPA** receptors, these receptors open because these are **agonists**. **AMPA** is an agonist of this **AMPA** receptor and **Glutamate** is also an agonist. Agonist means that it tends to favor opening. Otherwise there are blockers which are called **antagonists**. For **GABA** instead they are called... these receptors are called **GABA A**, type A. So they have another type of problem... **benzodiazepines**, open **GABA** receptors.

There is another type of receptor, however, which is called, it is not **ionotropic**, but it is **metabotropic**. And **metabotropic** suggests cascades of metabolic reactions, where it does not necessarily have to do with energy consumption, but it has to do with a cascade of **steps**, at least that's how I... because there is potentially energy consumption but that is not necessarily it. Where, that is, the binding of the neurotransmitter molecule in a particular domain in the extracellular part of the channel does not directly turn on a pore, it does not cause a pore to form. Now I will show you an animation in which a change in the conformation of this, which is still a membrane protein (so it has a domain part outside, in the middle and inside), moves and this movement activates a cascade of intracellular reactions, in particular it activates a protein called **G-protein** in the intracellular part. This **G-protein**, you will see it shortly, let's say it starts to interact with a potassium channel, which is there on its own, nearby, and activates it.

You can imagine that while an **ionotropic** channel immediately binds and the pore forms and the current exits, a **metabotropic** channel perhaps takes a few milliseconds, tens of milliseconds longer, because the conformation changes, this **G-protein** activates, moves, goes to bother the nearby potassium channel, binding from the intracellular side and the potassium channel opens. Now I have trivialized it and it seems fast. It can be particularly long.

This is the animation.

>**Video 1: GABA-A Receptor**
>
>In this video, we explored the **GABA-A receptor**. It's an important protein complex for receiving inhibitory signals from other neurons. This **receptor** can be found in **the membranes of the postsynaptic neuron**. **The inhibition works by changing the membrane potential**. **The membrane potential is given by the difference of the ion concentration between the extracellular space and the cytoplasmic side**. **You can see the ion concentrations here**. **The concentration of each ion varies depending on the location**. **The chloride ions are relevant for the GABA-A receptor**. **Their concentration is higher in the extracellular space**. **The number of green ions floating around here is about accurate**. **The cube marks an area of 10x10x10 nanometers**. **That volume of extracellular space would be expected to hold about 70 chloride ions**. **Let's focus back on the GABA-A receptor**. **This receptor consists of 5 subunits, 2 alpha, 2 beta, and 1 gamma subunit**. This is the most **common**, but other **subunit** configurations are available. When the **presynaptic neuron floods the synaptic cleft** with **GABA**, **then GABA can bind to the receptor**. **It changes conformation slightly, thereby allowing chloride ions to enter the cell**. **The opening of the channel is short-lived**. **The excess GABA is taken up by GABA transporters located on the surrounding cells within milliseconds**. **The GABA that is associated with the receptor can also leave the binding pocket again**. **Thus the channel closes, ready for the next signal**. Thank you.

It can be neuroactive, it could function or interact with the channels or with the nervous tissue in different ways, it could leave it open a little longer, etc., etc. One thing I didn't say is that normally the neurotransmitter, once he said it, once this interaction ends, which is still a binding, there is nothing magical, it's not that "now I'm bound, now I expel you," it either diffuses away into the extracellular space, or it is recaptured by the synaptic bouton, or above all it is recaptured by **glial cells**, which is another family of very important cells in the brain, in some species even more numerous than neurons, which we will not talk about in this course.

>**Video 2: GABA-B Receptor**
>
>**In last week's video we took a look at the GABA A receptor**. **This time we'll focus on the GABA B receptor**. **The name is similar, but as you can tell just by looking at the structure alone, the way they work is very different**. **GABA A is a ligand-gated ion channel**. **GABA B, on the other hand, is a G-protein coupled receptor**. **This animation focuses on the GABA-B receptor located in a synapse of a postsynaptic neuron**. **However, GABA-B receptors can also be found in other locations where they perform different functions**. **While GABA-A receptors use chloride ions to modulate the membrane potential, GABA-B has this effect by changing the potassium ion concentration**. **Potassium is positively charged and is more abundant in the cytoplasm compared to the extracellular space**. **Therefore, releasing potassium into the extracellular space makes the cytoplasm more negatively charged**. **Let's have a look at the structure of the GABA-B receptor**. **The receptor has two subunits, subunit 1 and subunit 2**. **These two subunits are held in close proximity by coil-coil domains in the cytoplasm**. When the **neurotransmitter GABA floods the synaptic cleft, GABA can bind to the binding pocket of subunit 1**. **This changes the conformation of both subunit 1 and 2**. **A G-protein can bind to subunit 2**. **The G-protein consists of subunits G-alpha, G-beta, and G-gamma**. **Lipid modifications on the alpha and gamma subunits allow the proteins to be anchored to the cell membrane**. **When the G protein binds to the receptor, GDP and G alpha can be replaced with GTP**. **This activation causes the G protein to leave the receptor, and G alpha separates from G beta and G gamma**. **In this step, a signal amplification can take place as one receptor can activate multiple G proteins**. **G$\beta\gamma$ can then diffuse along the membrane and bind the potassium channel GIRK**. **GIRK is a tetramer and has four binding sites for G$\beta\gamma$**. **By binding, the G$\beta\gamma$ facilitates the opening of GIRK**. **Potassium ions can flow out of the cell**. **The channel is deactivated when G$\alpha$ hydrolyzes its GTP and picks G$\beta\gamma$ back up**. **The final thing I want to touch on is GABA-B desensitization**. **KCTD12 can be bound to subunit 2 of the receptor**.

All these extra things are because this mechanism of a cascade of events naturally has (this is the phylogenetically most ancient way, in invertebrates many things work like this) automatically **for free** a kind of negative **feedback**. When many **G-proteins** are activated, then there are signals that tend to turn off or self-amplify. It is a very interesting thing that molecular biology has invented amplifiers, or thus an amplification with **positive or negative feedback**, which are used in electronics to perform computation. Here in particular, maybe there was only a little bit of **GABA** and I want to amplify it and I can amplify it with this cascade of intracellular reactions, but at a certain point I want to stop, I want to inactivate because maybe I've been keeping the cell inhibited for too long, and again there are mechanisms that are **negative feedbacks**, they tend to block this **G-protein** activation.

Let's take a $10$-minute break.

Well, the message that you must engrave in your mind is fundamentally this. For excitatory (and there will be another **slide**), inhibitory, the **Nernst reversal potential** of the ionic species to which those ionotropic or metabotropic channels, indirectly, are permeable, counts. If this is more depolarized compared to the resting potential, then it is a depolarizing excitatory effect, otherwise inhibitory.

And so this excitatory synaptic transmission can be **fast**, in the case of excitatory synapses, **fast** and these are **AMPA** receptors. Or it can be a little **slower**, but they are still **ionotropic** and they are... okay, no, here I should have said no, the fast ones are the **ionotropic** ones because I read the neurotransmitter, the pore opens and it's done. In the **metabotropic** case it is much **slower** because the receptors activate a protein, an intracellular cascade that activates an ion channel, etc., etc. So the difference is **fast**. Fast means that the activation is on the order of a fraction of a millisecond. So here there is the presynaptic **spike** and here there is the postsynaptic membrane potential, one can often write $V_{\text{post}}$, this would be $V_{\text{pre}}$. After a certain delay this goes up very quickly and the activation comes, the turn-off kinetics are fast. Here the time constant is on the order of $1-5$ milliseconds for **AMPA** receptors, they are **glutamatergic** receptors. While there are other receptors, still **ionotropic** though, that we will see because they are interesting, they are very important especially for memory, where this tail is a little **slower**, but slow on the order of **$50-100$ milliseconds**. These are called **NMDA**.

While there are much **slower** **metabotropic** receptors where this thing here becomes hundreds of milliseconds, that is, here the activation tends to stay up and go to zero very slowly. This is on the order of, suppose, **$500$** or **$1000$ milliseconds**. These are called, for example in the case of **Glutamate**, with immense originality of names, **GLUMR**. **Metabotropic Glutamate Receptor**. It's easy to remember. Okay, another important receptor, particularly because it is also linked to the **neuromuscular junction** (this chemical release thing, makes muscles work, and is therefore something that I believe was so **successful** from an evolutionary point of view, from the point of view of movement, of electromechanical transduction, it has become ubiquitous from the electrical point of view of information signal transmission). So acetylcholine receptors are called **acetylcholine receptor**, with maximum originality. And all of these, even if they are... This means that it is activated by a substance called **AMPA** or also by **Glutamate**. This is a substance called **NMDA** or also by **Glutamate** and this is a substance called **acetylcholine** and it is **acetylcholine**. I don't remember the name of some selective agonists. Some are called competitive, competitive because they compete with the endogenous neurotransmitter right for the cholinergic receptors. I don't remember if there are substances, there certainly will be. And there are substances that block them, but I won't talk about them.

While the **metabotropic** ones you have, sorry the M goes first, it's **mGLUR**, not **GLUMR** but **mGLUR** (**Metabotropic Glutamate Receptor**), sorry. For **GABAergic** inhibitory synapses, it's simpler. There is **GABA A** and **GABA B**. **GABA B** is **metabotropic**. More or less the kinetic constants are similar. The **metabotropic** one is on the order of tens, hundreds of milliseconds. **GABA-A** is on the order of a few milliseconds, a little slower than **AMPA**.

In this case, all these receptors (this **glycine** receptor is representative of the central nervous system in the spinal cord and is activated by a neurotransmitter called **glycine**, because in the peripheral nervous system there is no **Glutamate**, **GABA** or at least there are other neurotransmitters that are important). I'm having doubts about whether the **spinal cord** should be called the **central nervous system** and not the **peripheral nervous system**. I believe it should be called the **central nervous system** and the **peripheral nervous system** is the periphery, not the spinal cord. However, this **glycine** receptor is typical of the spinal cord.

In all cases, these receptors or the channel that is then activated by the conformational change of the **metabotropic** receptor, are such that they are sensitive, selective to an ionic species whose Nernst reversal potential is lower than the resting potential and that is why they are hyperpolarizing, **alias** inhibitory. It's not because this molecule, **GABA**, the **GABA** neurotransmitter is particular, sorry, the neurotransmitter is particular, it is the receptor that says it all.

How do you combine the two worlds, the electrical world of the charge balance equation with the story of synaptic transmission? Nothing could be simpler: it is as if you had as many branches as there are ionic species or ion channels selective to different ionic species, ion channels of the same type. Here are the **fast inactivating** ion channels selective to sodium, here are the **delayed rectifiers** selective to potassium, here are the chloride channels which are not voltage-dependent. Another branch, and this other branch does not necessarily have this arrow that graphically means: "Look, the value of this resistance or conductance, which is the same thing, changes over time depending on the potential." Here it changes over time depending on whether or not there is a neurotransmitter in the **cleft**, in the intersynaptic space. If there is, then these receptors begin to open, if there isn't, they stay closed. So here this conductance and ultimately this current, this magnitude here, will have a kinetic scheme, open, closed, which we will see now.

So nothing could be simpler, paradoxically in the context of what I have obsessed you with so far, the story in particular with the ohmic description, which is more than adequate for this type of current, in the case of chemical synaptic transmission.

For the electrical one?

For the electrical one it's always the same thing, but in the sense that there too I change, I add another branch, but here you did not flinch from the fact that there is also a battery, an ideal voltage generator, which is the Nernst potential of the ionic species for which that particular ligand-gated channel is selective, to which it is selective. In the case of electrical synapses it is a bit different because here these **connexins** could be selective to different ionic species, maybe to multiple ions they could have less pronounced selectivity and simply be permeable to different ionic species. Do you have any intuition? It is literally a piece of wire.

Do you have any intuition about what the expression to describe the current due to a **gap junction** could be? First of all, as said, it is something that concerns both the (I call it) presynaptic neuron and the postsynaptic one. In reality there is no big distinction, in the sense that they are both instantaneously **pre** and **post**. So somehow I have a charge balance equation for neuron 1 (this would be the current due to the **gap junction**), and then I have another equation (I am neglecting the various terms for sodium, potassium, chloride, **leak**, blah blah). Here you should tell me what the heck I put. I repeat, here you have a membrane, you have a resistor and you have another membrane. I'll make it even more explicit. Here the potential across this membrane is $V_1$ and here it is $V_2$. What is the expression of this current? What should I put here in the other one? Great, perfect. Exactly, exactly.

And here fundamentally this potential difference, literally here it would be from here to here, yes it's true, I should apply **Kirchhoff's** loop rule, but I see with my eyes that here it is $V_1 - V_2$. This difference across this resistor is $V_1 - V_2$. Your colleague simply invoked **Ohm's law** and said that yes, the current inside a resistor is this difference divided by $R$. He wasn't necessarily thinking about the direction of the current, but if I do things with all the trimmings, imagining, remembering the consumer convention, for which if I take this arrow, this positive potential in this direction, then **Ohm's law**, $V = R \cdot I$, or $I = \frac{V}{R}$, is positive in this other direction. So, here, since the current is entering, I actually write this thing here:
$$I_{\text{gap}} = \frac{V_1 - V_2}{R_{\text{gap}}}$$
where we call $R$ $R_{\text{gap}}$. I think $R_{\text{gap}}$ will appear now. And up here, however, you see, here it enters neuron 2, here it is leaving neuron 1, so here I must put the same current with a minus sign. I put the same current apart from the minus sign.

Then I get carried away by my usual paranoia and I say: "Wait, I am... it's true that here I have a differential equation where the state variable appears here with the sign okay and here it's true that okay because there is the minus sign, so it doesn't make me explode, it doesn't make positive exponentials that explode come out." And from here I see two things: the first is that there is this symmetry, in the end it's the same term apart from the minus sign, but the fact that the synapse is excitatory or inhibitory (because this term here and or this term here, clearly cannot be simultaneously, this one is positive or negative) depends on what those two neurons were doing, $V_1$, because $V_1 - V_2$. If the two neurons are, for example, synchronized in the emission of their action potentials, if they were exactly two exactly identical cells and exactly synchronized, this term would no longer be there, because it depends on the difference. So if two neurons are exactly synchronized, roughly speaking (because obviously they will be different, they will necessarily have different capacitances, they will have slightly different surfaces, they will have sodium, potassium, etc. conductances, they are not exactly the same thing). but if they were, two exactly synchronized neurons doing the same thing on their own, they no longer influence each other through a **gap junction**.

Conversely, and this is the thing that is easy to understand once you remember **Ohm's law**, which is therefore useful also in biophysics and biology, if $V_1 - V_2$ is positive, then the synapse for this neuron 2 is excitatory, because the current tends to be positive, it tends to be depolarizing, while in the other neuron it is the opposite. But as long as $V_1$ is instead less than $V_2$, things change. So here effectively, I am connecting them, let's say, with the equivalent circuit, it is perhaps better seen, but in the end even with the two **blobs** I hope you can swallow it without too many objections. And here I don't have a **Nernst** potential because in the end it behaves more like a **continuum** between the cytoplasm of one neuron and the cytoplasm of the other neuron.

So I understand the **signless** part now because it depends on the sign, it can be positive or negative, depending on the activity at that moment. The fact that it is **bidirectional** I see because I cannot escape if you are making a **gap junction** to me and I am making one to you and we have the **connexins** aligned. And the fact that it is **slow**, this is not immediately easily seen, unless you are electrotechnical experts for whom you could say: "Stop, here you have the capacitance and here you have the resistance, somewhere you have $RC$." So this thing here if it is on the order of tens of milliseconds it is slower than the **AMPA** receptor that I erased just now which went up quickly, it went up in a fraction of a millisecond. Here what does a filtering, what conditions the coupling of signals are the **RC** properties of this contraption. This contraption here is an **RC**. That is, if I imagine having some interesting signal here, yes, I see it at a certain point here, but I have a resistance here and a capacitor here. So if you consider one of the two neurons as a signal generator, you discover that the membrane potential in the other neuron is in the **low-pass**, filtered, **low-pass** version, simply due to the capacitive and resistive properties of this contraption. So here I can be as fast as I want, but this is slow to integrate signals, particularly because this **gap** resistance is very large, in the sense that **connexins** are not excellent pores.

And in fact we saw here that the echo was quite small, this was something like perhaps **$2$ millivolts**, so yes it works, but it's not that this kinetics is particular, it reflects what the kinetics of the first neuron is in the case of **spikes**, that when the second neuron were to have, in this case I am the one injecting a negative current and it goes down. My goodness, it goes down slowly and this guy follows, follows very reduced with a factor, called the **coupling factor**, very low, it is not one-to-one, these **connexins**, this resistance is not zero, and it is slow as much as the membrane is slow to integrate signals.

So probably evolution favored the development or constitution, particularly perhaps for the **neuromuscular junction**, which works (the **neuromuscular junction** requires that there is not necessarily, yes but yes, a geometric proximity between the synaptic bouton and the muscle), but it is not said that the muscle must express the same **connexins**. I am the one spitting **acetylcholine** at you and you contract because you have cholinergic receptors. And it is faster. Perhaps chemical synaptic transmission was adopted precisely because it is much faster than this one and so many of us are perhaps better than insects.

This is exactly the same formula and you notice that I am... here I have simply written the **Thevenin** equivalent circuit without worrying about whether it is sodium channels, potassium, etc. The thing that reassures me is that this state variable, as mentioned, appears here also with the minus sign and this also appears here and it is another minus sign. This is a way, if you cannot remember this electrotechnical aspect, you can remember that I, on the other hand, get agitated if you make exponentials that explode, because nothing must explode here, it is a dissipative system, there is nothing that self-feeds that can amplify here amplification is not there. So the synaptic current term in this case again also becomes an arc... sorry a branch in this circuit. It does not have the **Nernst** potential but it has the potential of both neurons, so it depends on **pre** and **post**.

Before telling you this story, I wanted to insist and say: if you are brave you could try to implement not one of the **Hodgkin-Huxley** models, but two. Simply copy, paste and change the name of the variables. Here I couldn't call it $V$ and $V$, otherwise you would have said: "But it's the same $V$." So you could try to take two independent **Hodgkin-Huxley** neurons and couple them with an electrical synapse. You might discover that if you inject the same current into both these neurons, for example, but use different initial conditions, if the **gap** conductance were zero (so the two neurons were not talking to each other), you would see the two **spike trains** out of phase. As soon as you connect this, so when you redo the simulation with this, you would see that progressively over the course of a few **spikes**, perhaps one or two, the neurons synchronize. The reason why in invertebrates, in particular (I told one of your colleagues, I forgot to say it), the heart cardiomyocytes work like this with **gap junctions** and what is the reason? They must be ultra synchronized to contract at the same time. Yes, it's true, there are spatially distributed contractions, exploited for example by **pacemakers** that make one part contract and then the other, otherwise the heart would not have the effect of a pump, if everything was synchronous. But beyond this aspect of propagation and which is also linked to the activation of the nodes, so of the nervous system of the heart, the tissue must react as a **whole**, even though it is made up of distinct cells. This is why these **gap junctions** are there.

And in the experimental case that I told you about at the beginning, where but perhaps only invertebrates, but okay, perhaps also vertebrates, but only in the initial phases of development. No, even in the adult there are in particular subpopulations of identified neurons, in particular they are families of **interneurons**, so **GABAergic** neurons that spit out **GABA**, and therefore presumably have an effect, when they are excited, they inhibit the others, they are connected by **gap junctions** probably because inhibition, in addition to having a role in information processing, is very subtle which I am not going to tell you about now, but it's not simply that excitation does everything and inhibition only serves to prevent epileptic seizures. But from that interpretation of containing excitation in the end inhibition is a brake. It might be useful to have that even if spatially distributed, in my cortex they are several millimeters in all directions, it's not that all inhibitory neurons are there at that point, they are distributed, it might make sense that they must behave globally in the same way, that is, they must be synchronized.

A term used by some researchers is that inhibition can function like a **blanket**, a duvet, a blanket, a blanket that therefore has an instantaneous effect on an entire network. Imagine that different points of your visual cortex represent, by activating, different points of the world. Now, apart from seeing your faces illuminated, I see them distinctly. Now I'll say something silly that might not be so stupid, if in particular, even without staring at one of you, if one of you particularly, I don't know, I say: "Damn, I must have run into this person at the supermarket the other day," it may be that due to this effect of increasing contrast, which means, in my cortex, since you are in different points on the retina, even in my visual cortex you are, due to **retinotopy** you are distributed, you are represented in different points, your faces. It may be that if I have an overall, global inhibition, not local, but global, I can make, let's say, stand out, for example, the one of you who has the whitest, palest face (because now I'm being silly), because maybe it reflects better, I don't know, because you got less sun, I don't know, because I don't know. In this case the effect of inhibition makes me turn off all the competitors, all these **foci** of excitation and perhaps makes the most important one win, what from a computational point of view is imagined (it's not certain, let's say it doesn't always exist, we don't know if this mechanism exists exactly), is called **winner takes all**, which yes is an **ABBA** song, not an equation, I wish it were an **ABBA** equation, but it means that it is a method to increase contrast and only the strongest one wins because the others are suppressed. To do this I need inhibition that is common and global, even if I have one inhibitory neuron and I have it here. No, I want all inhibitory neurons to be synchronized. I'm not saying that **gap junctions** have this as their only role, people don't know. In particular that type of inhibitory **interneuron** population has both **gap junctions** and chemical synapses between them and so it is a big mess and so **bottom line** we don't know.

One interesting thing, so to try to link it to something engineering-related which, in addition to the clearly modeling, mathematical, descriptive, mechanistic aspect, I would like to quickly present a concept that goes in the direction of this **connectomics**. As for chemical synapses, it is hugely important to be able to understand what the connection diagram is, and this is done in a brute-force, very difficult way, by putting in a lot of pipettes, making one neuron fire, seeing the echo of the others and so on. When there is a connection, I know there is an anatomical connection, when I see the echo, when I see for example an excitatory postsynaptic potential or an inhibitory postsynaptic potential.

In the case of **gap junctions** I can imagine that, considering the **steady state**. And if I inject a current here, if I have a pipette here, if in the middle there is only one jump, so there is no chain of cells connected by a **gap junction**, I might be connected to this cell, but in between... what if there are two others. The only thing I do is I put an electrode in one, an electrode in another, I depolarize, I hyperpolarize and I see that in both cases I have coupling, but I don't know if it is a very, very weak coupling or a coupling... so it is very weak because the **connexins** are few, or it is very weak because in between, and I don't see them anatomically, there are two other cells that are intermediate.

And intuitively, this is something I understand here. Here I only have this conductance of this neuron, then I have this other resistance, in the end it is in series, they are all in series, this is in series with this, which is in series with this. So if by chance there was another **gap junction** and another cell and another **gap junction**, it would be a series, a chain of resistors. So from the point of view of the amplitude in this **DC** regime, where capacitors disappear because transients disappear, if I see a very, very weak coupling, so I hyperpolarize or depolarize and I see a very, very small echo in the other neuron, I could say: "Ah, damn, so from a topological point of view there are several neurons in between," or, and I couldn't discriminate this, "there is a single **gap junction** but with this resistance value which is very high, so it is poor coupling." This is what I have in mind.

I put in pipettes randomly, I don't know exactly if this, assuming this is a chain, is a simple case of a one-dimensional network, the dimension doesn't change much. I don't know that 2 and K have proximity, so they are connected but not directly, they are indirectly. So it's as if I could, I would like to infer a distance between the connections, because if I have the distance I could perhaps reconstruct the connectivity, I could reconstruct the topology. It could be interesting. I could see as if I opened a cell phone, saw the electrical diagram, I could infer, in the end it might be almost impossible. If I open an **iPhone** I can't get an idea, maybe roughly, of where the battery is and where the **CPU** is, but theoretically I could infer function from structure.

And for what I told you, in the **DC** case it is quite complicated. The difficulty is this ambiguity. I could have, in this simple case, a cell in between that attenuates it a lot. Suppose I inject into 1 and record in 3. I could do the opposite, but I also have 2 in between. Or I have 4 and 5 or 6, whatever it is, they are with a single **step**. If you want I could define a kind of distance. Here the distance between 1 and 3 is 2, because there is an intermediate jump. Here the distance is unitary, there is immediate proximity, but you see that here it is done on purpose, here the resistance is very small, while here it is a bit larger. So I might, when interpreting the two situations, not be able to distinguish the case of a network like this, of a topology like this where there is an intermediate neuron, if I only look at the difference between $V_1$ and $V_3$, I say: "Okay $V_3$, you are attenuated, but you are attenuated because 2 was in between." Suppose I don't see 2, I see $V_1$ and $V_3$. Here I see $V_4$ and $V_6$ and here I see practically the same thing and so I could conclude and say: "There is a cell in between," but no, they are directly connected.

Interestingly, perhaps it is not particularly interesting for you, if I look instead at the **transient** and not just the **steady state**. The **steady state** I cannot distinguish, this is an example that I constructed specifically for this ambiguity. If I look at the **steady state** I realize that the curves, the time constants, the exponentials have slightly different kinetics, because here I have a capacitor here, I have a capacitor here, I have a capacitor there are only two capacitors, not three. So if I could, I did it, analytically write the differential equations that go from one to the other, I would see that I have more terms, more capacitors, a first-order equation is not enough for me, I have to write a second-order equation. I don't like working in time, so I don't like seeing these **transients** like this, because they are very difficult. Experimentally it is impossible to see something like this, because of that noise, the amplifier noise but also the channel noise. Yes, I see that there is a minimal difference, now I don't know which is which anymore, the dashed part and the continuous part, but the curves are not superimposable, so much so that I see that the dashed ones are slightly different from the continuous ones.

What I can do is, since engineers do that for a living, I inject **sinusoids**, so I move to the **Fourier** domain, I study a **permanent sinusoidal regime** because I know, or hope, in particular I don't inject one frequency at a time, I inject a **chirp**. It is called a **chirp** because if you listened to it on a speaker it would have a whistle that increases in frequency like a bird's chirp. I do this here, you see that it becomes more and more, the oscillations are visible, they are of the same amplitude and they are visible at the beginning because they are slow, then it accelerates, accelerates, accelerates. It accelerates up to, I think, **$50$ cycles per second**, so it doesn't go to kilohertz frequencies, but it is enough for me to induce in a linear, **RC** system, which some of you will know can be written, so the differential equation can be written as an algebraic equation in the frequency domain, as a different phase relation. If I look at the two situations, I compare $V_1$ and $V_3$ or $V_4$ and $V_6$, I don't have the same, so if I have a cell in between I have an extra **phase shift**. Technically this is true at infinite frequency, in reality it is also seen at relatively high frequency.

And what we did experimentally is, since these inhibitory **interneurons** are connected to each other with **gap junctions**, if I **patch** two, can I by chance know or have an experimental method to say: "Are they directly connected or is there an intermediary?" Something even more interesting, but I won't talk about it, is that neurons are not balls, unfortunately they are not these circuits, that is, they are not structures that are single compartment, with **lumped parameters**, they are distributed objects. What happens if I have my axon or my dendrite and they are two dendrites that touch distally, distal from where I have the pipettes, it's as if I had a cable, an electrical cable, an ohmic connection there and I realize it from here. Anyway, to make a long story short, this is a method literally taken from electrical engineering and describes a technique to infer the **electrical connectome** of a network of neurons connected by electrical synapses.
