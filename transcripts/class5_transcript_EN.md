## ðŸ§  Neuronal Excitability: Ionic Bases

Alright, today I'd like to continue with the topic of neuronal excitability, and I'll probably open a parenthesis, a sort of preliminary on excitability, at the end or during the second hour. You can find both slide files separately; they are already on the site, in the repository. I'd like to start from what I mentioned last time would be intuitions worthy of a Nobel Prize. Indeed, even if I'm not certain this is what motivated or fueled the correct intuition of **Hodgkin** and **Huxley**â€”two Nobel laureate electrophysiologists in the '60s who, in the '50s, clarified the molecular basis, if you will, of the emergence of electrophysiological signals, particularly the action potential.

And I'd like to do this by considering an axis: this is the membrane potential axis, so I'm thinking about the electrostatic potential inside versus outside. Outside is conventionally zero for me, because you know that any other choice is possible; I choose to put the reference in the extracellular medium at zero. And I told you last time that, based on our considerations, there are specific consequences of the different ionic concentrations of the various ionic species inside and outside the membrane. For example, where is **sodium** most concentrated? Outside, thank you. **Potassium** is the opposite. **Chloride** is also highly concentrated outside, but remember that from the point of view of valence, having a negative sign, it counts as $-1$ in the context of the Nernst potential or reversal potential I'm thinking of.

Because, in a way, as I did at the end or middle of last week's lecture, I'm thinking that sodium, being concentrated... so again I'm thinking of the $\log(x)$ function, somewhere in Nernst, in the Nernst potential, there is the ratio of the concentration... sorry, the logarithm of the ratio of concentrations, and I always forget but I think it's *out* here and *in* here, because in fact, for sodium, I know it's positive. Do you roughly remember what value it had? It was positive because the logarithm is positive when the argument is greater than one, and the argument is greater than one when the concentration outside is greater than the concentration inside. And do you roughly remember the value? What were the units? Millivolts, so it's up here, it doesn't matter, the important thing is that it's quite high. Whereas for potassium, you'll remember, the situation is reversed, so it's definitely a negative value, and it was a value around **$-80 \text{ mV}$**. You might also remember, for chloride, I showed in a forced, contrived way... frustration because it wasn't exactly the **$-70 \text{ mV}$** I was looking for.

So, you have a reversal potential for potassium currents ($E_K$) that is very low, and a reversal potential for sodium currents ($E_{Na}$) that is very high, very depolarized and very hyperpolarized. And if you imagine, over time, taking an excitable cell and somehow, for example, you are the one injecting an ionic current inside the membrane, thus making the membrane more positive... because it's you, for example, with a glass pipette filled with a chloride solution and having that famous silver-chloride wire, which I remind you is what makes it possible, as a first approximation, to exchange between one world, that of metallic electrodes that have electrons as charge carriers (I'm also thinking of semiconductors, holes, but anyway, very different from the charge carriers in an electrolyte or an aqueous solution with charged ions). If you do this and give these *steps*, these current steps, you expect that unlike a passive cell, a cell that isn't excitable, where the reaction is boring, it's the reaction of an **RC** circuit, a charge and discharge with exponential arcs. We did it together, and I hope some of you follow up on my requests when I say "try doing it as an exercise," in the end, it's Calculus 1 or at least elementary math of solving differential equations.

You know that the charging curve in this case doesn't look like that. This is a *sketch*, it's not exactly the shape of an action potential but it's very close, where okay, there's a charging curve but then there's a kind of phenomenon that isn't described by that physics, that biophysics of an RC circuit. And the thing one notices is that this behavior is more or less always contained between this maximum value around **$+50 \text{ mV}$** (actually calcium, the Nernst reversal potential for calcium, since the concentration inside is practically zero and outside is a few millimolar, is even higher, anyway $100 \text{ mV}$ or $50 \text{ mV}$ here) and down here **$-80 \text{ mV}$**.

So this *range* might suggest, and it did suggest to brilliant minds who figured it out when even from an experimental standpoint, the
possibility of experimentally accessing a biological preparation like a neuron, a cortical cell from a rat, was surely very difficult if not impossible. They based their intuitions on these considerations, so on the biophysics of membranes and on the observation that at rest, unless the cells are dead, all cells are polarized, they are negatively polarized, and excitable ones can depolarize, meaning they lose their negative electrical polarization. It's all outdated jargon, if you will, because why not just say "the potential is negative, it becomes positive"? Oh well, okay, they say "the membrane is polarized," which is not trivial, it means that evidently, as we've tried to flesh out in recent weeks, there is some energy expenditure that changes the concentration, the distribution of ionic concentrations.

Excitable cells have the added fact that this electrostatic potential changes rapidly, it depolarizes, it becomes positive, it even reverses, not only is the polarization lost, on the contrary, it becomes positive, but it still doesn't exceed the values that are roughly those of the reversal potential due to sodium and calcium. And in the end, it repolarizes and even hyperpolarizes, because you see it goes lower, further down than the resting value, and even in this case, it doesn't go to an arbitrarily negative value, it touches that value.

So why the Nobel-winning intuitions? Because the endpoint of all the previous part that I titled "neuro-electronics" led us to formulate a relatively simple equation because it's formulated in the Ohmic case, although we know how to write it in the more general case where it's always an approximation (I'm thinking of the Goldman equation and the use made of that Goldman equation to write the ionic current terms when we don't want the simplification, the simplicity of the Ohmic approximation style). But when we settle for the Ohmic approximation, we can, in fact, write that the resting potential (now maybe I'll relax, I'll remove the word "resting," but anyway, under certain conditions, if there's a *steady state*, if it's at rest, not at equilibrium, because I wouldn't wish thermodynamic death on anyone) can be considered the weighted average of the Nernst resting or reversal potentials of the individual ionic species, so weighted by the membrane conductances for each species, divided by the total sum of the conductances.

$$V_{rest} = \frac{G_{Na} \cdot E_{Na} + G_{K} \cdot E_{K} + G_{Leak} \cdot E_{Leak}}{G_{Na} + G_{K} + G_{Leak}}$$

I urge you again to check the units, maybe I don't need to tell you, you are already very experienced. If I expect this to be millivolts, because here I measure millivolts, I have an oscilloscope, I have a multimeter that shows millivolts, the electrostatic potential... here I check that obviously $G \cdot E_{Na}$ must be... it's not millivolts, it's a current (millisiemens times millivolts) it must be divided by millisiemens, so it makes sense to me that there's an expression like this, because at least dimensionally it works, beyond the fact that we derived it together.

Staring intently at that expression and remembering the idea of a **weighted average**, perhaps you might have developed the intuition of the center of mass: if I somehow take most of my mass, I move it, the position of this center of mass moves. In this case, if I... it's the only thing I can do... given that I'm not changing the ionic concentrations (I'm certainly not changing them as rapidly as in a millisecond, I can't move the sodium that's outside, put it inside, and vice versa), so the thing that presumably changes, as intuited at the beginning of the last century, is that the **conductances** can vary, the ionic permeability of the membrane can vary.

So you might see that depending on the numerical value of these $G_{Na}$, $G_{K}$, $G_{Leak}$, the potential, for example, the resting potential, can lean more towards $E_K$ and $E_{Leak}$ ($-80$ or $-65$, whatever) rather than being $+50$. If calcium were also there, it would be even more evident. So, looking at this equation, you might intuit that if things start to move very, very quickly, with traces shooting up and down $100 \text{ mV}$ in a fraction of a millisecond, clearly something is changing with these coefficients: $G_{Na}$, $G_{K}$, and $G_{Leak}$.

So what I'm doing is, again, invoking *steady state*, even though in general you'd tell me "but it's not *steady state* because the cell is firing, it's emitting action potentials." But if you'll allow me, this is still functionally useful for me to understand what can change.

So this was what we called the **Thevenin equivalent** or anyway the way in which, by hypothesizing that the membrane was passive, I could for simplicity write a single conductance term and a single resting potential term, by studying the limit, the equilibrium, the *steady state*. So assuming that, for example, as $T$ tends to infinity, without having current terms (or by putting in current terms but stationary ones), at a certain point the potential tends to have a fixed value. And this fixed value depends on the combination of these Nernst potentials and these values, $G_{Na}$, $G_{K}$, and $G_{Leak}$.

If you love math, or if instead you love it more if you're visual, you could compare that case where what I did was set $G_{Na}$ and $G_{K}$ to zero. If I do that, it's as if... zero conductance means no more current passes, it means infinite resistance. And the circuit equivalent of a resistor with infinite resistance, or a resistor with zero conductance, is an **open circuit**, an open. A thing that has two terminals, one here and one there, and there's nothing there, it doesn't even have a capacitor, they have nothing, the current doesn't pass. An open circuit. But from this expression, mathematically it's easier, because $G_{Leak}$ and $G_{Leak}$ cancel out, since in the denominator $G_{Na}$ and $G_{K}$ also went to zero. And so it's obvious that everything is shifted to $E_{Leak}$, which I remind you is simply an easy way to group all the ionic conductances that are not selective, not particularly selective, they are partly sodium, partly potassium, so with a mixed permeability. And you can also think of it as just the conductance due to chloride, it's not that crucial.

Suffice it to know that this quantity is on the order of **$-65 \text{ mV}$**, $-70 \text{ mV}$, it doesn't matter. Graphically, I've torn out those resistors, and if I look at this circuit, even if I understand nothing about electrical engineering, I can think that from here to here, I can theoretically disconnect the capacitor. Because in a regime like the one I'm studying, in the so-called **DC regime** (a regime where quantities no longer vary in time; DC traditionally stands for *Direct Current*, but it has come to mean a state where signals in an electrical circuit no longer change in time), as opposed to AC (AC regime, which again would mean *Alternating Current*, but alternating because it's necessarily *time-variant*), while DC means stationary, it means constant.

And this is a constant case. You know that the capacitor equation, the constitutive equation of the capacitor component, which ultimately survives here, $C \frac{dV}{dt}$... becomes $I = 0$ because when $V$ is constant (this is why they say the capacitor "blocks" direct current, because when quantities become constant, the derivative is zero), $I = 0$ remains. And $I = 0$ is the constitutive equation of an open circuit, which means "no current flows."

So go ahead and use Kirchhoff's laws if you want, the current doesn't flow there anyway, it's a branch that doesn't exist. So that's why I've put this capacitor in gray or made it evanescent. What remains is that between here and here there is a potential drop that is the membrane potential under these conditions, when the sodium and potassium conductances are zero. So it's as if those (they are ion channels) are squeezed, are closed, they are closed doors. Sodium and potassium ions no longer pass, the only ones that pass, pass through that $G_{Leak}$. And it doesn't matter what $G_{Leak}$'s value is, if $G_{Leak}$ predominates over the others, $E_{Leak}$ wins and the potential is equal to $E_{Leak}$. Do you want to see it from a circuit point of view? Between here and here, the voltage drop is theoretically linked to the sum (Kirchhoff's loop law), it's linked to the sum of this ideal voltage generator and the drop across this resistor. But the current here is zero (you can apply Kirchhoff assuming you're not injecting current, this current is zero), so there is no voltage drop here. Therefore, between here and here is the same voltage drop as across that voltage generator. Two ways (if you prefer algebra, not even math, algebra, or if you are visual people for whom you've disconnected branches and this remains, this wins) you can probably understand where I'm heading.

So in this context, if I leave things as they are, the membrane potential across that capacitor stays around $E_{Leak}$. If you now suppose that I have $G_{Leak}$ (so I leave this branch as it is, I don't touch it, I assume it's passive), while the branch (so this one) that represents the ionic permeability for channels selective to sodium ions, I now say is no longer zero. I'm thinking of opening these channels, I'm not telling you how much I'm opening them, it's not that important.

Again, mathematically, you see that here it's the weighted average between these two quantities, $+50$, $+60$, I don't remember what it was, $+56$ and something (it depends on the species, naturally), but anyway, it's on the order of $50 \text{ mV}$, and this value here, $-70 \text{ mV}$. So it's clear that the potential is intermediate. And the more this weighted average is skewed towards this value $E_{Na}$, the more the membrane potential will tend to approach $E_{Na}$. In fact, what happens if I leave things like this and allow the circuit, or this equation, to reach (this equation here) the new *steady state*, with this amazing animation, it's as if I expect that with its dynamics (dynamics due to the time constants involved, whatever $\tau = RC$, time constant... obviously $RC$, okay, yes, the capacitance is fixed, the conductance here would become... the time constant would be $C/G$, $G$ is obviously changing in time), so the way this system relaxed to $E_{Leak}$ a moment ago might be different from the way, the temporal dynamics with which, the speed with which it now goes towards that other, this new resting potential. So it's clear it goes towards... it doesn't reach it because this $G_{Leak}$ is not $0$. If it were $0$, then yes, this term would no longer be there, it wouldn't be in the denominator, $G_{Na}$ would cancel out here. In reality, it's a bit below, it depends on how large $G_{Na}$ is. Nevertheless, it goes up.

Now I'll show you what happens, the other case: if I turn off $G_{Na}$ and turn on $G_{K}$. So I've ripped out this branch, set the resistance to infinity or the conductance to zero. The same thing happens here, now I have the weighted average between $-80$ and $-70 \text{ mV}$, okay, it will go towards $-80 \text{ mV}$. Again, it depends on the numerical values. Again, with this fantastic animation, the trace goes down, with its own dynamics. Again, dynamics due practically to an RC circuit. I repeat, the time constants obviously depend on the capacitance, but these conductances change in time, so it may be that the time constant is not strictly fixed. In other words, the speed with which I can go to some *steady state* (it's not guaranteed this *steady state* exists, but if it did exist under the conditions I told you, i.e., I rip out an element and then wait even an infinite time, I have patience) could be different, it could be much faster to go up or to go down, or vice versa.

For now, I'll do the following: I'll remove $G_K$ again, put in $G_{Leak}$, and from this initial condition, the system evolves again. You see that simply with a stupid observation on the concept of a weighted average, imagining that these coefficients, these weights, have a biological meaning (they are biophysically the permeabilities, but they must have to do with some mechanism that allows ions to cross the membrane). Cross the membrane passively, in the sense that it's not against the electrochemical gradient. In other words, since there's a lot of sodium outside, if I open the door, some sodium will enter by pure diffusion. This is what I call displacement current, but it's passive, it's not against, it's not like if I have so many sodium ions outside that I have to spend energy to push more out against what would be the spontaneous gradient.

And in this way, somehow, I've intuited that if the shape of the action potential is that it starts from here, goes up, slows down, stops, goes down, but goes much further down than the value it was at before, and then stops, slows down, stops, and then comes back up, it means that somehow there must be some sequence of permeability changes. And this... this permeability change cannot happen at the same time. I can't have the sodium and potassium channels (the sodium channels and potassium channels) opening instantaneously, synchronously, because otherwise, I wouldn't have this shape. The typical duration of an action potential is on the order of a millisecond, one or two milliseconds, it depends on the species. Cardiac action potentials, I think you'll see them if you haven't already, are much longer. Ditto for the action potentials of pancreatic beta cells, which you'll never see, but I'm telling you because I've studied them and they are wider, longer.

I'll tell you, I'll give you a couple of ideas why: they involve some sequence of permeability changes and obviously these permeability changes must be rapid. If the membrane potential at the beginning, in this phase that is often called the ***upstroke*** (the rising phase of the "stroke," it's ugly in [Italian], but in the literature, you read *upstroke*), if this happens in a fraction of a millisecond, it means these sodium channels must be able to open their gates very quickly.

Now, Hodgkin and Huxley, I told you, had no idea that these ionic conductances, these membrane permeability properties, were concentrated in space, were discrete, and were concentrated. I've already told you that they are proteins, I'll show you some animations, I discovered a guy on the internet who uses 3D modeling software typical of video games (I put it on Teams, for those who use Teams to read announcements, you will have seen the video). They are physically pores made of proteins that change their three-dimensional conformation, even very rapidly. It's not trivial that this change in conformation and consequent formation of a pore to let ions pass, as if it were the opening of a door to let ions pass... it's not trivial that this happens so quickly.

And you might think that some cells don't have this. Although they have potassium ions, they might not have these membrane permeability properties for potassium ions that are so rapid (a bit delayed, actually, delayed in the sense that it happens with a certain delay), it might simply not happen. The recovery from this point, which is called the ***overshoot*** (I don't know how to translate it, *overshoot* because it's like an extra elongation, a shot beyond the limit, *overshoot* because it not only reaches zero but exceeds zero. It's like in control theory, when there's an excess in the control of a variable, for example, of position: I'd like it to reach here, it exceeds, and then possibly comes back). In some cells, it might be that the return to the resting potential only involves these conductances that are always there, I never turned them off.

So the only thing that changes is that here the sodium, or the permeability that plays the role of sodium... you see I'm talking about sodium which is at $+50 \text{ mV}$, but if you had, for example, calcium which is at $+120 \text{ mV}$, from your point of view today it wouldn't change much, because you still don't know that calcium channels and sodium channels exist and that from the point of view of kinetics, some are faster than others. So, in effect, the cardiac potential of cardiomyocytes is wider because there are calcium channels, it's not sodium, calcium is the main actor, which open and close. Here it seems that, from how I'm telling it, which is the case for nerve cells, you have a system where not only do I have a mechanism to make the potential shoot up, but I have another mechanism to make it return very rapidly down. I could do without it, but then I'd be like the cells of the heart, where there's no consciousness, thought, emotions, etc., where this happens simply due to the passive RC properties, whereby from here, the potential slowly tends towards the resting potential.

Hodgkin and Huxley went further and tried to explain mechanistically how this kinetic of what they called ***gating***, from the English word "gate," could occur, whereby it's the opening and closing of a door. Although in '52 they somehow even provided experimental evidence that **multiple ionic species** were involved, multiple changes in permeability, it wasn't a generic change in permeability as proposed some 50 years earlier, or experimentally verified about 20 years earlier. They said, "no, no, it's different ions, and the membrane is specifically permeable in different ways, and from moment to moment, the permeability is not the same for all ions."

However, they couldn't measure the fact that these ion channels were doors, pores; they thought they were electrically charged structures (I'll tell you in a moment why they had this idea that they were charged particles), interspersed in the phospholipid bilayer and somehow more akin to a **transporter**. Something that grabs an ion from one side, obviously to pass it through the membrane it must be, as I told you, strongly hydrophobic and therefore in the lipophilic layer inside the membrane... to carry an ion from inside through this layer and out, energy must obviously be spent.

They thought it was a transporter and they thought, for a reason that will become clear, that this transporter had an electrostatic charge component, so much so that when the potential, for its own reasons, depolarizes or hyperpolarizes, these charged particles (since they are sitting in the middle of the membrane *sandwich* and the membrane potential is felt across the membrane) would feel an electric field and therefore a force pushing them to one side or the other of the membrane.

To make a long story short, they got very close, but they obviously didn't have the experimental tools to be able to say "no, look, it's not a diffuse property, there are no transporters, they are pores and they are discrete" and, as we'll see later, they are also so small that they are **stochastic**, they are noisy, they function randomly. It's not a door this big, it has nothing to do with quantum mechanics, but they are so small that they are affected by the conditions of a thermal bath at physiological temperature, $37 \text{ degrees}$, where there is thermal agitation.

What they did, in fact, was to propose an **equivalent circuit model**, and I repeat, this has always amazed me, they weren't engineers, they were physiologists, and not using digital computers, but using mechanical calculators, they were able to solve the (four) differential equations they proposed. You already know one of them, which is the charge balance equation. And they compared what was their experimental recording of an action potential which you see, yes, $0$ is their reference, again it's just a reference, they translated the axes and set them to zero, but here it starts from $-70 \text{ mV}$, goes up, depolarizes, and then hyperpolarizes and then takes a certain amount of time. This and this graph here are on two different time scales, you see one is between $0$ and $2 \text{ milliseconds}$, whereas here it's *zoomed out*, it's between $0$ and $10 \text{ milliseconds}$. So you see it goes up and down quickly, you understand why they called it a ***spike***, because it's very sharp.

And this is the mathematical model. And if I hadn't told you which was which, you might have said "boh, yeah, the one below, I mean, yeah, it does the same thing." It's not perfectly identical, but it's pretty good for the 1950s... and this type of mathematical formalism and this type of mathematical description of permeability properties, we continue to use it to this day, 70 years later.

What I told you before corresponds to the phenomenological reality of how these sodium and potassium channels work. Today we know that sodium and potassium function in a **voltage-dependent** manner. The channels themselves have, within them, a portion of their three-dimensional conformation's domain that has a net electrical charge. Everything is electronic, I agree, but you know that the charge distribution in an amino acid sequence, in a protein, depends on the conformation, depends on whether or not it's neutralized by other molecules. These have a sensitivity to the transmembrane potential, to the electrostatic potential across the membrane.

And both, this shouldn't confuse you, **both open** when the membrane potential becomes more positive, when it depolarizes. Now I'll show you this sort of cartoon, this animation. I remind you that there's a lot of sodium outside and little inside, and there's a lot of potassium inside and little outside. So if I have these doors that are selective, for whatever reason... as soon as they open, the flow of transport current occurs according to the electrochemical gradient. So a lot of potassium exits and a lot of sodium enters for free. It's not because there's a difference in how these two activate or pass from a closed state to an open state. As soon as I switch to an open state, the permeability is non-zero, the conductance is non-zero, it's no longer zero, and so the ions that need to pass, pass. I believe the sodium ions are the green ones, they will pass from outside to inside, and the orange ones which are yellow, whatever, go from inside to outside.

Sodium is an exception, I ask you (I'll play it several times) to keep an eye on this sort of **additional gate**, it's like a door that has a double... and only the sodium channel has it, not the potassium channel. Keep an eye on this object too, while the action potential is somehow highlighted in real-time based on the redistribution of this charge across the membrane. I'm not saying the equilibria change, these always remain the same, but if I open it for a moment, a lot of sodium enters, but it's not necessarily that I've caused the balance of sodium inside and outside to be altered and become... it continues to be maintained. The quantity, let's say, the imbalance is such that if you open it for a while (I'll show you an exercise first), it's not that you dramatically mess up the ionic fluxes. So you eat bananas and chocolate to maintain those ionic concentration gradients which, if you left the door open for a long time, for several tens of minutes (you'll see), then the concentrations would start to change a little bit.

This is the animation. For some reason (which could be the experimenter injecting positive ions) the potential starts to become more positive, and the sodium channels have opened, and after a bit, the potassium ones have opened. I'll play it again because it's a bit fast.

So the potential, for other reasons, starts to depolarize. Sodium opens, then that hatch unfortunately closes, the potassium channels open. A third time: they open, they **inactivate**, and after a bit, because it's slower to react, the potassium opens. They follow that sequence I described as necessary to explain this rise, fall, and rise again.

This additional hatch wouldn't be necessary. This hatch, which is called... it's a *gate*, an **inactivation** gate, in fact, it activates with a certain delay after the channel is open. Even though (I'll play it again) the channel is open, this hatch closes at a certain point. So, like with potassium, it acts with a certain delay, not everything is instantaneous, rather it depends on the chemical kinetics. Now the channel continues to be open, but the hatch is closed. This causes the potential here at the apex, at the *overshoot*, to stop.

If this additional *gate* wasn't there, when the potassium channels open, they would find further conductance from sodium, because sodium, I told you, opens just like potassium. It starts to open when the potential is depolarized, is a little more positive than it was. So if I open and potassium opens, okay, let's see who's better, whereas I told you that what I need is *either* sodium *or* potassium. If they are both activated together, things don't work, in the sense that the potential would have some intermediate *steady state* value (assuming a *steady state* could be reached). Instead, I want it to go down, I really want to remove this depolarization. Ideally, I need this because I'd like to restore the system to a starting condition.

So evolution has not only created an object like the sodium channel, which is among the fastest biological objects in the world (it manages to activate in a fraction of a millisecond, tens of microseconds or fractions of a millisecond it activates), but it also has a mechanism to close it. While potassium, which is more delayed, is more *delayed*, because it inherently has a kinetic, it's clumsier, has a different mass, I don't care, I'll treat it phenomenologically, it has different kinetics.

This is, if it's of interest to you, a kind of small table showing the characteristic time of a lot of biological phenomena, expressed on a logarithmic scale, so they are like orders of magnitude, and you see that the activation of a sodium channel is among the fastest reactions there are, compared with the reaction of an enzyme, with the transit time of an ion passing through a channel. And it's what I often say here in this region: it makes sense, it's like the Ferrari (I'll say it a couple of times), like the Ferrari of ion channels, because not only is it an interesting channel, it's something that activates with the electrical potential.

Many, and it doesn't take a Nobel Prize, recognize that the fact that proteins exist that sense the electric field is the turning point of evolution for using an **electric code**, electromagnetism phenomena in information processing. This makes sense, I'm not sure if you can already grasp it, it's very complicated to study, but it's the key to electrophysiological phenomena. You have pores whose opening and closing depend on the electric field, but once they open and let ions pass, their opening or closing changes the electric field.

So the fact that nerve cells speak the language of electricity is because there are ion channels that open and close according to the value of the electricity. But this opening changes the electrical potential, and if the electrical potential changes, the permeability changes, but if the permeability changes, the electric field changes. So from an experimental point of view, it's rather complicated to be able to decouple the two things. I'd like to be able to understand when the door opens, because I see it opening and closing, as you saw in that cartoon of sodium conductance and potassium conductance. I see them opening and closing, but who controls whom? It's not trivial to understand, especially how.


### ðŸ“‰ An Exercise: Metabolic Impact of a Single Action Potential

Before the break, I wanted to return to that point where... so you can think of this as an exercise, but we can do it together... where: why do you eat chocolate and bananas? It's because you want to maintain the reversal potentials. You have ion pumps for that, obviously, but you might think that, okay, by opening the channels, opening the doors, I dramatically mess up the concentrations. So I have an extreme need for the ion pumps to always be working. The ion pumps are there, they work 24/7, and you need energy (ATP) to run them. But the single action potential isn't that dramatic in metabolic terms. Let me show you.

What I'm doing is, I'm imagining taking a small piece of a neuron's morphology. Here, for example, you experimentally see a **pyramidal neuron**. Pyramidal neurons are excitatory neurons, I've told you about them in the first lecture, in the cortex and also in the hippocampus. This cable you see is called a dendrite, dendritic tree, and here you can glimpse the axon. They have different colors because this, these cones you see, it's a microscope image (transmission microscope... actually it's a fluorescence image, so I'll tell you what it is in a moment). Inside these pipettes (here you see the horizontal section viewed from above in the microscope), I've already shown you a couple of images of a pipette exploring and poking or penetrating the soma of a neuron previously. So you have another one that, for those who are very skilled and gifted experimentally, manages to penetrate, to poke a portion of the cell membrane that is very small, a cable, which is empty inside (in the sense that there's a space so if I put an electrode I can measure inside versus outside), but it's very small and it's also complicated in the plane, in this case it's a brain slice, with a tip one micrometer in diameter to manage to hit it and not go above or below.

I don't know how many of you do needle and thread, or have ever tried in your life to put the thread through the eye of a needle: it's not trivial. In fact, few researchers manage to make these recordings, which are called *somato-dendritic* recordings, somatodendritic recordings. They have different colors because inside each pipette there was a different dye, and this is a fluorescent dye that when excited at a particular wavelength fluoresces blue and green.

So, I'm considering a piece of dendrite. I'm showing you that in the case of a whole neuron's morphology, rest assured that a single action potential doesn't dramatically change things. And so I'm thinking of a small cylinder with a **length of $50 \text{ \mu m}$** and a **diameter of $10 \text{ \mu m}$**. By the way, it's quite chubby, quite large. It could be in the, let's say, proximal parts (because it's in proximity to the soma) of this apical dendrite. So a dendrite that extends at a distance from the soma, but judging by the value of $10 \text{ \mu m}$, it's quite large.

So I have the volume, and you know that if I have the volume and I talk about concentrations, I instinctively want to multiply the damn concentration by the volume, because at least I'm thinking in absolute terms. It's up to you whether you prefer to count things in moles or count things in number of particles. The extra information here is that I know the membrane potential changes, there's a $\Delta V$, and obviously $\Delta V$ makes me think of $\Delta Q$, so a charge. Knowing what the elementary charge is, if you tell me what the $\Delta Q$ is... Since the membrane is a capacitor, being a capacitor, if I know the $\Delta V$ I can perhaps infer the $\Delta Q$. And if I know the elementary charge ($1.6 \times 10^{-19} \text{ Coulomb}$) I can work back to how many ions were exchanged in this process.

Anyway, things in order. This is the lateral surface area of this cylinder, $2\pi r \cdot L$. I've called $2r$ the diameter because I specified the diameter, and so it's this quantity here, $\pi \cdot 500 \times 10^{-8} \text{ cm}^2$. Why did I do it in square centimeters? Because the membrane capacitance that I told you, the specific membrane capacitance that I told you to memorize, is on the order of what? First of all, what are its units? A capacitance. Farads. And the only thing I ask you to remember is that... it's not one Farad because that would be a supercapacitor, it's one microfarad, but that's not all. Since it's a specific capacitance, it has this value when it's referred to the unit of surface area expressed in square centimeters. **One microfarad per square centimeter** ($1 \text{ \mu F} / \text{cm}^2$), you can remember that. That's why I converted micrometers to square centimeters. I know it can be boring, but once you understand or remember the trick from high school or maybe elementary school, you do the conversions, you're fine.

The capacitance ($C$) so I've expressed it in... so I've multiplied by the square centimeters and this is the total capacitance, in microfarads. $\Delta Q / \Delta V = C$, so I now know how to express $\Delta Q$ if the $\Delta V$ is $100 \text{ mV}$ (from $-70$ to $+30$, about $100 \text{ mV}$, $+50 \text{ mV}$ whatever). And so I can calculate $\Delta Q$ and I get a quantity that is in Coulombs. I invite you to do it once, but the conceptual process is... okay, pardon, here it wasn't the volume, it's also the volume now, but the surface area is necessary because this is the capacitance referred to the unit of surface area. If it's a capacitance, it means it works, the law that holds for capacitors is: knowing what the change in potential is over time, I know what the change in charge is over time.

This is the change in charge, $\Delta Q$. Is it a lot or a little? I don't know, I have to divide by $1.6 \times 10^{-19}$ to get the number of ions exchanged, the number of particles. Or if you're a fan of moles, you just need to divide by the **Faraday constant** ($F$) which is the charge of one mole, simply a matter of units. If you divide by $F$ you get it in moles, if you divide by $E$ (by the elementary charge of the electron, $1.6 \times 10^{-19}$), you get it in particles.

So it would seem like 10 million, in reality, it's a negligible number, if you remember the example we did perhaps two lectures ago, where I showed you what typical millimolar concentrations corresponded to. So if you think about sodium, potassium... even sodium, of which there's a lot outside and less inside, is still... calcium would be a bit different, and in fact, there are reasons why even a single action potential matters. In this case, it's one nanomole, so okay, you can think that if now I can express not only the lateral surface area but the entire volume, and I think that inside there was $1 \text{ millimolar}$ before... Here I've converted it for you into the number of ions. One millimolar, sorry, $1 \text{ millimolar}$ means that in here there are on the order of $10^{17}$ ions, that is, it's 11 orders of magnitude more.

You don't deplete it, this tiny peripheral piece of a neuron's morphology, by firing just once. To empty that tiny little piece that has a $1 \text{ millimolar}$ concentration of sodium, to empty it you would need 240 of these action potentials. That's a lot, typically, particularly cells in the cortex fire once or twice a second, so you can think that this is on the order of minutes, and because the equivalent of ions is enormous. And there was another note I remembered, which is that if you consider a concentration of $50 \text{ millimolar}$, just to fix the idea, it means you have to fire for 6 minutes, generate action potentials at **$30 \text{ spikes}$ per second**, which is something even I can't do. $30 \text{ spikes}$ per second, and it is, despite not being a very high frequency, but in the central nervous system, it's very high and typically not sustained for 6 minutes continuously. Otherwise, you have an epileptic seizure, let's talk about that. And in fact, during an epileptic seizure, there is a change in ionic concentrations.

---

## ðŸ§ª Digression: Kinetic Schemes and the Law of Mass Action

Let's restart with a digression. For the moment, I haven't actually told you in detail how these ion channels change, how they open and how they close. I've simply told you that phenomenologically they depend on the transmembrane potential, and I need to make a digression on what are called **kinetic schemes** or **first-order kinetics** or **Markovian schemes**. I don't know if you've ever heard these things, you've heard them in the context of chemical reactions, where you probably just swallowed them as a kind of formalism that, although it's not... doesn't contain the quantum mechanics that today we know is necessary to understand why chemical reactions occur. Simply, if I put some of those symbols that you know, with pluses and arrows, you know, "okay, this works for me because it predicts what I phenomenologically get, for example, in a beaker."

But it's not that the molecules add up $Na + Cl \rightleftharpoons NaCl$. This is a formalism, it's just that you've always accepted it as something qualitative. I'm making it quantitative, so it can be another potential nightmare. I hope not.

Everything that's at the basis of this description, which is phenomenological (I'm not telling you how the molecules combine, the various covalent bonds, the van der Waals interactions, no, I'm literally throwing all that out the window) and what I'm telling you is, at most, *how quickly* these reactions happen. $Na + Cl \rightleftharpoons NaCl$ simply means that for some reason, when I put these two compounds, two objects, these two states, these two conformations (obviously it's more correct to say objects), I put them together, there's a kind of transformation that I'm not describing, because it's not described here, the state changes and, at most, I tell you how quickly this happens. Often, perhaps some of you remember that coefficients, *rates*, kinetic constants were put in.

This formalism is due, it's the instance... the horrible instantiation... the consequence of a principle that I won't derive for you from first principles, it comes from equilibrium thermodynamics and is due to two Norwegian researchers from the... 1800s, and it's called the **Law of Mass Action**, which you've surely heard of. I'll repeat it for you in quantitative terms and in more abstract terms, where I'm not necessarily interested in chemistry, I'm interested in describing a phenomenon where I don't go into detail, but I want a way to describe how quickly this phenomenon happens.

And so, again, there are objects $A + B$. They can be chemical species, they can be states. If it rains and I have the flu, I don't know, I become radioactive. There's no possibility of going back, and with a particular speed, a rate of change... and at the end of this, it's just a few slides, at the end of this, I'll tell you about it again in stochastic terms, in microscopic terms and not mesoscopic as we're talking at the moment.

This type of formalism literally means only that when there's an interaction between these two objects, the rate at which $C$ appears is **proportional** to how much $A$ there is and how much $B$ there is. So if I write it like this ($A+B \rightarrow C$), I'm ignoring the mechanism, but I'm saying that $C$, the rate at which $C$ is produced, appears, is converted (whatever, depending on the context), depends on the quantity of $A$ and the quantity of $B$. It kind of makes sense because if I don't have $A$ or I don't have $B$, $C$ won't change, it won't be produced (or it won't disappear in the case of an arrow in the other direction).

And mind you, I'm not talking about absolute quantities, I'm talking about the **rate of change**. Okay, I made the unfortunate choice of calling it $C$, but it doesn't matter, $A$, $B$, $C$ has nothing to do with membrane capacitance. The rate of change means I'm describing how quickly the quantities change over time. All of physics, biophysics is made of relationships... take Newton, take the charge balance equation... everything is easier when expressed in terms of physical laws that have to do with the change over time. It's not easy for me to establish a physical law to say "$C$ equals...", "$C$ as a function of time equals...". In this case, we do it, but the physical law tells me how the rate of production changes.

So written in other terms, the rate of production of $C$ is linked to the concentration, the number, the moles, it doesn't matter. You know they are all equivalent concepts. Talking about concentration or density or talking about the total number of objects is the same thing, it means I'm as if I could multiply both sides by the volume. If I have reactions where these are concentrations, they are in millimoles, okay, I multiply all quantities by the volume of the beaker, I talk about the number of objects. And I'm saying again that the way in which, for example, $NaCl$ appears depends on the product of the concentrations. And the whole point is the rate of production, it's not "$C$, what is it now?". This is perhaps more intuitive than you think, but I wanted to stress it a lot.

$$\frac{d[C]}{dt} = k \cdot [A] \cdot [B]$$

So another consequence of this is that if this was $k \cdot [A] \cdot [B]$, it means that the rate at which $C$ is produced is not dependent on the past history, it depends only on the current state, the state of $A$ and $B$, how much concentration of $A$ there is and how much concentration of $B$ there is. Clearly, if you make it "reversible" it will also depend on $C$, but it doesn't depend on the past history. And I don't know how many of you know or are familiar with **Markovian models** in general. Maybe in *machine learning* you've seen something. In probability theory, there's a way to model phenomena that's called with a property that is Markovian, and it means "there is no dependence on the previous history."



### ðŸ§® Examples of Kinetic Schemes

Here are some examples I'll give you. You'll see it's a little game where we write... you'll see I pull out some differential equations (I just pulled one out as a differential equation) and you'll see it's very easy. Here, for example, there's $A$ which is converted into $B$, reversibly, $k_1$ and $k_2$ for the moment make sense, they indicate how quickly those reactions occur.

$$A \underset{k_2}{\stackrel{k_1}{\rightleftharpoons}} B$$

And to write this differential equation quantitatively, I'm in fact invoking the law of mass action. In fact, let's say, I look at each of these nodes (it's like a graph, it is a graph, in this case, there are only two nodes, there are transitions between nodes; Markovian models are exactly a graph with transitions). Here, for each of these nodes, I write a differential equation, which tells me how that species... ionic... [transcriber error, means "chemical species" or "state"] changes.

So here I have $A$, here I have $B$, I'll write two, $\frac{dA}{dt}$ and then another equation, $\frac{dB}{dt}$. When I set out to look at this $A$, I imagine it as if you had a kind of container of liquid, which has a kind of leak, a pipe, a drain, which is this little arrow going from $A$, away from $A$. It's a drain for me, this, of which $k_1$ is like the diameter. I'm reasoning in a light-hearted way, but not really. In the hydraulic case, there too the cross-section of passage (and in the end, we've partly discussed it) of the outflow flux would give a change in the speed of disappearance in this case. Creation or disappearance, production or disappearance or consumption, is simply a convention to say if the derivative is positive or negative, if it increases or decreases.

So I have drains and I have faucets. So when I think about $A$, I think exactly about the **balance**, the conservation of mass. $A$ changes over time because there's this drain and because there's this faucet. This drain changes all the more rapidly the more $A$ there is. This is the law of mass action. $\frac{dA}{dt}$ disappears as quickly as (you see, **minus**, because this is outgoing, here I put a minus) the coefficient I have and this $k_1 \cdot A$.

And it makes sense because if I don't have $A$, it can't disappear. In other words, if I had an equation where I write $\frac{dA}{dt} = -k_1$... here you don't make me too happy, let's say I'm not unhappy because... there are... there are no exponentials here. Do you know how to solve this differential equation? How do you solve the differential equation? You integrate... [confused text] So this thing here is a straight line, the solution is a straight line with a negative slope. If you don't know the math, review it, but you see that here it means that the speed at which $A$ appears or disappears is constant. So it means that if I wait one minute, two minutes, it continues to decrease, it even becomes negative. In the case of chemical reactions or quantitative descriptions of objects that in the end are either in one state or another (but we're talking about the number of objects), it cannot become negative.

So the law of mass action solves this problem for you. It's natural that if I have, for example, a chemical reaction in which one species transforms into another but it's not reversible, at a certain point there's no more of it, it becomes zero, it doesn't become negative. So that's why this law doesn't hold, it aligns with what is observed experimentally. So $-k_1 \cdot A$ (which was the level of the liquid in this possible container I'm imagining) and then there's the faucet $k_2$ which counts as a **plus**, because you see the arrow is incoming. Clearly, it must be proportional to $B$ and the constant of proportionality is $k_2$. Here too it's as if I had two fluxes, each of which depends on how much stuff there is in this container, what the height is, how much mass there is in each of these containers.

$$\frac{dA}{dt} = -k_1 \cdot A + k_2 \cdot B$$

There's another differential equation to write ($\frac{dB}{dt}$) and it's easy to show that one is enough because these differential equations... you need $N-1$ of them where $N$ is the number of states, because the $N$th one is linearly dependent. In fact, I'll show you that, apart from the symmetry (you see that here $k_2 \cdot B$ appeared with a plus sign, here it appears with a minus sign; here $k_1 \cdot A$ with a plus, here with a minus).

$$\frac{dB}{dt} = +k_1 \cdot A - k_2 \cdot B$$

I'm tempted to add these two equations. Adding two equations means adding the terms on the left-hand side and the terms on the right-hand side. And if these two equations are true, their sum will also be true. If I do that, I discover that the sum of the derivatives, which I can, for example, rewrite as the derivative of the sum (thanks, invoking the linearity of the derivative operator), is equal to zero. Because this object here cancels with this one, and this one cancels with this one. I hope I don't have to do the little calculation.

$$\frac{dA}{dt} + \frac{dB}{dt} = \frac{d(A+B)}{dt} = (-k_1 \cdot A + k_2 \cdot B) + (k_1 \cdot A - k_2 \cdot B) = 0$$

And if the derivative of $A+B$ is zero over time, it means that **$A+B$ is constant**. And that's obvious. If I have $A$ and $B$, in the end, nothing is created, nothing is destroyed. If I have sodium plus chlorine plus NaCl (this deserves a bit of attention and we'll look at it later), it's clear that at every instant there must be the same quantity of matter that was there before. If I put all this on a scale, the scale doesn't increase or decrease, it's simply a conversion. But the total is conserved.

So what we will do constantly, we will do frequently, is: since $A+B$ equals a constant, I can call this constant $W$. I can call it $100\%$, or if you want to split hairs you could say "I'll change the variables and each... I'll call for example $a_{small} = A_{large} / W$, $b_{small} = B_{large} / W$", so that $a+b$, with this change of variable, amounts to at most $1$. So I can think in **fractions**: fractions of elements that are in a state $A$ and in a state $B$. Maybe fractions of channels that are in a closed state, or in an inactivated state, or in a whatever state. Since they are discrete three-dimensional conformations (this is an assumption that we can discuss later if you like), and I'm interested in talking about a fraction of the total $100\%$: what is the fraction of open sodium channels at rest? Are they all closed? Or is $25\%$ open, $15\%$ open? It can be convenient to think in terms of fractions.

If I do that I can... before doing that, to show you that the two equations... are... one is enough, they are linearly dependent, you can use $A+B = W$ (constant), and the constant is a given of the problem (it can be $100\%$, it can be $1$, or it can be how many grams of sodium and chlorine you had at the beginning). And you see that I can express $B$ as $W - A$. So this differential equation ($\frac{dA}{dt}$), I don't need to have another coupled differential equation, one is enough for me, because I get an equation in which only $A$ appears.

$$\frac{dA}{dt} = -k_1 \cdot A + k_2 \cdot (W-A)$$

And I'll let you solve this because it's the usual, boring first-order differential equation, constant coefficients, and if you don't solve it at the exam, you'll tell me why, where I failed to awaken your interest in something basic.

Here the thing I can propose is... okay, let's use fractions. And to do that, if you don't like the change of variables, I can say: "okay, we have to do the change of variables anyway, but I can also tell you, let's divide both sides by $W$". So here, since the derivative of $A$... so $\frac{1}{W}$, which is a constant, I can bring it inside the derivative sign (since it's a multiplicative constant, it doesn't depend on $t$, I can arbitrarily bring it inside or outside). I want to bring it inside because I already have the change of variable ready. But I did $\frac{1}{W}$ here on the second term and also here on the left $\frac{1}{W}$. It becomes $-k_1 \cdot (A/W)$... and $a_{small}$. Here I have $W/W$, which leaves $1$ (the $100\%$, thank God things remain consistent) and here again $A/W$ which becomes $a$. So I can write it like this:

$$\frac{d(A/W)}{dt} = \frac{da}{dt} = -k_1 \cdot a + k_2 \cdot (1-a)$$

And I can factor out $a$ so that it becomes in the form I'm used to, $\frac{da}{dt} = \dots$ (that constant that makes me nervous if the coefficient is positive or negative because there are exponentials that explode, or vice versa that dissipate, and all in all this is a dissipative system, there's no reason for it to explode) $\dots + \text{a forcing term}$. And so I can write it by factoring as:

$$\frac{da}{dt} = -(k_1 + k_2) \cdot a + k_2$$

This is a number because $k_1$ and $k_2$ are given. I see with great joy that there's a minus sign in front so I relax, also because $k_1$ and $k_2$ I told you are the speeds, the velocities, the *rates*, the conversion rate. And the rate means "so many molecules per second," it's positive, it's not negative. So $k_1$ and $k_2$ are positive quantities and so here I have the guarantee... I know how to solve this differential equation which is in fact, apart from $k_2$ which is a constant term, it's an exponential... [correction] it's an exponential arc that has a transient part which then ends and then a constant term remains. How do I know there's a constant term? I assume there's a *steady state*. If there's a *steady state* I set the derivative to zero, and the *steady state* value ($a_{\infty}$) is due to $k_2 / (k_1 + k_2)$.

I don't know how many of you vaguely remember that this formalism here allowed (now clearly I'm wrong because I don't remember, I'd have to derive it in 30 seconds but I won't do it now) allowed from here, even if this was something almost discursive, almost phenomenological... it was phenomenological... to say "look, if I give you these *rates*, I'm telling you how at *steady state*... *Chlorine*... the ratio of concentrations ($[NaCl] / ([Na] \cdot [Cl])$) ...it could be the opposite, I don't remember now, we'll see later why... is equal to a function of $k_1$, $k_2$... something like that." So in chemistry, it made sense to use this formalism because you had very interesting conclusions about the *steady state*. In the end, chemical reactions happen at such a speed that you don't really care about deriving the transient. We do, we want to have the most general case possible (kinetic, Markovian schemes), we want to have both the *steady state* and the transient. Why am I interested in the *steady state*? Because in a moment I'll apply exactly the same language to describe ion channels and you'll see that I can describe and understand the action potential exactly based on those kinetic constants. So yes, it's the same differential equation, the usual one.


### âš›ï¸ The $Na + Cl \rightleftharpoons NaCl$ Case and Mass Conservation

Let's talk about this sodium $Na + Cl$. I think I put, okay, compared to what I wrote on the board, in that slide $k_1$ is inverted with $k_2$, but they are two parameters, they are two names, they don't have any meaning. So here I made a mistake precisely because here it was a ratio between $k_1$ and $k_2$. In any case, this is what perhaps some of you remember from chemistry, and I'm not sure if anyone ever showed you where this ratio comes from. And so allow me to write again for this kinetic scheme, again for each of these nodes (sodium, chloride, sodium and chlorine), I write a differential equation.

$$Na + Cl \underset{k_1}{\stackrel{k_2}{\rightleftharpoons}} NaCl$$

Here, to remind myself and to remind you that in this context they are concentrations, but I can easily think of them as absolute numbers, by multiplying both sides by the volume. So it's not that important. When I called them $A$, $B$ before, here it could be $A \rightleftharpoons B + C$, it doesn't matter if you consider it as concentration, as a fraction, or as a total number, it doesn't matter. In the end, it's simply a change of variable. The concept is that you have a state: the state of the sodium-chloride molecule in a bound configuration, whereas in this case you have the individual particles, the individual unbound molecules. You can think in these states... I'm repeating this to you because if we know that ion channels have different conformations, different states (open, closed, inactivated... because there's that hatch inside which makes it explode, it's no longer two states "open or closed". It's open-inactive, open-active, closed-active, closed-inactive... it becomes four if I have that hatch, it's all the combinations of two elements, three elements, taken two at a time... that thing there, now I have to think about it because *combinatorics* was not a very interesting subject for me).

So reasoning in terms of either concentrations or molecules... [corrects himself] sorry, I was getting the order of the slides wrong... so for each of these, I write a differential equation. Here it is, okay, I consider it concentration, I write $\frac{d}{dt} [NaCl]$. And again I have the drain and the faucet, so here there's a $-k_1 \cdot [NaCl]$, because "however much liquid there is in this container, it takes it away that quickly". At the very least, if you have a bathtub that is very full, a bathtub, a sink that is very full, the flow is much faster because the quantity of matter is much more... it presses on the drain much more than if you have a trickle. The reason why, in fact, the profiles with which the height of the liquid changes in a container, in a *reservoir* with leaks, are exponential arcs, it's not that it goes down suddenly, instantaneously, or linearly, they are exponentials. Where there are exponentials, it means that the state variable which is here also appears in the second term, remember that this is what makes me happy or unhappy, so it's the key, exponentials appear because they are the eigenfunctions of this class of differential equations.

So here it disappears with rate $k_1$, so **minus** (it disappears), as fast as there is $[NaCl]$. And it appears (or *apparates*? no, appears) as quickly as, proportionally to $k_2$, how much there is of both the concentration of sodium and chlorine, by the **product** as per the law of mass action. It makes sense that there's a product, the product is easy to nullify, it's enough for one of the two to be zero, the product is zero. I need both species to make this reaction happen, so the fact that there's an operation here that is the product fits with the intuition. There's a **plus** sign and so it expresses the fact that here I have the creation, the formation again, I have the passage into this bound state.

$$\frac{d[NaCl]}{dt} = -k_1 \cdot [NaCl] + k_2 \cdot [Na] \cdot [Cl]$$

If I take this equation only and consider the *steady state*, I say that here I'm not interested in solving it for the transient. Note, I don't know how to solve it for the transient yet unless I write the other equations, I have to write another differential equation for sodium and one for... [error, means chlorine], because here it's an equation where, okay, here this is the state variable, call it $f(t)$, $x(t)$, call it whatever you want, it's also here, but here there's another term that... they are other quantities, in theory it's a system of three coupled differential equations (not all three are dependent for the reasons stated). However, if I'm only interested in the equilibrium and I consider it by saying "a *steady state* exists". If it exists, it means the quantities don't change over time. There's a time derivative here, it means the derivative is zero. In this case, I can rearrange the terms, bring this to the first member, change the sign, and if I divide $[NaCl]$ by $[Na] \cdot [Cl]$ I get $k_2 / k_1$ in this second member.

$$\text{If } \frac{d[NaCl]}{dt} = 0 \implies k_1 \cdot [NaCl] = k_2 \cdot [Na] \cdot [Cl] \implies \frac{[NaCl]}{[Na] \cdot [Cl]} = \frac{k_2}{k_1}$$

If you do this once in your life, you'll understand why chemists perhaps often proposed this to you as an extremely powerful method. In reality, the knowledge of this description with the language of differential equations is even more powerful because you can describe the transient, you can particularly, even in other contexts more related to chemistry, not be satisfied with what the final result is, but you might need to understand how quickly the chemical species in a bioreactor, for example, are formed or not formed.

Was this thing about kinetic schemes known, or is it the first time you're hearing it? You already knew it? No, okay, okay. The idea is: am I boring you or not. This... if you can give me some feedback, if not.

This to conclude this part: the story of the conservation of mass. If you write it like this, it requires being a little careful, because okay, I have to write a differential equation for each species. And again it's a little game: I put myself here, I'm writing $\frac{d[Na]}{dt}$. You see it disappears (this is a drain), so there's a $-k_2$, proportionally to how much sodium there is... in fact, there is... sorry, how much sodium there is but also how much chlorine there is, otherwise the reaction doesn't happen, it doesn't disappear, I beg your pardon. So $[Na] \cdot [Cl]$, premultiplied by $-k_2$, which is this one here. And then sodium appears with a *rate* that is proportional to $k_1$ times how much $[NaCl]$ there is. So $k_1 \cdot [NaCl]$ minus... Chlorine is practically identical, you see it's practically identical, but here on the first member it's different, it's the differential equation that describes how chlorine changes.

$$\frac{d[Na]}{dt} = -k_2 \cdot [Na] \cdot [Cl] + k_1 \cdot [NaCl]$$
$$\frac{d[Cl]}{dt} = -k_2 \cdot [Na] \cdot [Cl] + k_1 \cdot [NaCl]$$

So, either you blast through a series of algebraic steps like "subtract the second and third equations", "add the first and second", so you algebraically manipulate what it becomes, imagining that there's a *steady state* (so setting all these terms to zero). You have, that is, a system of non-linear algebraic equations. And you know that a system of non-linear algebraic equations requires, eventually to make explicit "something equals something else", or to make explicit reactions, requires a bit of algebra, you have to sweat a bit. I've sweated for you, but just an epsilon, it's not a difficult thing.

Another way is: if you try to add all three of these equations, on the left you get the sum of the derivatives, which means the derivative of the sum. The derivative of the sum doesn't come out to zero if you add all three equations, because it's true that this term here ($-k_1 \cdot [NaCl]$) cancels with this one ($+k_1 \cdot [NaCl]$) and this term ($+k_1 \cdot [NaCl]$) cancels with this one ($-k_2 \cdot [Na] \cdot [Cl]$), but this term here still remains ($-k_2 \cdot [Na] \cdot [Cl]$) and also this term here ($+k_2 \cdot [Na] \cdot [Cl]$). I don't know if you see it, if you try to do it once you'll see it doesn't work out. "But how? That guy told us that if you add all the differential equations together you get the conservation of mass".

Yes, but in this case, to... to have the sum of the derivatives equal to zero, you have to add this equation **twice** to the others. That is, $2 \cdot [NaCl] + [Na] + [Cl]$ is constant. If you do it, you'll see that having a coefficient $2$ here (and also a coefficient $2$ here), here $1$ cancels and here the other one cancels. This and this make $2$, they are equal. And also this $-k_2 \cdot [Na] \cdot [Cl]$... they make $2$ times. So if I want the first member to be zero (the sum of the derivatives or the derivative of the sum to be zero), I have to put two times this quantity here... ah pardon, to the sodium chloride.

And this makes sense because if you imagine taking *snapshots*, instantaneous photographs in time, and you start, for example, in a condition where you have $10$ (again I can do it as if they were molecules because I've multiplied by the volume, so yes this is a chemical reaction, in theory I should reason in concentrations, but you know that concentrations and absolute quantities are the same thing apart from a scale factor which is the volume). So I have $10$ molecules of sodium, $5$ of chlorine. I need one of sodium and one of chlorine to make one of sodium chloride. So in this case, after a certain time, the sodium has gone down to $5$, because more than $5$ don't have... more than $5$ of chlorine. So each one of sodium reacts with this one of chlorine and... one of sodium and one of chlorine make... so they are two... one of sodium and one of chlorine make one of sodium chloride. So here this is gone, this has become $5$ and this has gone back to $5$ ($10-5$ makes $5$). If I do it again, since the reaction is reversible, I can think that two of these have returned, they have two molecules, so it means that for each one you had $1$ and $1$. In fact, this from $5$ became $6$ and then $7$. This from chlorine became first from $0$, $1$, $2$. If you do the sum instant by instant ($10+5+0$, $5+0+5$), you always have the same number, except in this case that, okay... Except in this case, that is, pardon, you don't have the same number unless you multiply this quantity by $2$ ($2 \times 3$, $2 \times 5$). Another way to see it is that sodium plus chlorine is heavier, it weighs double, because it has two balls, one attached to the other. So this is another way to tell the story of the conservation of mass. When you have kinetic schemes (and we'll never see them) where you have $A + B \rightleftharpoons C$, it continues to hold with attention to the balancing discourse. There is anyway a balance because mass is conserved.


### ðŸŽ² Exercises on Kinetic Schemes

Now let me go back to this little scheme here, where I ask you to help me write the differential equation for each of these kinetic schemes. Here again, for each of these nodes, for each of these quantities, I have to write a differential equation, so I'll write $\frac{dA}{dt}$ and then afterward I'll write $\frac{dC}{dt}$.

**Scheme 1: $A \xrightarrow{k} C$**

Can you help me write $\frac{dA}{dt}$?
I repeat, it's a little game. If you're not with me, it's either because you don't care, or because you're sleeping. So, $\frac{dA}{dt}$... You see there's only one drain? How do I write it?

*(Voice from the audience: Minus k, A)*

I can't hear you, sorry. Minus $k \cdot A$, perfect. And $C$, in the end, is the dual case, how do I write it?

$$\frac{dA}{dt} = -k \cdot A$$

*(Voice from the audience: K times A)*

$k \cdot A$. And indeed, I like it because if I add them, this and this cancel out and $A+C$ is constant (the derivative, the sum of the derivatives is zero, so the derivative of the sum is zero).

$$\frac{dC}{dt} = +k \cdot A$$

---

**Scheme 2: $A \underset{k_2}{\stackrel{k_1}{\rightleftharpoons}} C$**

In this case, what do I need to change? Since it's no longer... not only is there the drain but there's also... there's the faucet from the other side. So there it was $k_1$, so I'll write $k_1$ here. Can you help me complete it?

$$\frac{dA}{dt} = -k_1 \cdot A \dots$$

*(Voice from the audience: Plus k2 times C)*

Okay. And here it will be the same but with a minus... [corrects himself]

$$\frac{dA}{dt} = -k_1 \cdot A + k_2 \cdot C$$
$$\frac{dC}{dt} = +k_1 \cdot A - k_2 \cdot C$$

...and in fact, if I add them, it works out.

---

**Scheme 3: $A + B \xrightarrow{k} C$**

In the end, this $A + B \rightarrow C$ (reversible/non-reversible), we've looked at it, so I won't stress you too much. In the end, it's almost sodium chloride. You also have the conservation of mass indicated, you see that here $A + B + 2C$... [correction: the scheme $A+B \rightarrow C$ implies $dC/dt = k \cdot A \cdot B$, $dA/dt = -k \cdot A \cdot B$, $dB/dt = -k \cdot A \cdot B$. The conservation here is more complex, e.g., $A-B = \text{constant}$]. And here too, again, it's not because it's reversible or non-reversible that the conservation of mass changes. I'm putting in some chemical species or states anyway.

If the channel is open... if I have a thousand channels, if $500$ are open, the others will be closed. It's a kind of principle, not an exclusion, it's a conservation of states. If it's not open, it must be closed. Okay, if there's inactivation, there can be a third state, but there can't be channels that are in some "whatever" state that isn't... it's either open or it's closed, since there are only two states. Here it's either $A$ or it's $C$.

Something that might be of interest to you, but I won't talk about it, is useful in **polymerization** reactions, where you don't have just one molecule of $A$. In fact, even in a context of electrophysiological signals, but we won't talk about it, in particular circumstances where multiple ions or multiple neurotransmitter molecules are needed to bind to a receptor. Actually, I'm thinking of calcium, but let's not talk about it.

---

**Scheme 4: $nA + B \xrightarrow{k} C$**

When you have a multiplicity, since this was a product ($A \cdot B$), then in the second member of the differential equation, the law of mass action says that the rate of appearance, well... depends on the product $A \cdot B$. So you have $N$ times $A$, so you have to do $A \cdot A \cdot A \dots \cdot A \cdot B$. This is the reason why you have this exponentiation to the $n$. And it's typical of polymerization reactions.

$$\frac{dC}{dt} = k \cdot [A]^n \cdot [B]$$

In this case, watch out, beyond the fact of the... so you either get there with the intuitive heuristic that the quantity of matter must be equal, or rigorously, by working with algebra, manipulating these equations, you would realize that the conservation of matter, so the mass balance, takes a form that is no longer the previous one (it was $2 \text{ times } C$ in the case of $A+B$...), so it's $n+1$. Anyway, we won't see this in the course.

---

### ðŸ”¬ Stochastic vs. Mesoscopic Interpretation

Before closing this premise, this *excursus* on these kinetic schemes, [which] were used by Hodgkin and Huxley to describe ion channels, as I've tried to insert several times in the last few minutes. I'll tell you, however, what another interpretation is, which is not a "population" interpretation. When I talk about a sodium concentration of $5 \text{ millimolar}$, I'm thinking of a system of $10^{18}$ ions, I'm thinking of a very numerous system, regardless of whether I express it as concentration or density, or multiply by volume somewhere. So I'm thinking of an enormous number of particles. I'm thinking of a system of many objects, **mesoscopic**. Likewise, when I told you a little while ago "if I have a thousand sodium channels..." and last week we discussed it, saying "these ion channels are interspersed in the membrane and from an electrical point of view I can... if it's possible, if they are all identical, they are independent (in the sense that they don't influence each other, for the moment I haven't talked about this, but one can imagine not), they all sit across the membrane and so they all experience the same potential". Kirchhoff, or the rules of electrical engineering for series resistors or resistors or parallel resistors, gives me those little formulas to group them. But I always have the idea that when I group them, I'm talking about a large population, so in that case, it's a mesoscopic description.

These differential equations written like this are **deterministic** equations. I solve them, I get the exponential arcs, and I go home, I have no stochasticity. In reality, the exact same formalism has a reinterpretation that is very important from the point of view of electrophysiological signals, because if you do an experiment with a biological neuron (I'll show you), it's not exactly a deterministic object. Today I'll show you an example of a computer simulation of the Hodgkin and Huxley model, and I'll show you the deterministic model and the non-deterministic version. If you plant an electrode somewhere in my brain, you don't see $-70 \text{ mV}$, you see a fluctuation as if it were a stochastic process. And if you show me a photo of Swiss chocolate, Belgian chocolate, some parts of my temporal cortex recognize it and start firing, but they don't fire like this [regularly], as instead I'll show you that the described neurons fire in a *pacemaker*-like way, like a metronome, regularly. They fire irregularly.

And so where does the stochasticity come from in this description? It's not there, they are ordinary equations (they could be partial derivatives, but they are deterministic equations). When it's written like this ($A \underset{k_2}{\stackrel{k_1}{\rightleftharpoons}} B$) and you put the magnifying glass not on a population of channels, a population of particles, a population of ions, of molecules (sodium and chloride), but on a **single one**, it means that $k_1$ and $k_2$ are not the rates of appearance or disappearance, of formation or consumption. They are the **instantaneous probability** of being able to make a transition, of binding. So the single molecule can (in the case of an ion channel) has a certain probability from instant to instant of changing state, of jumping. And every time I talk about a phenomenon to which I attribute a probability, I'm implicitly saying "it's like flipping a coin". I have the coin, if I flip it... I'll avoid doing it, but who knows what came up... or a kind of free random number generator, because nature is a stochastic nature.

So in reality, it's the same notation but a different meaning, where $A$ and $B$ are no longer concentrations but are the state of occupation. In the case of an ion channel, I look at an ion channel and I see if it's open or closed. In the case of a sodium molecule, I can see if it's unbound or bound. And the correct way (but you'll see in a moment that my usual favorite boring differential equation, always first-order constant coefficients, always comes out), but this wording says that the probability of a transition between state $A$ and state $B$ in the unit of time, conditional on the fact that at state at time $t$ (so at the current time) I am in state $A$ (so the past doesn't count, only where one is at this moment counts). The transition $A \rightarrow B$ cannot happen if I am in $B$. If I am in $B$, the transition from $B$ to $A$ can happen. So the fact that I'm using these conditional probabilities... yes, I'm thinking of Bayes' theorem which you've perhaps heard of, but it's not crucial. I'm simply translating into a formalism where the transitions are two, either from left to right or from right to left.

And the important thing is that these probabilities are probabilities that are defined in a small interval. I don't know how many of you know or have a reasonably okay background in probability theory or statistics. I don't know how much you remember (and I'll probably revisit it, and if I don't revisit it, I'll revisit it in another course next year). There are two types of stochastic variables or really stochastic processes: discrete-time or continuous-time. The probability that this phone rings *right now* is zero. The probability that a drop of rain falls exactly on the center of your head is zero due to a mathematical fact of the null measure of the set over which you do a particular integration. Whereas the probability of getting heads or tails... [correction] not discrete-time, continuous-time... has discrete values and continuous values. A coin has only two discrete values, while the instant of a phone call is a random variable that I'll call $T_0$ and this $T_0$ is a number that belongs to the set of real numbers (positive reals, suppose), so it's continuous.

There's a fundamental difference, so that's why I'm saying here that I'm considering a $\Delta t$, otherwise it makes no sense to write "the probability of transition at the instant $t$". So in theory, I should write that the probability of transition is given by these quantities, and these $k_1$ and $k_2$, if here they represented a speed, a rate (so dimensionally they are in units of $\text{second}^{-1}$), conversion per second... conversion can be dimensionless... "three times per second", "per second" is what interests me. So here $k_1$ and $k_2$ are not probabilities, they are instantaneous probabilities or **rates**.

So the correct description of this formalism is that the probability of transition is given by $k_1 \cdot \Delta t$. I won't draw it out, strictly speaking, this is actually the Taylor series expansion of an arbitrary function that I don't know but of which I'm only specifying the first order and I'm saying that the higher-order terms are negligible. But I won't tell you this, and I'll also tell you that the probability that in a time interval $\Delta t$ there are two transitions is zero because I choose $\Delta t$ sufficiently small. This is a thing about the Taylor series expansion, of a quantity that is in general an arbitrary function of time, that is, it's a function $\Delta t$. Because if I say "what's the probability the phone rings from now for the next two seconds?", it's clearly smaller than the probability "if the phone rings from now for the next five hours". You can intuitively understand this. The probability that that phone rings from now to 5 hours from now probably goes to $1$. But if I take the $\Delta t$ very small, the only thing I can do is say, "okay, if it's small, I think it's a function of the interval and I'll approximate it with Taylor". This $k_1$ wording is the first order of Taylor.


## âš¡ The Hodgkin-Huxley Model: Connecting Kinetics and Conductance

So, why is the story of Markovian kinetic schemes very interesting? Well, it's what Hodgkin and Huxley used. So, in fact, for them, the changes in conductance $G_{Na}$, $G_{K}$ (and in their model, because they were focused on the permeability of a particular biological preparation, the squid giant axon) were everything. And they described this change in permeability in phenomenological terms. They thought there was some electrically charged mechanism, but in fact, they said, "for me, a channel is either **open** or **closed**." I don't know, the permeability is due to a number of microscopic objects that can be in two states.

*(Pause, logistical comment about the power strip)*

So, for the moment, for simplicity, we see the channels as open or closed, or let's see how they managed. The *link* between the electrical component and the component describing the states I'll give you now. And it's fundamentally that of considering the conductances that are called **active** (because they are not constant in time, they are not passive) as variables. They are variable, **voltage-dependent** conductances.

Now let's see where I put the voltage here, where I put the membrane potential. One possibility, thinking about that microscopic interpretation (and here too, it doesn't take a particular genius), is that if the channels are normally closed but start to open when the potential becomes depolarized, evidently it is, at least in this transition ($C \rightleftharpoons O$), from closed to open... here, what was a probability per unit of time (the probability of making that transition) or the *rate* in the mesoscopic case, in the deterministic case... it's here that the dependence on the potential will come out. And that's exactly what is done.

That is, the total conductance, for example, suppose you're talking about $G_K$, given that electrically we had grouped (again I mention Kirchhoff, I mention the little rules of electrical engineering for parallel resistors or parallel conductances, etc.), the total conductance is the sum of the individual conductances. So in a way, it's the number of these channels ($N$) times the conductance of the single channel ($\gamma$). Now, to be quicker, I'll say it's the number of **open** channels ($N_{open}$), because if they're closed, they don't contribute to the conductance. So the *link* is here. The conductance depends only on the occupation of one state; in the other state, there is no conductance. There could be five states with intermediate conductance values. In the simple case we're looking at now, in the description of these membrane ion channels, it's enough (it's sufficient, I'll show you in what terms it's sufficient) to assume that the channel is either open (fully open) or closed. It's not "slightly open". But intuitively, if they are changes in configuration, of three-dimensional structure, you might say "in one state there might be a lower conductance than in another". It seems that in nature, in most cases, only one state coincides with an electrically functional state.

So here it's the total number... sorry, the number of open channels (for example, of potassium channels) and this is the conductance of the single channel ($\gamma$), when it's open, obviously. $10 \text{ picosiemens}$, something like that, on the order of picosiemens. If I have $1000$ channels and $500$ are open, it will be $10 \text{ picosiemens} \times 500$.

$$G_K = N_{open} \cdot \gamma_K$$

Maybe it would be more useful to think in terms of **fractions**. That is, to say: "I'd like to be able to know what the maximum conductance can be". So I'd like to write what the maximum conductance is... if you let me write it like this, $\bar{g}$. This here is $\gamma \cdot N_{total}$. So these are open channels, it means channels in this state. Here are all the channels, and at most, when they are all open, they will contribute to the conductance with $\gamma \cdot N$. Let's see how I write this for you. Having multiplied this quantity because I liked having this $\bar{g}$ which is therefore $\gamma \cdot N_{total}$. Now I have to divide because $N_{open}$ is left here, but to make things work out (otherwise it's not an equivalence) I have to further divide by $N_{total}$, which I multiplied by here, this $N_{total}$, the total number of channels.

$$G_K = (N_{total} \cdot \gamma_K) \cdot \left( \frac{N_{open}}{N_{total}} \right)$$

This quantity ($N_{open} / N_{total}$) is exactly a fraction. So it's $500 / 1000$ total channels, so that I can write $0.5$. This is very convenient for me, also from the point of view of the experimental description, because it abstracts me from figuring out how many channels there are. So, the channels, I told you, are discrete entities, and they are quantities that are stochastic (and we'll see why later), but when I talk about them at a population level, and particularly in Hodgkin-Huxley's time when it wasn't understood if it was a distributed or concentrated property of the membrane, perhaps it's more congenial for me to talk about fractions. And so I define this quantity here, $N_{large, open} / N_{large, total}$, $n_{small}$. I'll call $n_{small}$ a quantity that is between $0$ and $1$. It's a fraction, it can't be negative because the channels... it's a number, it can't be a negative number of open channels. The open channels are from $0$ to $N_{total}$. So at most, this ratio can be $100\%$, it can be $1$.

So note, from here I've multiplied and divided by $N_{total}$. And so I like to write $N_{total} \cdot \gamma = \bar{g}$, $g$-bar, barred, times $n$.

$$G_K = \bar{g}_K \cdot n$$

And this thing here, in theory, I can do for all active conductances. Where am I heading? The story of fractions came up when, a little while ago, an hour ago, we were talking about kinetic schemes. Kinetic schemes like this one:

$$C \underset{\beta}{\stackrel{\alpha}{\rightleftharpoons}} O$$

Allow me to call this $\alpha$ and this $\beta$. They are numbers, they are conversion rates, rates of opening or closing. If I want to write... if $n$ is "number of channels in this state ($O$) divided by the total number"... can you tell me how I should write $\frac{dn}{dt}$? The usual little game, only that instead of $k$ I'm now using $\alpha$ and $\beta$. Before it was $A$ and $B$, now it's $O$ and $C$. And now I'm thinking that here it's as if it were $n$. And there, obviously, you have to tell me. Remember that nothing is created, nothing is destroyed. How do I write $\frac{dn}{dt}$?

*(Voice from the audience: Minus alpha n...)*

I can't hear you?

*(Voice from the audience: Minus alpha n plus beta...)*

Perfect. Okay? So, okay, if you want, yes. We can do one more step.

$$\frac{dO}{dt} = -\alpha \cdot O + \beta \cdot C$$

And then... for $C$. Can I write it in a way... here I should also write an equation for $C$. Will you allow me to write that $C = W - O$ (a certain quantity minus the open ones)? So if it's like that, here instead of $C$, I'm using $W$ because we used it before, $W - O$. How do I write... how do I get to $n$, where $n_{small}$ is "number of channels that are open relative to the total number"? So if you like, sorry, but it's the same thing, it's $O / W$. How does this equation become? It becomes $\frac{dn}{dt}$. It remains practically the same, but I'm stimulating you to send oxygenated blood to your brain.

$$\frac{d(O/W)}{dt} = \frac{dn}{dt} = -\alpha \cdot \frac{O}{W} + \beta \cdot \frac{W-O}{W}$$
$$\frac{dn}{dt} = -\alpha \cdot n + \beta \cdot (1-n)$$

*(Voice from the audience: Minus alpha n...)*

Minus $\alpha \cdot n$, okay? And then?

*(Voice from the audience: Plus beta times one minus n)*

Perfect. And if you want, I can factorize, it becomes $-(\alpha + \beta) \cdot n + \beta$. Okay?

$$\frac{dn}{dt} = -(\alpha + \beta) \cdot n + \beta$$

Okay, this is the "stuff" of the Nobel Prize, because $\alpha$ and $\beta$ can be identified experimentally if you manage to see, for example, the *steady state* or the transient of this equation. And this equation, at least in theory, if $\alpha$ and $\beta$ were constant, I'd know how to solve it. If $\alpha$ and $\beta$ were functions that I don't know, that I can identify experimentally, if they were functions of the potential, okay, it would be a bit more complicated, in the sense that I can't do it with paper and pencil, I have to use a computer to do the simulation.

This thing here, I remind you that here it's the same thing as saying how the total conductance ($G_K$) changes over time, which is a maximum value ($\bar{g}_K$) times the fraction of channels ($n$). It's simply scaled. While $n$ goes from $0$ to $1$, this quantity $G_K$ goes from $0$ to $200 \text{ mS/cm}^2$. Nothing changes.

Experimentally, Hodgkin and Huxley were able to verify (and now I'll tell you how), they were able to measure the conductances. In reality, they measured the currents. They measured the potassium current ($I_K$), for example, which is $G_K \cdot (V - E_K)$. I remind you, $V$ is the membrane potential, and $E_K$ is $-80 \text{ mV}$. So I know $E_K$ because maybe I was the one who put the concentrations inside and outside my bath and my squid giant axon. So $V$, maybe I know it because I control it. If I measure the current ($I_K$), I can get the conductance ($G_K$). But if the conductance is $\bar{g}_K \cdot n$ (apart from this quantity $\bar{g}_K$ which will be a number, it will be what it will be), I can deduce the stationary or transient value of $n$. And so if this is my model, I can determine $\alpha$ and $\beta$. And if I succeed, I can try to put it all back together in a mathematical model and see if what comes out is an action potential. And the answer is yes.

And this type of formalism is so successful that we still use it today for a potassium current with inactivation, or a current called muscarinic current, or (we'll see) it's used to describe the current that flows through an ionotropic glutamate receptor, which you should... I overheard one of you telling me that evidently Zoli is doing ion channels, so postsynaptic receptors, ligand-dependent (or ligando, I don't know how it's said). So not only voltage-dependent, but dependent on the concentration of a neurotransmitter at a certain point. They are still channels, they are still pores. I'll show you that exactly the same formalism works.

Let's go back here. Here it's a bit complicated. Potassium, if you remember in that cartoon, opens with a certain delay. You've never seen the equation written as $\tau \frac{dn}{dt} = -n + \dots$ (something, for example, let me call it $n_{\infty}$)? Have you never seen a differential equation written like this? It's simply the exact same equation in which I've put a quantity, a parameter that I'll call $\tau$ (tau). I put it here because if I divide both sides by $\tau$, I get $1/\tau$ here. And this $1/\tau$ is exactly what goes into the exponential that makes me happy: $e^{-t/\tau}$ (thank God). So $\tau$ tells me (you either put it here, or you put it here in the denominator) what the time scale is.

$$\frac{dn}{dt} = - \frac{1}{\tau_n} \cdot n + \frac{n_{\infty}}{\tau_n} = - \frac{n - n_{\infty}}{\tau_n}$$

This thing here ($\alpha + \beta$), in the end, is the inverse of a $\tau$, because you see it's in the numerator. And it makes sense to me, because $\alpha$ and $\beta$ were rates, so "per unit of time", or "transitions per unit of time". So this is the inverse of a time. So $1 / (\alpha + \beta)$ is a time. So this thing here ($\alpha + \beta$) tells me how quickly... eventually the word *delayed* is no longer there...
eventually if one channel or another has, simply due to these kinetic coefficients, a slower reaction capacity.

Again, the same stupid first-order differential equation with constant coefficients that you do in Calculus 1, anyway it's always that one, it's always... And from here I read something: that if a channel is later, is more delayed, slower to activate or inactivate, it must be because of these two numbers. Okay, they will be functions, but numerically those functions will be different, they will be (especially now that they are in the numerator) they will be small numbers. That is, the transition *rate*... $1$ over a small quantity means it's a large quantity, so a large $\tau$, so something slow, which has a slow time scale of a few milliseconds, instead of sodium (which I'll show you in a moment, for now we're not talking about it) which instead was very fast to activate, at least to activate.

So this is the type of formalism where $n$ relates directly to the kinetic schemes.

---

### ðŸ›ï¸ Fundamental Assumptions of the Model

Before sodium, however, I must tell you what the fundamental assumptions of this are. This doesn't always hold.

1.  **Population Description (Mesoscopic)**: I'm thinking that I'm describing these channels as a **population**, so much so that I describe them as "fraction of open channels". I'm not describing *that* channel, there could be thousands or millions (we'll do that later), but for the moment we're describing a deterministic, mesoscopic system. A choir made of so many voices that even if one is out of tune, you can't hear it. The larger the choir (it's common experience), the less the individual voices count, because it's a summation, an average. The summation of sounds (because sound pressure waves reach my ear and add up) or the summation of currents or conductances (Kirchhoff) is the same thing. Saying "sum" or saying "arithmetic mean" is the same thing, the arithmetic mean is just further normalized but it too is a sum. I'll avoid citing some theorem from probability theory, but there too the irregular, stochastic behavior becomes more similar to the average behavior (in that case it's the ensemble average) when I have a large number of objects or attempts. What you may have heard described as the **Law of Large Numbers**: if I flip a coin a billion times, roughly the fraction will become $50-50$ if it's a fair coin.

2.  **Statistical Independence**: We are anyway in a deterministic context and so collectively these channels, a population, undergo conformational changes and the only control variable is the electric field that enters here ($\alpha(V), \beta(V)$) because these channels are interspersed in the membrane. This is important because the moment we write it like this, we're saying they are **statistically independent**. They are identical and statistically independent, in the sense that if we are all channels, if I open, I don't directly influence you to open because my state has changed. At most, I can influence you because if it's a morphologically distributed structure, I open here, the potential changes locally, maybe this perturbation of the electric field is the only thing we have in common. But there's no phenomenon of **cooperativity**, i.e., the fact that if two or three or a hundred or whatever channels open, then the others open even more. It's a hypothesis that exists in many contexts of biology, it doesn't seem to be there in the case of channels, and it works because we can make this description which is the sum, the average. If they weren't statistically independent, there would be problems, we couldn't describe their fraction...

3.  **Discrete States and Markovian Property**: Okay, there are these discrete states through which the population relocates, moves. These are the voltage-dependent coefficients I anticipated. And this, in the end, is the law of mass action: the reaction doesn't depend on the past, it depends only on the concentrations of the reactants, that is, it depends on the current state (if I'm in the open state or if I'm in the closed state). This is the **Markovian property**, it doesn't depend on the past. And it's exactly what happens with chemical reactions, where I'm not thinking that if two ions, sodium and chloride, bind, they influence (at least in those simple terms, if the solution is sufficiently dilute) how another two nearby ions combine or dissociate.

---

### ðŸ”¬ The Experimental Challenge: The *Voltage Clamp*

The complication that required Hodgkin and Huxley not only to get their hands dirty from a mathematical point of view, but also electronic, paradoxically, is what I mentioned in the previous hour. That is, the fact that changes in conductance ($G$) cause (because they cause currents, $I$) changes in potential ($V$), because there's the charge balance equation that says $C \frac{dV}{dt}$ is equal to the summation of these currents.

$$C \frac{dV}{dt} = - \sum I_{ionici} = - \sum G_{ionici} \cdot (V - E_{ion})$$

...let me write it with $I$, okay, for how I've written it here, a minus is needed, for how I wrote $V - E$, otherwise they are... So if I change $G$, I change $I$. And by changing $I$, I change $V$. But by changing $V$, I change these $\alpha$ and $\beta$! And by changing $\alpha$ and $\beta$, $n$ changes, and so $G$ changes.

And obviously, like many phenomena in biology, it's exactly a **feedback** (positive or negative, it doesn't matter at this moment), but I'd like to decouple them. If I have to measure quantities, it's very complicated to see that the current changes, so the conductance changes and the potential changes; changing the potential changes the conductance. I would need a way to isolate things.

And Hodgkin and Huxley invented a **feedback electronic amplifier**, capable (also helped by the particular geometric configuration, in truth, of the squid giant axon) of measuring the instantaneous potential, comparing it with a desired value, and, depending on whether... so creating an error signal. If the potential is $-65 \text{ mV}$, but I have as my... I want to impose... now I want to study if the channels are closed or open at $-70 \text{ mV}$, but the potential is changing to $-65 \text{ mV}$, there's a kind of difference. And this error is injected inside in the form of current, a negative current. So if it was $-65 \text{ mV}$, if I inject a negative current, the potential tends to decrease, maybe it becomes $-72 \text{ mV}$. Now $-70 \text{ mV}$ (desired) $-72 \text{ mV}$ (actual) is a positive or negative quantity... whatever... positive. So now I inject... I change this feedback (which obviously must act on fast enough time scales, not too much otherwise it starts to have oscillations, but that's another story), I inject a positive current. Instead of $-72 \text{ mV}$, it now becomes $-70 \text{ mV}$.

So I'm making a **P** (proportional) control system. You've perhaps heard in engineering about the PID control system, for example. They are the simplest possible control systems: Proportional, Integrative, Derivative. Here it's simply based on an error, a feedback amplifier corrects and tends, as they say, to **clamp** (from the English *clamp*), it blocks the potential at a particular value.

I need to block the potential at a particular value because I have that damned $C \frac{dV}{dt}$. If $V$ doesn't change in time (and it doesn't change in time because there's an electronic contraption attached to a biological preparation that acts electronically much faster than the biological preparation and keeps $V$ constant), $\frac{dV}{dt}$ is $0$. And what I'm left with is $\sum I = 0$.

So in theory, if I measure the quantity... so if by chance the membrane potential were to change (and it does for endogenous reasons, because these damned conductances woke up, activated, are closing, are opening), the potential changes and I have to inject a current artificially from the outside that *exactly* compensates for that current that caused the change in potential. So I manage to measure *exactly* the sum of the currents. And this is a first, very important point. So if I manage to nullify this, by looking at the feedback signal that I have to use to keep $V$ constant, I can see what's happening at the level of currents.

---

### ðŸ The Use of Toxins to Isolate Currents

So, this is a brilliant point from Hodgkin and Huxley. But it's not enough. They had to use, because otherwise, you would have had the sum of the currents... since both sodium and potassium each, suppose, change their conductance with similar equations (not identical, they aren't... you saw before, sodium activates first, then there's a kind of hatch that closes it from inside the cytoplasm, potassium activates with a delay, evidently this quantity $\alpha$ and $\beta$ are smaller in absolute terms, blah blah blah blah). I'd like to break all the things apart, otherwise I see them superimposed and I haven't solved much.

The answer came from the availability of **toxins** that some venomous animals (including spiders, fish, and other species) produce precisely because they are neuroactive, precisely because they bind specifically to certain channels.

There's a toxin that is my favorite because when we buy it in the lab for experimental reasons because we need it (you'll understand why in a moment), it's called **tetrodotoxin**. It's a nice acronym: **TTX**. When you buy TTX, since it's something that is quite potent, you have to sign a declaration saying "I declare that I am not a terrorist, that I do not want to use tetrodotoxin for terrorist purposes." That toxin is a very powerful blocker, an antagonist, as they say, selective for sodium channels. It sets the total conductance to zero for me. It's a plug from the outside, it plugs the channels, the external mouth of the channels (maybe it's a bit more complicated than that), but it binds to a binding site in the extracellular part of the sodium channel subunits' domains and blocks ion conduction through that channel.

If that's the case, I might have solved it, and I could, with this amplifier called a *voltage clamp* (because it clamps, it holds the potential fixed), I could study the kinetics and I could identify the parameters of the **potassium current alone**. I repeat, I know that the current I measure ($I_K$) is $G_K \cdot (V - E_K)$, but this ($E_K$), I know it's $-80$, $-90 \text{ mV}$ because I measure it, because I know the concentrations I put in. $V_m$, I'm holding it fixed at a value I choose, thanks to the amplifier. This quantity ($V-E_K$) is a constant value. $G_K = \bar{g}_K \cdot n$. In the end, if I'm only interested in the transient, I can take the current I measure, divide by this quantity that I know (because it's a quantity I can calculate), and I will have at most... I will have, in scaled terms, $n$. I'll see it as... up to a multiplicative factor. And I can do this because I no longer have interference from sodium.

There's another toxin, which at the moment I don't remember which beast it comes from. If I remember correctly, this one [TTX] comes from the *puffer fish*. The one that you can't eat in Japanese restaurants unless it's particularly... the chef... it's not sushi, but if he doesn't treat it well, because otherwise, it obviously blocks your sodium channels, so you stop breathing and so you die, there are no more action potentials.

There's another toxin called **TEA** (I don't remember what it stands for... tetraethylammonium, [correction: tetraethylammonium])... you can look it up on Wikipedia. This thing is specifically a toxin, a selective antagonist for **potassium currents** (for some potassium currents). And this is nature's gift, which has created extremely dangerous animals, but since they have to kill you in the most effective way possible, it blocks exactly that channel or this other one to create motor deficits and paralyze you in the end. So it blocks, by binding to the extracellular part of the voltage-gated potassium channel's mouth, and blocks conductivity.

So if I do it by putting in only TEA instead of TTX, I can study the **sodium currents**. And again, I do the same thing. I won't go into too much detail, this is just because the type of experimental work by Hodgkin and Huxley is not trivial, it's not that of "I just woke up and found the equations that work." They validated them and identified the parameters experimentally.

---

### ðŸ•Ž Parameterization: $\tau$ (Tau) and $n_{\infty}$ (n-infinity)

So for potassium, this is what we have, I've already told you. And if you allow me to divide both sides by $(\alpha + \beta)$, this is no longer here because I've moved it over here. But dividing and multiplying, I still have to remember that I have to divide it here. So that's why I have $1 / (\alpha + \beta)$ here, here I have nothing left (but the minus sign reminds me that these things almost certainly don't explode, again it's a dissipative system) and then I have a quantity $\alpha / (\alpha + \beta)$.

$$\frac{1}{\alpha_n + \beta_n} \frac{dn}{dt} = -n + \frac{\alpha_n}{\alpha_n + \beta_n}$$

Does this fit with my intuition? Because $\frac{dn}{dt}$... $n$ is a fraction so it's dimensionless, but $\frac{dn}{dt}$ has units of $1/\text{time}$. $1/\text{time}$ multiplied by... [correction] $\tau$ (because these here are the inverse of a time) cancels with time [correction: $\tau$ has units of time, not $1/\text{time}$]. So on the left of the equals sign, a dimensionless quantity remains. Here it's also a dimensionless quantity, and this too must become a dimensionless quantity. So since $\alpha$ and $\beta$ have dimensions of inverse time, $\alpha / (\alpha + \beta)$ is again dimensionless because the numerator and denominator have the same dimension.

And I can think of this quantity as **$\tau$** (tau), because it has the meaning of a time, of the time constant of that equation.

$$\tau_n(V) = \frac{1}{\alpha_n(V) + \beta_n(V)}$$

And this quantity here, I'll call it **$n_{\infty}$** (n-infinity), because it's what... as "infinity" for me, it's what, if a *steady state* exists (it might not exist, though), if it exists, that is, I think of letting $t \to +\infty$, of taking the limit for $t \to +\infty$, if a *steady state* exists, then $n$ no longer changes in time. Then this quantity, the derivative, is $0$. Here it's $0$, on the left it's $0$, and $n$, for it to be $0$, must take exactly this value here. So I know that this here is the value $n$ would take if the *steady state* existed. So I call it $n_{\infty}$.

$$n_{\infty}(V) = \frac{\alpha_n(V)}{\alpha_n(V) + \beta_n(V)}$$

$$\tau_n \frac{dn}{dt} = -n + n_{\infty}$$

Why do this? Why do I do this? Because experimentally it might be easier to work with $\tau$ and $n_{\infty}$ rather than with $\alpha$ and $\beta$. Both are functions of the potential, but it might be simpler to say "okay, you have this contraption, this electronic gadget that clamps your membrane potential. Well, fine... let's say, you set a profile for the membrane potential where you change it like a step." Okay, you'll have some... the current too, evidently the channels open, they close, they do something, but at a certain point, I go to a *steady state*. With those experiments, I can instantly understand what the dependence on the potential is, if I test multiple potential values, of this $n_{\infty}$. If I have $\alpha$ and $\beta$, since $\alpha$ and $\beta$ are... it's a bit more complicated, a bit less easy. I can always go from the $\tau$s and from $\tau_n$ and $n_{\infty}$, I can get to $\alpha_n$, $\beta_n$. (I'm starting to say $\alpha_n$ and $\beta_n$ because for sodium I'll have other quantities).

---

### ðŸ“‰ Analogy: The Low-Pass Filter

An interlude that resonates with what one of you asked me during the break. Again, I'm not a mathematician, so I can loosen the grip of rigorous mathematical formalism, which, however, we have maintained so far. But every time I have an equation of this type ($\tau \frac{dx}{dt} = -x + x_{\infty}$), I don't know about you (you probably have healthier thoughts), but it makes me think of an **electronic filter**. It makes me think of a box, a black box where $x$ is the output and $x_{\infty}$ is the input. In particular, the way it's written, this thing here makes me think of an RC circuit, it makes me think of a **low-pass filter**. It makes me think of something that even when I throw something very fast at it, like an impulse, it attenuates it, it dampens it, it's a bit lazy, it has an inertia, it has these exponential arcs. But from the quantitative point of view of the hands-on electronics engineer, they say "okay, yes, you're smoothing out the corners for me." This is what a low-pass filter does: if you go too fast, I cut you off in frequency, and so only what is lower passes, the lower frequency components pass.

Beyond the fact that yes, mathematically you can write it with the convolution integral, in the general case where $x_{\infty}$ is neither constant nor sinusoidal, it could be an arbitrary function. When it's like that... if you like working in the time domain, this mess of the convolution integral... well, it's a mess... it's obviously a mess in the case where $x_{\infty}$ is generic. In the case where it's a known function, this convolution integral might be less... it might be less intimidating. If, on the other hand, you're more inclined to be in the frequency domain (Laplace or Fourier, whichever), you know that the convolution integral becomes a product in the frequency domain, because differential equations in the time domain have a much simpler algebraic form as their correspondence in the frequency domain, in the transformed domain. Here it's a differential equation, in the transformed domain it becomes an algebraic equation. This is why Fourier or Laplace is used in electronics or in systems theory. We won't be doing that.

It's simply to tell you that this filtering thing is universal. And the fact that this is low-pass filtering explains well why. If I, as an input to a system like the one we saw together, the passive behavior of a non-excitable membrane, am subject to an external input that changes rapidly like a step, it's as if I'm squinting, I'm narrowing my eyes... in reality, I don't see these transitions very well, I see things more blurred. In effect, the output is like the blurred input. This is to say that every time you see an equation like this (when it's like this, when it's written like this, first-order, it doesn't matter if it has constant coefficients or not... actually, constant coefficients, we know how to do analytically, if they're not constant, I'm stuck with this type of convolution), in fact, I can think of it as a way in which the output **tracks** the input, apart from a delay. Here, if you see this exponential arc that first rises and then falls, in effect, the output is mimicking the input, which was a DC input that rose and fell, apart from a bit of laziness, *sloppiness*, a bit of sluggishness. You see it doesn't rise straight up, it takes a while.

So, whatever the complex shape of this input, the output ultimately follows it with a delay (what I'm saying is wrong, obviously, but roughly speaking, not particularly). So when I have time-invariant inputs I can do it analytically, but when I don't, it's as if I can say (again) if the input varies very, very slowly, I can think of it as constant, so it's trivial to say "okay, $x$ is instantaneously... it's always at *steady state*", because $x_{\infty}$ changes very, very slowly. If $x_{\infty}$ changes a little more rapidly, maybe $x$ continues to track it, but it can't keep up, because $x$ moves a bit more slowly because it's a low-pass filter. This is very useful information, and it will be useful for us to understand this thing about some channels being slow to activate. What does "slow" mean? It's slow in this case here.


### ðŸ“Š The Activation and Inactivation Curves

Now I'll show you, before the break, how Hodgkin and Huxley systematically analyzed these $n_{\infty}$. Now here I'll give you both $\alpha$ and $\beta$, and $n_{\infty}$ and $\tau$. And I suggest you look at $\tau$ and $n_{\infty}$ because $\tau$ tells you what the inertia is, how slow the system is to react. $n_{\infty}$ tells you what the *steady state* is, what point this equation is heading towards, what it's chasing. Whereas $\alpha$ and $\beta$, yes, they have an interpretation ("it's easy for the channel to go from closed to open," and I told you "the channels open when the potential is depolarized"), but functionally I don't grasp it with the story of "I need sodium to open, because when it opens I have the first phase; if potassium also opened I wouldn't go up, I'd stay more or less flat, because potassium is strong, it wants to keep me at [negative values]". So first the sodium must open, then after a bit the potassium must open, so the potential has time to rise and then to fall. If I don't have this delay, things don't work. And the delay must come from here, sorry, from $\tau$, not from $n_{\infty}$. From $\tau$, because it's $\tau$ that controls these dynamics for me.

#### Potassium Channel (Variable *n*)

So, I only need to show you this because the others are sodium channels and I'll talk about them in a bit. You see that $\tau_n$ (so this graph was obtained by doing a particular experiment, systematically with this *voltage clamp* amplifier, fixed at $-100 \text{ mV}$, then fixed at $-98 \text{ mV}$, then fixed at $-90 \text{ mV}$, then fixed at $-80 \text{ mV}$... trying to cover this whole *range*). And you see that $\tau_n$ is a **function of the potential**, so forget about me being able to do things analytically from now on, because they are not constant coefficients. Yes, it's the usual linear first-order differential equation, but the coefficients are not constant. So I have to do it numerically, maybe I'll invoke Euler, or Runge-Kutta. I need a numerical method, which in the math *refresh* part (if any of you needed it) you found it. If not, I assume that roughly you all know how to implement a crappy numerical method like Euler's (but don't knock it, it's a useful thing).

So it changes in time, but roughly from the point of view of units it's on the order of **a few milliseconds**. It peaks at $-70 \text{ mV}$ at $5 \text{ milliseconds}$. So it means that when the channel is closed (if you'll allow me this *stretch*), the channel is closed, the membrane potential is at $-70 \text{ mV}$, then the potential starts to depolarize... but so yes, $n$ tends to follow $n_{\infty}$.

$n_{\infty}$ has this shape here, this **sigmoid**. The fact that it's a sigmoid is profound because it has inspired a large part of today's *machine learning*, where everything... the units of an artificial neural network... has activation curves that are (nowadays only *threshold linear* is used), in other paradigms, in other contexts, perceptrons have a transfer function that is sigmoidal, that saturates. Here it must saturate because the fraction of open channels is either $0$ or $1$ or in between, it's not like it can become $1.5$, it doesn't make sense, at most it's between $0$ and $100\%$.

And you see that the thing I told you, i.e., "the [potassium] channels open when the potential is depolarized," is translated by this sigmoid because for potentials that are hyperpolarized ($-100 \text{ mV}$, $-70 \text{ mV}$), the fraction is practically zero (which is not exactly zero, you have to read this axis here). Suppose at $-70 \text{ mV}$ it will be $0.2$, $0.1$, $0.15$... $15\%$ of the channels are open at rest. But when I move the potential more and more (statically, obviously), I move it more towards depolarized potentials, you see that this grows and at a certain point, having reached... so the midpoint is around $-50 \text{ mV}$. So if I exceed $-50 \text{ mV}$, what happens after is further... it saturates, and from $0 \text{ mV}$ onwards you can increase it even more, those are the channels, they are all open.

And the $\tau$... so $n_{\infty}$ is what $n$ tracks. So $G_K$ (the potassium conductance) tracks this curve here, depending on how the potential goes. Clearly, this tracking is... [limited] barring a separation of time scales, which is a simplifying hypothesis I made a little while ago for this thing that "$x$ tracks $x_{\infty}$". The output of a filter tracks the input of the filter, apart from a delay. How much delay? On the order of a few milliseconds. Yes, the milliseconds reduce, they become $1-2 \text{ milliseconds}$, but it's still an object that is slow to react, slow as in $1-2 \text{ milliseconds}$. That's why it's called ***delayed***, retarded. That's why it's called "the opening is delayed".

#### Sodium Channel (Variables *m* and *h*)

In the case of sodium, of the sodium channels, which I'll finally tell you about before the interruption, you see that the activation curve **$m_{\infty}$** (I call it $m$ instead of $n$, but the form of the equation is the same). You see that this too is practically almost indistinguishable, it's just a little more *steep*, a bit steeper, and maybe it's shifted a little more to the left, if I remember correctly. No, it's even shifted to the right, so paradoxically at slightly depolarized potentials, the potassium would open before the sodium, if it weren't for the fact that potassium is very slow.

Look at the **$\tau_m$**, so the temporal dynamic of that equation for sodium ($m$), what values is it at? It's a **fraction of a millisecond**. I'm simply reading the axis. I understand that this too is not a constant but is a function that changes in time, in the end, it's a function that doesn't change too dramatically, it's a kind of Gaussian, a bell curve. Here it's on the order of a fraction of $0.1-0.2 \text{ milliseconds}$. So that's why the sodium starts immediately and the potassium starts later, because the potassium has at least one or two orders of magnitude slower speed to activate.

But there are three of these graphs, because it turns out that because of that wretched (or not wretched, very useful) intracellular hatch that creates the fact that the channel can also be open, but it is **inactive**. And I told you this at the beginning of today's lecture, saying that "it's a useful thing to slow down and therefore make the duration of the action potential much *sharper*, much shorter, more sculpted, more defined, shorter". To do that, I have to imagine that there's a temporal dynamic of that *gate*.

Currently, I'm thinking that a potassium channel, I imagine it like this, and here it has a kind of little gate. This little gate is a device that can jump between two states, open or closed. This is potassium, this would be selective to potassium, and since potassium (suppose here is inside and here is outside), the potassium ions go from inside to outside. For sodium, here too I imagine there's a little gate, but there's this thing that is also called the *ball and chain* model (chain and ball/sphere), or hatch or whatever you want, in which this little ball at a certain point blocks from the intracellular side.

You could also think that this object here is an additional little gate that, however, works with **inverse logic** (to use a term that might be familiar to someone in digital systems electronics). That is, instead of opening when the potential depolarizes (as the other one does), it works the opposite way, it closes. And in fact, if you look at this middle graph which has a state variable that isn't called $n$ (for potassium), isn't called $m$ (which is the activation variable for sodium), it's called **$h$** (for historical reasons Hodgkin and Huxley called it that), and it has to do with this inactivation *gate*. You see that this sigmoid works with inverse logic: that is, the more hyperpolarized you are, the more $h$ is at $100\%$, that is, all these *gates*... are open. And it makes sense, because in that cartoon I showed you, when the potential was very polarized, very negative, the channel was closed (this is closed here, $m$ is closed here), but the hatch underneath was open, it was wide open. Why? Because here $h_{\infty}$ is at $100\%$ or close to it. The more the potential tends to depolarize, the more (and quite rapidly) this tends to close.

With what speed does the sodium tend to track... well, the $h$ variable tends to track its *steady state*? You see that $\tau_h$, which is this dashed curve, is even **slower than potassium**. So roughly, let's say it's a factor of $2$ slower than potassium, anyway it's on the same order of magnitude as potassium, relative to sodium. Sodium is a Ferrari, that is, at $0.1-0.2 \text{ milliseconds}$ it reacts quickly with that time scale, with that time constant. But the fact that then the hatch slowly closes and then the neighboring potassium channels also open has a latency, and this latency is given by this order of magnitude. Here on the order of a few milliseconds, here again from $5$ to $1 \text{ ms}$, here from $8$ to $1 \text{ ms}$.

So obviously the interesting things happen between $-70$, $-80 \text{ mV}$ and $+20 \text{ mV}$, that's why this graph was made here. And with this description, I'll show you after the interval that we can make a detailed simulation of how it works, of how the action potential emerges. I'll stop here for ten minutes.


---

## ðŸ§© Assembling the Full Model

Okay, so here you see the story again of the hatch and the channel... the activation and inactivation part. So in analogy with what was said for the potassium channels, for the sodium channels (at least for their activation and inactivation), you have, in fact, a similar description, qualitatively but quantitatively, numerically different. Now these curves here, in a bit I'll show you, are ugly, numerical functions, but it doesn't matter because anyway we're entering the condition where we don't do anything with paper and pencil anymore, we have to use a numerical method to simulate four differential equations (at this point, which I'll show you) and so we might as well throw in what's ugly because it was identified experimentally. There's a somewhat deeper component from the point of view of biophysical interpretation, but I won't talk about it, I'll just mention it.

### ðŸš§ Is There an Excitability Threshold?

So a first interesting question you might ask yourselves is whether, having come this far, you could conclude if there is or isn't an **excitability threshold**, a discrete threshold that... that should motivate (if it's a motivated thing, and it might not be, it isn't) that describes a motto, a standard description in literature or textbooks, which says that neurons are **all-or-nothing** devices and that, once an excitability threshold is crossed, they emit (so if the input is large enough) an action potential.

Here it's not easy to pinpoint, there are no switches, there are no functions that depend on the membrane potential in a binary way. "Now the probability is zero, you stay in the closed state and shut up", or "now the probability is extremely high... the probability per unit of time is high to make the closed-open transition". There's a sigmoid that yes, is related to the step, is related to a *switch*, to an interruptor. In the end, the step corresponds, from the point of view of programming language, to that *statement* called `IF-THEN-ELSE`. `IF-THEN`: "if there's a condition then do something else". In biology, there are no Boolean laws or transistors, digital circuits, or at least they aren't like those made by man, so it's not so surprising that there aren't binary behaviors. And I'll point out that even in electronics, transistors don't have a binary behavior, they are used in a regime where the transitions are binary, but in some contexts of transfer functions, they are more similar to a sigmoid. This is a rather profound thing.

Is there a membrane potential above which all channels are open? No. Maybe around $-50 \text{ mV}$, but there are considerations that I'll show you later. Yes, around $-50 \text{ mV}$ there's the midpoint of potassium channel activation at *steady state* (so imagine the transient), anyway it's not that the channels are... $n$ is already directly equal to $n_{\infty}$, it has to go there, it has to track it. And it's not just him in isolation, there's also sodium collaborating. But anyway, for sodium, you see that perhaps it's a little more... the midpoint is greater than this $-50 \text{ mV}$. And for the $h$ inactivation *gate*, it's even more hyperpolarized. So perhaps a value... from here, from this static analysis, of an excitability threshold (which you always hear about and which is even used experimentally, but in another context), there's no trace of it here. It's more of an analog system that has a propensity to make a transition. Here we're talking about a fraction of channels: "from this point on, $50\%$ of potassium channels will be open", "here $10\%$ of these inactivation hatches of the voltage-gated sodium channels will no longer be available, $80\%$ or $90\%$ are already closed", etc.

### ðŸ§® The Final Equations of the Model

It's exactly the same thing repeated and written decently. I got carried away and wanted to immediately tell you this correspondence between time scales and *steady state* values that the state variable tracks. And here you not only find the $n$ we discussed, but you have by analogy the cases of which you experimentally have the graphs again, shown before, of the $m$ *gate* and the $h$ *gate*. I call them *gates* because, as I've indicated them here, beyond the fact that one works with a different logic, they equip the same sodium channel, they are not distinct channels.

How do I put these together? Because for potassium, we did it here, I know how to write the total potassium conductance, because I multiply it by... I have a total value... [text missing]... a maximum value $\bar{g}$ times the fraction of open channels, so times this $n$. So to the charge balance equation that was written here, $C \frac{dV}{dt} = \sum I$, I add three other differential equations, which are here. Ideally, one would want to add at least two more, one for sodium and one for potassium... one for the sodium channels and one for the potassium channels. For sodium, two are needed, because it has this strange characteristic of also having inactivation.

And we don't talk about it in this course (those who do the neuroengineering curriculum will meet me again next year), there's an enormous variety of voltage-gated channels, whose combination with similar equations (not exactly identical but similar, of the same form, with a $\tau$, the $\text{variable} - \text{variable}_{\infty}$) allows for describing the conductivity, the ionic conductance for many other channels. There are so many that some scholars call them... they define it as the **ion channel zoo**. So "if you are a pyramidal cell you have that set of 20 ion channels", "if you are a cortical inhibitory interneuron, which is called *fast spiking* because when you stimulate it, it fires very, very, very rapidly compared to a pyramidal neuron, it has another set of channels". As if nature had done what in video games in the old days you had the console and you put in a cartridge with a different game. Here, in the case of the squid giant axon, you only have sodium (voltage-gated) and potassium. So the correct name for these channels is:
1.  **Sodium**: *Fast Inactivating*, voltage-gated (it inactivates rapidly).
2.  **Potassium**: *Delayed Rectifier* (it's a delayed rectifier).

The "rectifier" part we don't care about, the "delayed" part I've told you now so many times that you're probably nauseated. There are many other channels, and the other channels can have the same type of complement, so every time you have a different current, you have one or two or three more differential equations. You understand why it's not trivial to possibly extract principles and, conversely, even to do accurate numerical simulations.

So, for potassium, we've seen it. Forget this exponent to the fourth power for a moment. For sodium, the way Hodgkin and Huxley experimentally found that things work (*fitted*), is by taking the two state variables that describe one *gate* (so a population of $m$ *gates*) and a population of other inactivation *gates* ($h$), and **multiplying them together**.

$$G_K = \bar{g}_K \cdot n^4$$
$$G_{Na} = \bar{g}_{Na} \cdot m^3 \cdot h$$

Note that this product is not so trivial, not so unexpected, because when is the current and conductance active (non-zero)? When the channel is **both open AND not inactivated**. Since $h$ works in inverse logic, when $h=1$ it means the channel is *not* inactivated. The channel activates when $m \approx 1$. So $1 \times 1$ [correction: $m^3 \cdot h$]... will be different from $0$. So the fact that the product is a mathematical operation that easily "kills" one of the two things when just one of them is small, is $0$, accounts for a good agreement with the experimental data.

Here you see the individual currents and you see these exponents to the fourth ($n^4$) and to the third ($m^3$) only on this *gate*, on this state variable $m$ and not on this one ($h$), because experimentally if you don't put them, you have a poorer agreement with the experiment. If you put $3.5$ it's a bit better, but it doesn't go so well. If you put $4$ here ($n^4$) and if you put $3$ here ($m^3$) it works perfectly.

There is a comment, a deeper motivation for the fact that this is effectively... So, this is how Hodgkin and Huxley found it. In retrospect, with researchers in the '70s-'80s and subsequent years understanding the single, discrete nature of ion channels (before, it wasn't known they were ion channels), it was understood that both potassium and sodium channels are not the result of a single protein, but are the result of **four subunits**. As a first approximation, it's as if each of these subunits has a voltage-dependent domain and when you put them together... now I'll show you an image or maybe not, maybe next time... this is a kind of side section. I imagine it as a kind of object that has four of these "sausages" and when these four "sausages" get close, they combine (they are the subunits... surely Zoli is telling you this in a much, much more accurate way for synaptic receptors), but from a functional point of view, only when the "sausages" are close, then the pore forms in the middle of them. And their voltage-dependent properties are, as a first approximation, statistically independent and each of them satisfies a kinetic equation of this type ($dx/dt = \dots$).

The fact that there is $n^4$ here and not $n_1 \cdot n_2 \cdot n_3 \cdot n_4$ is because these state variables are independent, since they all do the same thing, instead of writing $n \cdot n \cdot n \cdot n$ (there are $4$), I write $n^4$. Here [$G_{Na}$] there's the same thing with the four subunits: one is responsible for inactivation ($h$) and the other three for activation ($m^3$).

There are obviously slightly deeper reasons related to another description that perhaps we'll see next week, we'll definitely see it next week, of a discrete, stochastic interpretation, which I anticipated is anyway allowed by the kinetic schemes. Whereby this $n^4$ or $m^3$, this product... the fact that there are products here, which is not a given (I've sold it to you now simply as a mathematical convenience from the fact that if you have four non-zero objects, you multiply them together, it's enough for one to turn off, you've turned off the product) is a very powerful thing from a computational point of view, whatever that means. From the point of view of probability theory, the product comes out because it's a matter of understanding that the channel is open when the joint event "all subunits are in an open state" (and there are four subunits, for example, of potassium). And the probability of a joint event where the individual events are statistically independent is the **product of the probabilities**. This has to do with set theory, I don't know if... you should have... those who have seen probability theory, you've seen that it was dished out to you starting again from set theory that you perhaps saw in some initial university math course. It has to do with the operation of union... and actually intersection. So the probability that "it rains today" AND "that I feel tired" AND "that the phone rings", since they are three independent events, is the product of the probabilities. And this is why, for those who are interested, why there are these products.


### ðŸš€ The Complete System of Equations

So: **four differential equations**.

1.  **Charge Balance Equation (Potential V)**:
    The charge balance equation, to which we dedicated enough time to understand where it comes from, and to which today we can... we can complement with the fact that the conductances are anything but (in some cases, obviously for sodium and potassium; for this one [Leak] it's not voltage-dependent, so it has a passive, Ohmic, harmless form).
    $$C \frac{dV}{dt} = - I_{ion} + I_{ext}$$
    $$I_{ion} = I_{Na} + I_K + I_{Leak}$$

2.  **State Variable Equations (m, h, n)**:
    But for the other two [Na, K], I need three other differential equations, because this is the Ohmic form where the state variable that says which channels are open and not inactivated for sodium and for potassium channels... [correction] ...active... assumes... [pause, reorganizes] ...so I lost my train of thought...

So the differential equations for each of these state variables:
$$I_{Na} = \bar{g}_{Na} \cdot m^3 \cdot h \cdot (V - E_{Na})$$
$$I_K = \bar{g}_K \cdot n^4 \cdot (V - E_K)$$
$$I_{Leak} = \bar{g}_L \cdot (V - E_L)$$

Whereas for the Leak, it's passive and doesn't cause problems. So to this differential equation ($dV/dt$), I substitute these currents which I can now finally write. And I add these other three differential equations which are of this type ($dx/dt$), which in the end, come from this little game of open-closed kinetic schemes, [including] with the difference that sodium has this exception.

$$\frac{dn}{dt} = \alpha_n(V) \cdot (1-n) - \beta_n(V) \cdot n$$
$$\frac{dm}{dt} = \alpha_m(V) \cdot (1-m) - \beta_m(V) \cdot m$$
$$\frac{dh}{dt} = \alpha_h(V) \cdot (1-h) - \beta_h(V) \cdot h$$

$\tau$ has this form $1 / (\alpha + \beta)$, $x_{\infty}$ (where $x$ can be $h$, $m$, or $n$) has this other form ($\alpha / (\alpha + \beta)$) that we've seen. It's simply a way of rewriting, but I was perverse because I had anticipated that it would be simpler to write $\tau_x$ and $x_{\infty}$ instead of $\alpha$ and $\beta$. Oh well. This is for adherence to the Hodgkin and Huxley model.

### ðŸ”¢ Parameter Values (Squid Giant Axon)

Here are the functions that are dependent on the potential, as found by Hodgkin and Huxley. Those of you with some biochemical or biophysical background might recognize something that formally has to do, in some cases, with a form of Gibbs free energy, where the electrostatic potential is the only field describing energy. But beyond this similarity that motivated Hodgkin and Huxley in their choice of the particular... You see an experiment, what functions do I put in? Nowadays, for example, one might put in a *DEEP* neural network, which is fundamentally a model to do a *fit* of a non-parametric function. It's like a Taylor series or sinusoid expansion... actually, it's a sigmoid series expansion in the case of a *deep machine learning* architecture. They had a biophysical background, so they said, "you know what, ion channels today, or as they thought of them, these molecules, these transporters, in the end, must obey the laws of biophysics and probably activate or deactivate based on Gibbs free energy." And that's why they wrote this type of formalism. But you can forget what I just said and just say "the *fit* of the experimental data was accurate when these were the waveforms".

Here are the values of the other parameters instead:
* **Capacitance ($C_m$)**: **$1 \text{ \mu F/cm}^2$** (let this be imprinted on your frontal cortex).
* **Maximum Conductances**:
    * $\bar{g}_{Na} = 120 \text{ mS/cm}^2$
    * $\bar{g}_K = 36 \text{ mS/cm}^2$
    * $\bar{g}_L = 0.3 \text{ mS/cm}^2$
* **Nernst Potentials**:
    * $E_{Na} = +50 \text{ mV}$ (or $+55$)
    * $E_K = -77 \text{ mV}$
    * $E_L = -54.4 \text{ mV}$ (or $-54$)

You see they are all referred to the unit of surface area because we are describing a *patch* of membrane. If you tell me "no, it's a neuron that is spherical," okay, one puts in the values and you'll see that nothing changes, in the sense that in this equation you multiply both sides by the surface area ($S$). You would multiply here ($C \cdot S$), you would multiply here ($\bar{g} \cdot S$), here, and in fact, you would also multiply here ($I_{ext} \cdot S$). In the end, $S$ is as if it cancels out, it doesn't change. The only thing that changes is if this $I_{ext}$ is your pipette. Your pipette is no longer a distributed mechanism per unit of surface area, it's concentrated. At that point, you are injecting $100 \text{ pA}$ of current or $-100 \text{ pA}$ of current. Anyway, this doesn't matter, it's just a consideration for the purposes of the simulations I'm showing you.

The Nernst potential is $+50$ for sodium, $-77$ for potassium, and $-54$ for the leak. These are the values one must use for the squid giant axon when wanting to capture both passive and active behaviors.

We'll see this next time, and I'll show you, also because you're tired, we're all tired, I'll show you something a little lighter, I hope.

---

## ðŸ’» Interactive Simulation (Google Colab)

On the site, if you go to the "Resources" section and go to the "Notebooks" part, you'll find at the bottom on GitHub (so here you find the notebook files, but I'm assuming you don't know... probably you know little about notebooks). Here you'll find, as I recommended you do in the first lecture, you'll even find buttons that, when pressed, open that notebook directly in **Google Colab**, which I remind you is a kind of cloud platform for, in effect, doing simulations with Python, in effect, running Python scripts.

So the one I'm showing you now is called "Cell Excitability with Hodgkin and Huxley". And assuming it works, so I can use Google's computational resources for free, I'll first show you how I play with it and then I'll try to unpack the content, I'll show you the code, hoping to entice some of you to get your hands on it.

You are lucky today because you don't have to study a Python book, you can somehow use *Large Language Models*, in what is called, and is very fashionable, *Vibe Coding*. So even if you know zero about a particular programming language, but you roughly know the logic of it, you can get involved and you can write working code. The educational experience would obviously require your brain to stay attached. I don't care, and at the exam, it's not a programming course, but Bioengineering is a multidisciplinary field and the attempt is to tell you: either you're already equipped or I'm putting you in a position to do it. You should chew on math, biology, and *coding*, because the overlap of these, beyond the purely *data science* aspects, characterizes the bioengineer.

Since I'm a swot, in the first part of this Google Colab, I wrote, because I like writing equations with LaTeX (which is this formalism), it allows me to rewrite these equations. And they are written in a... they are rendered graphically in a nice way, let's say, with the exponent, as if you were using the Equation Editor in Word, but in a simpler way, especially one that doesn't crash like Word constantly does. I invite you to write your future master's thesis in Word and tell me whether or not you will suffer.

I'll run all the cells and the first thing I'll show you is this **all-or-nothing** behavior based on the Hodgkin and Huxley model, where I am the one changing the external current ($I_{ext}$) which is indicated by this current step you see here.

So note: to graph it on the same plot, I cheated, because the value of the current (positive or negative) has units of hundreds of picoamperes or actually in this case I think they are microamperes per square centimeter. I can read it here because in this slider I see $-0.067$. I believe, if I remember correctly, they are $\mu\text{A/cm}^2$, so that's $67 \text{ nA/cm}^2$. But to avoid having another graph next to it, I cheated and plotted on the same graph by multiplying or subtracting by a certain amount, so you have it exactly underneath. But it doesn't mean the current stimulus is $-100 \text{ mV}$, it's simply... I'm the one who forcibly wrote that the plotted current was not the current $I_{ext}$ (I used the same name as in the equations), but was $I_{ext} - 100$, so when $I_{ext}$ is $0$, you show it to me at $-100 \text{ mV}$, and also multiply by $50$ so you expand it a bit.

Let me hide the code right away, otherwise maybe... but I hope you don't get too scared. If you're brave you'll see it's not a particularly complicated thing. The other control parameter is not just the amplitude of the "flick" of the current step, but also its **duration**. It's indicated here I believe in milliseconds, so here it's about $1 \text{ ms}$ (from $5$... yes okay it could be, if these are milliseconds, $6$ will be more or less here, so it could be). I can lengthen it, obviously, it doesn't work, but every time I touch this slider it should redo... okay, now it's redoing it. And I didn't want to show this, it's something else, so, okay.

What I'm doing here is giving a negative, **hyperpolarizing** stimulus, the amplitude of which I can change. And you see that the membrane potential which is... sorry, I have to go in the other direction... the membrane potential... (darn you action potential, emerging, now I'll tell you why it emerges obviously, afterwards, when I'm no longer stimulating).

When I set it to negative or even very negative, the membrane potential (which is this black trace) in fact does what a passive compartment, a passive RC, would do. It has a negative charging curve, if I make the duration longer it's an exponential arc exactly like the one we did with paper and pencil with the slides last time. I turn off the stimulus, the capacitor recharges.


### âš¡ The "Rebound Spike" Phenomenon

So, I get an action potential here because by hyperpolarizing... In this graph here, you see that it's especially the fault of the **inactivation ($h$)**. When I'm around $-70 \text{ mV}$ (as I probably am here, I'm around $-60$, here suppose around $-60$, $-70 \text{ mV}$), the inactivation ($h_{\infty}$), so we're here, is not completely at $1$. There's a little bit of inactivation. So all the other channels that are voltage-dependent adapt to a *steady state* where the sodium channels are [partially] inactivated.

If I abruptly bring the potential to about $-90 \text{ mV}$ and then let go (so I bring it almost here to $-100 \text{ mV}$), I am forcibly removing all the inactivation from it. It's as if it were... I'm tempted to say, a withdrawal symptom, I don't know if it's a good comparison. I have constant inhibition (inhibition in the sense of constant inactivation) and temporally, for that input of mine which was meant to say "see, it's negative, I'm going in the direction of non-excitability," the neuron must not fire during my stimulus. Because during the stimulus, both sodium and potassium turn off even more, it's all *boring*, it's all dull.

Things are a bit more complicated because by removing... by hyperpolarizing the neuron, I **remove the residual inactivation** from it. And when I let go, I get a ***rebound***, an explosion. It's like when... again, it's similar to a withdrawal symptom where I always take some drug, something, when I stop taking it... [it's a] wrong comparison. It's more like a mechanism of constant inhibition and as soon as I remove... it's more similar to the brake. I'm driving a car with the handbrake slightly on, if by chance I release it, clearly the car can accelerate a little more. In this case, the car's acceleration leads me, after several milliseconds, to have an action potential.

This is an excitable behavior that is seen experimentally, and in the model, looking at the model you would have said "no, man, it's only when you put in a positive current that you at most get passive responses where, okay, here you're depolarizing a bit, but you don't get *spikes*." If you increase it a little, there's that sort of sigmoid, you're still not triggering this positive feedback between sodium, sodium inactivation, and potassium. At a certain point... not even now... at another point... nothing, the behavior is similar to the passive behavior.

And at a certain point, if the current is large enough, here a sort of... you see there's a discrepancy between the black trace and the violet trace (so you can think of the violet trace as the passive behavior, there's still something different but I'll introduce it later). Here there's clearly something **non-linear**, which isn't explainable by an RC circuit, so something has started to activate, but it hasn't activated enough to recruit other sodium channels to trigger this explosion. Surely the potassium hasn't had a chance to intervene to bring the membrane potential back down. So this is the classic thing you would have expected at the limit. So okay, there's a charging curve, at a certain point, however, the potential is so depolarized that it's cascadingly recruiting the sodium channels which open, inactivate, and the potassium conductances bring the membrane potential back down.

One thing you could do would be to "tinker" (as it's technically called) with the code and instead of, at a certain point... this isn't where I wanted to do it... like this, okay here... instead of plotting the variable $V$... in the end, it's a system of four differential equations. Why $V$? Because $V$ is what I measure. $M$, $N$, and $H$, I can measure them indirectly but I need an amplifier with this *voltage clamp* that Hodgkin and Huxley invented, which we have in the lab and use frequently for experiments of this type. But in a simulation, I potentially have the power to do anything, because I just need to, instead of saying "plot"... (I don't think I save it here), so if I say here... I'd have to change... instead of plotting the potential, I could plot $m$ or $n$ or $h$ (there are three). And so simultaneously show you how, instead of just the potential, the variable $h$, the variable $m$, the variable $n$ change in the $0-1$ *range*. Or I could combine the variable $m$ and $h$ in that $m^3 \cdot h$ to represent how the sodium conductance changes, if the sodium conductance opens and then closes, and the potassium conductance which opens and closes with a certain delay.

If you want, I invite you, and if you have problems I'll help you do it, it could be a stimulating thing. I'm convinced that you can do it by pure analogy just by looking at the code. And now I'll tell you about the code, I'm not giving you a computer science lecture but I'll tell you by pure analogy, even if you know nothing about Python and *coding*, how it can make a little bit of sense. The only thing that would be important on your part would be to review the Euler method, for the numerical resolution of equations. But for the moment, let me play with this system a bit more.

So, the story of the *rebound* potential wasn't there, it wasn't easily... it wasn't visible from the model, and it's, in a way, an additional prediction.

---

### ðŸ”¥ Frequency Coding and Periodic Activity

One thing that the model (and a colleague of yours mentioned to me during a break) is: what if the stimulus were maintained? Now in this simulation, I'm only simulating $20 \text{ milliseconds}$, but in the next one, I do more. If I continue to maintain the stimulus, I don't give a *step*, a flick, but I continue to maintain the current sufficiently intense to be able to make the neuron fire... Because if the current is lower, okay, I can keep the current on all I want, but nothing happens. Yes, there's evidently a non-linear transient here due to the active conductances (sodium and potassium), but then it stabilizes at a new *steady state*.

If I change the current and the current isn't just a pulse, you see I get **periodic activity** of several action potentials. And by changing the current (you can't see it much here) the frequency changes. One thing you see here is that by changing the current, the amplitude of the action potentials starts to... suffer, it starts to become much smaller. And this is what's seen experimentally because if I ask the neuron to keep firing, I'm engaging it in a dynamic regime where a part of that blessed sodium inactivation ($h$) is never removed. Even though potassium tries, the membrane potential never goes to particularly polarized potentials, and this wretched $h$, this $h$ *gate*, continues to persist in this [low] region. Note: $h_{\infty}$ is an instantaneous quantity. $h$ evolves with this dynamic equation that has inertia, so it has this $\tau_h$. So it's true that $h$ tracks $h_{\infty}$, but it does so with inertia, and this inertia also involves persisting and not allowing the potassium... [correction] ...it continues to not be present... [correction] ...it continues to not allow a reset of the previous conditions.

What I'm showing you here is the same thing, but I keep (and I probably change the scale of the currents) I keep the current on for an "infinite" time (here I stop the simulation after $300 \text{ milliseconds}$). When the current, the amplitude of the current is in this case $120 \text{ \mu A}$... obviously it doesn't work anymore, damn it, damn it, the slider doesn't work anymore, there. If the current is hyperpolarizing, nothing interesting happens. This is a trivial thing, but it's only in one direction that you get excitation. So if you imagine the brain or a circuit of neurons as an interacting system, neurons talk and (you know from Zoli, and we'll see it too) exchange information only when they are excited. They are excited in one direction, that is, when they are depolarized. When they are not depolarized (with the notable exception of electrical synapses or *gap junctions*, which Zoli probably didn't tell you about) they don't communicate. It's a system, it's a network of units that are isolated. They only communicate if the neurons go "above threshold".

When the current is increased, has a sufficiently large value (so it's not enough for it to be positive, it must be sufficiently large)... for example, now it is... I don't know why it doesn't... okay... the behavior is not that of having sustained *spiking* activity, in this case, the current is too low, you get a single action potential and that's it. This business of the action potential is also due to the condition... the choice of initial conditions. It's a system of differential equations, it requires the choice of an initial condition for $V$, $m$, $n$, and $h$. This, obviously, can change things in the transient.

However, if the current continues to increase, you have an extremely simple example of **coding** a quantity (in this case, the amplitude of the current) into **frequency**. If I increase the frequency, beyond the fact that the amplitude of the *spikes* will decrease a little (I don't know if you can see it), the *spikes* become denser.


---

### ðŸ“ˆ Type 1 vs. Type 2 Excitability

I'll show you something remarkable, which is a, again, a prediction of the model that *fits* with reality. You see that if I (and I assure you this also happens when changing the current in arbitrarily small steps) pass from a condition where the output frequency is zero, it's practically zero... I define frequency, so a periodic activity of membrane potential oscillation which, for example, in a time window of $300 \text{ milliseconds}$, allows me to calculate the frequency as $\text{number of spikes} / 300 \text{ ms}$. (An alternative would be, since these *spikes* are regular, to take the interval between two successive *spikes* and take the inverse, but let's just count them).

Here the frequency is still zero. If I try to increase it a little, you'll see that it doesn't go from zero to... [waits for simulation] ...you see, it does little... that it passes from $0$ to a frequency that, if I remember correctly, is about $30 \text{ spikes}$ per second... let's see. Between $100$ and $200$ it's $1$, $2$, $3$, $4$, $5$, $6$, $7$, $8$. That's $8 \text{ spikes}$ in $100 \text{ milliseconds}$, so that's **$80 \text{ Hz}$** ($80 \text{ spikes}$ per second).

If you can imagine it, a kind of graph where you have the current (the stimulus) on the X-axis and the frequency on the Y-axis (the famous Rosetta Stone I was hoping to have but don't, in the first lecture, the first or second week), I have a *mapping* between the stimulus and the frequency. And it's, for example, what some sensory fibers in my spinal cord are communicating to me. If you have an electrode and you put it in the dorsal part of my spinal cord, you would hear that the frequency codes for weight: I add weight, the frequency increases.

In particular, the excitability modeled this way for the squid giant axon seems to lack graduality. It passes from $0$ to $80 \text{ Hz}$. Then if you change... (there were about $8 \text{ spikes}$ in $100 \text{ milliseconds}$). So, if I count them here, you can see by eye that there are more, but it's $1$, $2$, $3$, $4$, $5$, $6$, $7$, $8$, $9$, $10$, $11$, $12$, $13$, $14$, $15$... $15$. We've reached $150 \text{ spikes}$ per second.

So there is a monotonically increasing relationship between stimulus intensity and frequency, but at the excitability threshold, at the so-called **rheobase** (which stands for... we'll see this term later: the minimum current value to get sustained oscillatory activity), there I have a first-order **discontinuity**, meaning it passes from nothing to $80 \text{ Hz}$. Not all neurons are like this; these are called **type 2 excitability**. And it's particular because it depends on the specific values, so on the position of these activation and inactivation curves.

I won't do it, but it would be enough for me to slightly change the position of this sigmoid, *shifting* it by about $10 \text{ millivolts}$, and you would have a **type 1** behavior, where the frequency-current curve would instead become capable of firing at arbitrarily low frequencies ($3 \text{ spikes}$ per second, $4 \text{ spikes}$ per second, $10$, $50$, $80$, $120$). But it would no longer have a discontinuity.

Again, if you were brave, you could try to get your hands on it. And where would I think of putting my hands? It could be in the value... we said, of activation. In these curves, $\alpha$ and... unfortunately I gave them to you as $\alpha$ and $\beta$, you could rewrite them as $m_{\infty}$, but you could simply play with these numbers here. $+40$ you could change to $+50$. Now I don't remember if... well... I have to shift to the left, so I have to... no, maybe I have to put $+30$ or $+20$. You would have to do it in both the numerator and the denominator and maybe also decrease this. You might discover that the behavior remains excitable but with different characteristics. These are all things at a more advanced level that should or could (the hope is that they do) stimulate your curiosity and make you try to get your hands dirty. I won't ask you this at the exam, so it's your idea whether you're preparing to get 30 on the exam or if you're preparing for a professional life. This is *up to you*.

---

### â³ The Refractory Period: Absolute and Relative

Instead, I'll show you something more interesting, more important, that I will certainly ask you about at the exam, which is titled the concept of **refractoriness**. Again, this came up during the first or second break of today's hours, today's lecture, where a colleague of yours asked me: "yes, but doesn't a little time have to pass before the system is ready again to fire a new action potential?". In the end, here the excitable behavior... it's not like two *spikes* can be arbitrarily close. If I arbitrarily increase the current here, at a certain point the excitable behavior turns off. And it turns off because I'm no longer able to remove all the sodium inactivation, so the neuron remains stuck in a condition where part of the sodium remains inactivated, it never gets reset, potassium can't do anything about it.

I'll show you this in this other simulation, which has a more interesting paradigm to understand. Here I give **two stimuli**, very very brief, and they are separated by... in this case, it's about $30 \text{ milliseconds}$. It's these two flicks in red, and each one is controlled by a different slider. The value of these currents, as I've chosen them, is such that in both cases I give the flick, I momentarily manage to cross a kind of point of no return (which is around $-60$, $-50 \text{ millivolts}$), so the action potential is generated because the sodium starts to open and a chain reaction is generated. And so I see that a *spike* is generated.

Personally, I could look at the profile of action potentials for hours because it's surprising that these impulses come out, which are more... engineering language. The concept of an impulse or coding with a binary component is more of a thing from electronics and digital systems, not from biology and billions of years of evolution, but this is a personal consideration.

Watch what happens if I narrow, if I decrease the interval between these two impulses. At a certain point... (nothing happens because Google Colab hates me)... at a certain point the second *spike*, this one, won't be there anymore because the system hasn't yet recovered from emitting the first *spike*.

So, we're still at $20 \text{ milliseconds}$. So the refractory period I'm looking for is now less than $18 \text{ milliseconds}$. It's less than... okay, at **$16 \text{ milliseconds}$**, you see that here this... and I haven't changed anything, it's exactly the same current amplitude. Is this **absolute** or **relative** refractoriness? Which means: if I increase the current of the second impulse here, can't I maybe manage to wake up the neuron? Yes, I wake it up. It's **relative**. It means that not all the sodium channels have de-inactivated (some still have inactivation), but if I push sufficiently, I can manage to make the neuron fire another time.

So, aware of this, I decrease it even more. Okay, I got to $16$, suppose for time reasons I go to $13$. At $13$, I try to increase the current. Now, evidently, the current has to increase quite a lot. It might be that I can't do it anymore, that it's an **absolute** refractory period. Let's see if that's the case or not. There's something that seems to be generating, so I'm optimistic.

Here too, a consideration: here the shape of the *spike* is highly **stereotyped**. I've triggered the same cascade mechanism and the shape and duration are practically superimposable. I'm telling you this because in a moment, by decreasing the interval between the two a lot, you might think that when I increase the current a lot (at this point it was $3 \text{ \mu A/cm}^2$, now it's $13 \text{ \mu A/cm}^2$), you might interpret this black *blip* as an action potential. In reality, it's not an action potential. The action potential, I showed you before, was similar to itself. So probably here I'm already in a state of absolute refractoriness. There's nothing to be done. Usually, Murphy's law dictates that now... nothing, okay.

Even at the maximum I can, for **$10 \text{ milliseconds}$** the neuron is refractory. If I do $1 / 10 \text{ milliseconds}$ and multiply by $1000$, I get a value that is similar to a frequency. In other words, if a neuron cannot emit (at least with these isolated flicks) two *spikes* closer than $10 \text{ milliseconds}$, it means that the **maximum frequency** is less than $100 \text{ spikes}$ per second, **$100 \text{ Hz}$**. $100 \text{ Hz}$ means the period is $1 / 100$, which is $10 \text{ milliseconds}$.

Things may not be like this, but the frequency-current curve that I made you imagine in your head, might not only have this discontinuity (in the case of this type 2 excitability), but it might not be a curve that grows linearly, it might have a **saturation**. This saturation (we'll possibly talk about it in the next few times) is again at the basis of *machine learning*, of the analogy with units that function as *threshold linear*: they have a threshold before which the input doesn't make the unit fire, doesn't activate the unit, after which the behavior is not linear but is sigmoidal, it bends.

I'll finish here, next week we'll continue, suggesting you play with this. I don't think you've had many courses so far (or that you certainly will have at Modena) where you'll see the convergence of these things: the computational part, the data analysis part, the biological part, the electronics/amplification part (even if I'm not aware of how much... I don't remember what my colleagues G. Bertoni and G. Baldi... do with you... about amplifiers of objects), in a case that converges in the case of biological signals. I'll close here, thank you for your attention. All right.
